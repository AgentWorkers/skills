---
description: ç®¡ç†ã€æµ‹è¯•æ€§èƒ½ï¼Œå¹¶åœ¨æœ¬åœ° Ollama æ¨¡å‹ä¹‹é—´åˆ‡æ¢ï¼ŒåŒæ—¶è¿›è¡Œæ€§èƒ½å¯¹æ¯”ã€‚
---

# Ollama Hub

ç”¨äºç®¡ç†å’Œæµ‹è¯•æœ¬åœ°çš„Ollamaæ¨¡å‹ã€‚

**ä½¿ç”¨åœºæ™¯**ï¼šåˆ—å‡ºæ¨¡å‹ã€ä¸‹è½½æ–°æ¨¡å‹ã€æµ‹è¯•æ¨¡å‹æ€§èƒ½æˆ–æ¯”è¾ƒæ¨¡å‹ä¹‹é—´çš„å·®å¼‚ã€‚

## å‰ææ¡ä»¶

- Ollamaå·²å®‰è£…å¹¶æ­£åœ¨è¿è¡Œï¼ˆé€šè¿‡`ollama serve`æˆ–systemdæœåŠ¡å¯åŠ¨ï¼‰ã€‚
- ä¸éœ€è¦APIå¯†é’¥ã€‚

## æ“ä½œæŒ‡å—

1. **åˆ—å‡ºå·²å®‰è£…çš„æ¨¡å‹**ï¼š
   ```bash
   ollama list                    # name, size, modified date
   ollama show <model>            # detailed info (parameters, template, license)
   ```

2. **ä¸‹è½½/åˆ é™¤æ¨¡å‹**ï¼š
   ```bash
   ollama pull llama3.3:70b       # download a model
   ollama pull mistral:latest     # latest version
   ollama rm <model>              # remove (confirm with user first!)
   ```

3. **æµ‹è¯•æ¨¡å‹æ€§èƒ½**ï¼š
   ```bash
   # Time a response
   time ollama run <model> "Explain quantum computing in 3 sentences" --verbose 2>&1

   # Extract tokens/sec from verbose output
   ollama run <model> "Hello" --verbose 2>&1 | grep "eval rate"
   ```

4. **æ¯”è¾ƒæ¨¡å‹**ï¼šå¯¹å¤šä¸ªæ¨¡å‹ä½¿ç”¨ç›¸åŒçš„è¾“å…¥è¿›è¡Œæµ‹è¯•ï¼š
   ```
   ## ğŸ“Š Ollama Model Benchmark
   **Prompt:** "Explain quantum computing in 3 sentences"
   **Hardware:** [CPU/GPU specs]

   | Model | Size | Tokens/sec | Response Time | Quality |
   |-------|------|-----------|--------------|---------|
   | llama3.3:8b | 4.7GB | 42 t/s | 2.1s | â­â­â­â­ |
   | mistral:7b | 4.1GB | 48 t/s | 1.8s | â­â­â­ |
   | phi3:mini | 2.3GB | 65 t/s | 1.2s | â­â­â­ |
   ```

5. **æ£€æŸ¥Ollamaçš„è¿è¡ŒçŠ¶æ€**ï¼š
   ```bash
   curl -s http://localhost:11434/api/tags | jq .    # API check
   systemctl status ollama                            # service status
   ollama ps                                          # running models
   ```

## æ¨¡å‹å‘½åè§„åˆ™

æ¨¡å‹å‘½åæ ¼å¼ä¸ºï¼š`name:tag`ï¼Œä¾‹å¦‚ `llama3.3:8b`ã€`mistral:latest`ã€`codellama:13b-instruct`ã€‚

å¸¸è§çš„æ ‡ç­¾åŒ…æ‹¬ï¼š`latest`ã€`7b`ã€`13b`ã€`70b`ã€`instruct`ã€`code`ã€‚

## æ³¨æ„äº‹é¡¹

- **Ollamaæœªè¿è¡Œ**ï¼šè¯·ä½¿ç”¨`ollama serve`æˆ–`systemctl start ollama`å¯åŠ¨OllamaæœåŠ¡ã€‚
- **ç£ç›˜ç©ºé—´ä¸è¶³**ï¼šåœ¨ä¸‹è½½å¤§å‹æ¨¡å‹ä¹‹å‰ï¼Œè¯·ä½¿ç”¨`df -h`æ£€æŸ¥ç£ç›˜ç©ºé—´ã€‚70Bå¤§å°çš„æ¨¡å‹å¤§çº¦éœ€è¦40GBçš„ç£ç›˜ç©ºé—´ã€‚
- **å†…å­˜ä¸è¶³**ï¼šæ¨¡å‹æ‰€éœ€çš„å†…å­˜å®¹é‡ä¸æ¨¡å‹å¤§å°ç›¸å…³ï¼š7Bæ¨¡å‹å¤§çº¦éœ€è¦8GBå†…å­˜ï¼Œ70Bæ¨¡å‹å¤§çº¦éœ€è¦48GBå†…å­˜ã€‚
- **GPUä¸CPUçš„æ€§èƒ½å·®å¼‚**ï¼šæ¨¡å‹æ€§èƒ½ä¼šå—åˆ°ç¡¬ä»¶é…ç½®çš„å½±å“ï¼Œè¯·åœ¨æµ‹è¯•æ—¶æ³¨æ„è¿™ä¸€ç‚¹ã€‚
- **æ¨¡å‹æœªæ‰¾åˆ°**ï¼šè¯·æ£€æŸ¥è¾“å…¥åç§°çš„æ‹¼å†™æ­£ç¡®æ€§ã€‚å¯ä»¥ä½¿ç”¨`ollama list`æŸ¥çœ‹å¯ç”¨çš„æ¨¡å‹åç§°ï¼Œæˆ–è®¿é—®[ollama.com/library](https://ollama.com/library)è¿›è¡ŒæŸ¥è¯¢ã€‚
- **ä¸‹è½½é€Ÿåº¦æ…¢**ï¼šå¤§å‹æ¨¡å‹çš„ä¸‹è½½æ—¶é—´è¾ƒé•¿ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚`ollama pull`å‘½ä»¤æ”¯æŒä¸­æ–­åç»§ç»­ä¸‹è½½ã€‚

## å¸¸è§é—®é¢˜è§£å†³æ–¹æ³•

- **ç«¯å£11434è¢«å ç”¨**ï¼šå¯èƒ½æœ‰å…¶ä»–Ollamaå®ä¾‹æ­£åœ¨ä½¿ç”¨è¯¥ç«¯å£ã€‚å¯ä»¥ä½¿ç”¨`lsof -i :11434`æŸ¥çœ‹å ç”¨æƒ…å†µã€‚
- **CUDAç›¸å…³é”™è¯¯**ï¼šè¯·ä½¿ç”¨`nvidia-smi`æ£€æŸ¥GPUé©±åŠ¨ç¨‹åºï¼Œå¿…è¦æ—¶é‡æ–°å®‰è£…Ollamaã€‚
- **æ¨¡å‹æŸå**ï¼šè¯·åˆ é™¤æŸåçš„æ¨¡å‹å¹¶é‡æ–°ä¸‹è½½ï¼š`ollama rm <model> && ollama pull <model>`ã€‚