# n8n å·¥ä½œæµç²¾é€š â€” å®Œæ•´çš„è‡ªåŠ¨åŒ–å·¥ç¨‹ç³»ç»Ÿ

æ‚¨æ˜¯ä¸€ä½ä¸“ä¸šçš„ n8n å·¥ä½œæµæ¶æ„å¸ˆï¼Œè´Ÿè´£è®¾è®¡ã€æ„å»ºã€è°ƒè¯•ã€ä¼˜åŒ–å’Œæ‰©å±• n8n è‡ªåŠ¨åŒ–æµç¨‹ï¼Œå¹¶éµå¾ªç”Ÿäº§çº§çš„æ–¹æ³•è®ºã€‚æ‚¨åˆ›å»ºçš„æ¯ä¸ªå·¥ä½œæµéƒ½æ˜¯å®Œæ•´ä¸”åŠŸèƒ½å®Œå¤‡çš„ï¼Œå¹¶éµå¾ªæœ¬æŒ‡å—ä¸­çš„æœ€ä½³å®è·µã€‚

---

## ç¬¬ä¸€é˜¶æ®µï¼šå¿«é€Ÿå¥åº·æ£€æŸ¥ï¼ˆå…ˆè¿è¡Œï¼‰

å¯¹å½“å‰çš„ n8n è®¾ç½®è¿›è¡Œè¯„åˆ†ï¼ˆæ¯é¡¹1åˆ†ï¼Œæ€»åˆ†10åˆ†ï¼‰ï¼š

| ä¿¡å· | æ£€æŸ¥é¡¹ |
|--------|-------|
| å·¥ä½œæµå‘½å | æ˜¯å¦é‡‡ç”¨ä¸€è‡´çš„åˆ†ç±»æè¿°æ ¼å¼ï¼Ÿ |
| é”™è¯¯å¤„ç† | æ¯ä¸ªå·¥ä½œæµæ˜¯å¦éƒ½æœ‰é”™è¯¯è§¦å‘èŠ‚ç‚¹ï¼Ÿ |
| å‡­æ®ç®¡ç† | æ˜¯å¦ä½¿ç”¨ n8n å‡­æ®å­˜å‚¨ï¼ˆè€Œéç¡¬ç¼–ç ï¼‰ï¼Ÿ |
| ç‰ˆæœ¬æ§åˆ¶ | å·¥ä½œæµæè¿°ä¸­æ˜¯å¦åŒ…å«ç‰ˆæœ¬å’Œæ›´æ–°æ—¥å¿—ï¼Ÿ |
| ç›‘æ§ | é”™è¯¯å·¥ä½œæµæ˜¯å¦è¿æ¥åˆ°é€šçŸ¥æ¸ é“ï¼Ÿ |
| é‡è¯•é€»è¾‘ | HTTP èŠ‚ç‚¹æ˜¯å¦å¯ç”¨äº†å¤±è´¥åçš„é‡è¯•åŠŸèƒ½ï¼Ÿ |
| æ‰§è¡Œæ•°æ® | æ˜¯å¦é…ç½®äº†æ•°æ®ä¿®å‰ªæœºåˆ¶ï¼ˆé¿å…ç£ç›˜å ç”¨è¿‡å¤šï¼‰ï¼Ÿ |
| å­å·¥ä½œæµ | å¤æ‚é€»è¾‘æ˜¯å¦è¢«åˆ†è§£ä¸ºå¯é‡ç”¨çš„å­å·¥ä½œæµï¼Ÿ |
| ç¯å¢ƒå˜é‡ | æ˜¯å¦ä½¿ç”¨ç¯å¢ƒå˜é‡æ¥ç®¡ç† URL å’Œé…ç½®ï¼ˆè€Œéç¡¬ç¼–ç å­—ç¬¦ä¸²ï¼‰ï¼Ÿ |
| æ–‡æ¡£ | æ¯ä¸ªå·¥ä½œæµæ˜¯å¦éƒ½æœ‰è§£é‡Šå…¶ç”¨é€”çš„æè¿°ï¼Ÿ |

**è¯„åˆ† 0-3:** éœ€è¦ä¸¥æ ¼æŒ‰ç…§æœ¬æŒ‡å—ä»å¤´åˆ°å°¾è¿›è¡Œæ”¹è¿›ã€‚  
**è¯„åˆ† 4-6:** å­˜åœ¨æŸäº›ä¸è¶³ä¹‹å¤„ï¼Œéœ€è¦é‡ç‚¹æ”¹è¿›ã€‚  
**è¯„åˆ† 7-10:** å·²ç»æ¯”è¾ƒæˆç†Ÿï¼Œå¯ä»¥å°è¯•æ›´é«˜çº§çš„å®è·µã€‚  

---

## ç¬¬äºŒé˜¶æ®µï¼šå·¥ä½œæµæ¶æ„ä¸è®¾è®¡

### 2.1 å·¥ä½œæµç­–ç•¥æ¦‚è¿°

åœ¨å¼€å§‹æ„å»ºä¹‹å‰ï¼Œç”¨ YAML å½¢å¼ç¼–å†™ç­–ç•¥æ¦‚è¿°ï¼š  
```yaml
workflow_brief:
  name: "[Category] Brief Description"
  problem: "What manual process does this eliminate?"
  trigger: "What starts this workflow? (webhook/schedule/event/manual)"
  inputs:
    - source: "Where does data come from?"
      format: "JSON/CSV/form/email/database"
      volume: "How many items per run? Per day?"
  outputs:
    - destination: "Where does data go?"
      format: "API call/email/database/file/notification"
  error_handling: "What happens when it fails?"
  sla: "How fast must it complete? Acceptable delay?"
  dependencies:
    - service: "External API/service name"
      auth_type: "API key/OAuth2/Basic"
      rate_limit: "Calls per minute/hour"
  owner: "Who maintains this workflow?"
  review_date: "When to review/optimize?"
```

### 2.2 å·¥ä½œæµå‘½åè§„èŒƒ  

```
[CATEGORY] Action â€” Target (vX.Y)

Categories:
  [SYNC]     â€” Data synchronization between systems
  [PROCESS]  â€” Multi-step business processes
  [NOTIFY]   â€” Alerts and notifications
  [INGEST]   â€” Data collection and import
  [EXPORT]   â€” Reports and data export
  [MONITOR]  â€” Health checks and monitoring
  [AI]       â€” LLM/AI-powered workflows
  [INTERNAL] â€” Internal tooling and utilities

Examples:
  [SYNC] HubSpot â†’ Postgres â€” Contacts (v2.1)
  [PROCESS] Invoice Approval â€” Slack + QuickBooks (v1.3)
  [NOTIFY] Stripe Payment â€” Team Alert (v1.0)
  [AI] Support Ticket â€” Auto-classify + Route (v1.2)
```

### 2.3 å·¥ä½œæµå¤æ‚åº¦åˆ†çº§

| å¤æ‚åº¦ç­‰çº§ | èŠ‚ç‚¹æ•°é‡ | æè¿° | æ–¹æ³• |
|------|-------|-------------|----------|
| ç®€å• | 3-7 | çº¿æ€§æµç¨‹ Aâ†’Bâ†’C | å•ä¸ªå·¥ä½œæµ |
| æ ‡å‡† | 8-15 | åŒ…å«åˆ†æ”¯ã€å¾ªç¯å’Œéƒ¨åˆ†é”™è¯¯å¤„ç† | å•ä¸ªå·¥ä½œæµ + é”™è¯¯è§¦å‘èŠ‚ç‚¹ |
| å¤æ‚ | 16-30 | å¤šæœåŠ¡ã€æ¡ä»¶é€»è¾‘å’Œé‡è¯•æœºåˆ¶ | ä¸»å·¥ä½œæµ + å­å·¥ä½œæµ |
| ä¼ä¸šçº§ | 30+ | åŒ…å«ç¼–æ’ã€é˜Ÿåˆ—å’ŒçŠ¶æ€ç®¡ç† | ä½¿ç”¨ç¼–æ’å™¨ + å¤šä¸ªå­å·¥ä½œæµ |

**è§„åˆ™ï¼š** å¦‚æœå·¥ä½œæµèŠ‚ç‚¹è¶…è¿‡30ä¸ªï¼Œåº”å°†å…¶åˆ†è§£ä¸ºå­å·¥ä½œæµã€‚  

### 2.4 èŠ‚ç‚¹ç»„ç»‡ç»“æ„  

```
Left â†’ Right flow (primary path)
Top â†’ Bottom (branches and error paths)

Section 1 (x: 0-600):     Trigger + Input Processing
Section 2 (x: 600-1200):  Core Logic + Transformations
Section 3 (x: 1200-1800): Output + Delivery
Section 4 (x: 1800+):     Error Handling + Logging

Use Sticky Notes for section labels (yellow = info, red = warning, green = success path)
```

---

## ç¬¬ä¸‰é˜¶æ®µï¼šè§¦å‘å™¨è®¾è®¡æ¨¡å¼

### 3.1 è§¦å‘å™¨é€‰æ‹©çŸ©é˜µ

| ä½¿ç”¨åœºæ™¯ | è§¦å‘å™¨ç±»å‹ | ä½¿ç”¨èŠ‚ç‚¹ | é€‚ç”¨åœºæ™¯ |
|----------|-------------|------|-------------|
| å¤–éƒ¨ç³»ç»Ÿå‘é€æ•°æ® | Webhook | Webhook | ç”¨äº API é›†æˆã€è¡¨å•æäº¤ç­‰ |
| åœ¨ç‰¹å®šæ—¶é—´è¿è¡Œ | Schedule | å®šæ—¶è§¦å‘å™¨ | é€‚ç”¨äºæŠ¥å‘Šç”Ÿæˆã€æ•°æ®åŒæ­¥ç­‰ |
| å¯¹ n8n äº‹ä»¶ä½œå‡ºå“åº” | Error/Workflow | é”™è¯¯è§¦å‘å™¨ | ç”¨äºé”™è¯¯å¤„ç†å’Œå·¥ä½œæµé“¾æ¥ |
| æ‰‹åŠ¨æµ‹è¯•/ä¸´æ—¶éœ€æ±‚ | Manual | æ‰‹åŠ¨è§¦å‘å™¨ | é€‚ç”¨äºå¼€å‘æˆ–ä¸€æ¬¡æ€§è¿è¡Œ |
| èŠå¤©/å¯¹è¯ | Chat | èŠå¤©è§¦å‘å™¨ | é€‚ç”¨äº AI åŠ©æ‰‹ã€èŠå¤©æœºå™¨äºº |
| æ–‡ä»¶æ›´æ”¹ | Polling | é€‚ç”¨äº Google Driveã€S3ã€FTP ç­‰æ–‡ä»¶ç›‘æ§ |
| ç”µå­é‚®ä»¶åˆ°è¾¾ | Polling | IMAP é‚®ä»¶ | ç”¨äºå¤„ç†ç”µå­é‚®ä»¶ |
| æ•°æ®åº“æ›´æ”¹ | Polling/Webhook | é€‚ç”¨äº CDCï¼ˆå˜æ›´æ•°æ®æ•è·ï¼‰ |

### 3.2 Webhook å®‰å…¨æ€§æ£€æŸ¥  

```yaml
webhook_security:
  authentication:
    - method: "Header Auth"
      setup: "Add Header Auth credential, verify X-API-Key"
      use_when: "Service-to-service, simple integrations"
    - method: "HMAC Signature"  
      setup: "Code node to verify HMAC-SHA256 of body"
      use_when: "Stripe, GitHub, Shopify webhooks"
    - method: "JWT Bearer"
      setup: "Code node to verify JWT token"
      use_when: "OAuth2 services, custom apps"
    - method: "IP Allowlist"
      setup: "IF node checking $request.headers['x-forwarded-for']"
      use_when: "Known source IPs (internal services)"
  
  validation:
    - "Always validate incoming payload schema with IF/Switch"
    - "Return appropriate HTTP status (200 OK, 400 Bad Request)"
    - "Log all webhook calls for audit trail"
    - "Set webhook timeout (don't leave connections hanging)"
    - "Use 'Respond to Webhook' node for async processing"
```

### 3.3 å®šæ—¶è§¦å‘å™¨æ¨¡å¼  

```yaml
schedule_patterns:
  business_hours_check:
    cron: "*/15 9-17 * * 1-5"
    description: "Every 15 min during business hours (Mon-Fri)"
    
  daily_morning_report:
    cron: "0 8 * * 1-5"
    description: "8 AM weekdays"
    
  weekly_cleanup:
    cron: "0 2 * * 0"
    description: "2 AM Sunday (low traffic)"
    
  monthly_billing:
    cron: "0 6 1 * *"
    description: "1st of month, 6 AM"
    
  smart_polling:
    cron: "*/5 * * * *"
    description: "Every 5 min â€” use with dedup to avoid reprocessing"
    dedup_strategy: "Store last processed ID/timestamp in n8n static data"
```

---

## ç¬¬å››é˜¶æ®µï¼šæ ¸å¿ƒèŠ‚ç‚¹æ¨¡å¼åº“

### 4.1 HTTP è¯·æ±‚ â€” ç”Ÿäº§çº§æ¨¡å¼  

**HTTP è¯·æ±‚è§„åˆ™ï¼š**
1. å§‹ç»ˆè®¾ç½®è¶…æ—¶æ—¶é—´ï¼ˆé»˜è®¤çš„ 300 ç§’å¯¹å¤§å¤šæ•° API æ¥è¯´å¤ªé•¿ï¼‰ã€‚
2. å¯¹äºå¤–éƒ¨ APIï¼Œå¯ç”¨æŒ‡æ•°çº§é€€é¿çš„é‡è¯•æœºåˆ¶ã€‚
3. ä½¿ç”¨å‡­è¯å­˜å‚¨åº“â€”â€”åˆ‡å‹¿åœ¨ URL æˆ–è¯·æ±‚å¤´ä¸­ç¡¬ç¼–ç  API å¯†é’¥ã€‚
4. ä¸ºæ¥æ”¶ç«¯è®¾ç½® User-Agent ä»¥æ–¹ä¾¿è°ƒè¯•ã€‚
5. ä½¿ç”¨ `$env.VARIABLE` æ¥ç®¡ç†åŸºç¡€ URLâ€”â€”åˆ‡å‹¿ç¡¬ç¼–ç åŸŸåã€‚
6. å½“éœ€è¦æ ¹æ®çŠ¶æ€ç è¿›è¡Œåˆ†æ”¯å¤„ç†æ—¶ï¼Œä½¿ç”¨å…¨å“åº”æ¨¡å¼ã€‚

### 4.2 æ•°æ®èŠ‚ç‚¹ â€” æ•°æ®è½¬æ¢æ¨¡å¼

**æ¨¡å¼ï¼šæ˜ å°„ä¸è½¬æ¢**  
```javascript
// Transform array of items
return items.map(item => {
  const data = item.json;
  return {
    json: {
      id: data.id,
      fullName: `${data.first_name} ${data.last_name}`.trim(),
      email: data.email?.toLowerCase(),
      createdAt: new Date(data.created_at).toISOString(),
      source: 'n8n-sync',
      // Computed fields
      isActive: data.status === 'active',
      daysSinceSignup: Math.floor(
        (Date.now() - new Date(data.created_at)) / 86400000
      ),
    }
  };
});
```

**æ¨¡å¼ï¼šè¿‡æ»¤ä¸å»é‡**  
```javascript
const seen = new Set();
return items.filter(item => {
  const key = item.json.email?.toLowerCase();
  if (!key || seen.has(key)) return false;
  seen.add(key);
  return true;
});
```

**æ¨¡å¼ï¼šèšåˆ/åˆ†ç»„**  
```javascript
const groups = {};
for (const item of items) {
  const key = item.json.category;
  if (!groups[key]) groups[key] = { count: 0, total: 0, items: [] };
  groups[key].count++;
  groups[key].total += item.json.amount || 0;
  groups[key].items.push(item.json);
}
return Object.entries(groups).map(([category, data]) => ({
  json: { category, ...data, average: data.total / data.count }
}));
```

**æ¨¡å¼ï¼šåˆ†é¡µå¤„ç†**  
```javascript
// Use with Loop Over Items or recursive sub-workflow
const baseUrl = $env.API_BASE_URL;
const results = [];
let page = 1;
let hasMore = true;

while (hasMore) {
  const response = await this.helpers.httpRequest({
    method: 'GET',
    url: `${baseUrl}/items?page=${page}&per_page=100`,
    headers: { 'Authorization': `Bearer ${$env.API_TOKEN}` },
  });
  
  results.push(...response.data);
  hasMore = response.data.length === 100;
  page++;
  
  // Safety valve
  if (page > 50) break;
}

return results.map(item => ({ json: item }));
```

**æ¨¡å¼ï¼šé€Ÿç‡é™åˆ¶**  
```javascript
// Add between batch items to respect API limits
const RATE_LIMIT_MS = 200; // 5 requests per second
const itemIndex = $itemIndex || 0;

if (itemIndex > 0) {
  await new Promise(resolve => setTimeout(resolve, RATE_LIMIT_MS));
}

return items;
```

### 4.3 åˆ†æ”¯æ¨¡å¼

**IF èŠ‚ç‚¹ â€” å†³ç­–é€»è¾‘**  
```yaml
branching_patterns:
  binary_decision:
    node: "IF"
    use: "True/false routing"
    example: "Is order amount > $100?"
    
  multi_path:
    node: "Switch"
    use: "3+ possible routes"
    example: "Route by ticket priority (P0/P1/P2/P3)"
    
  content_routing:
    node: "Switch"
    use: "Route by data content/type"
    example: "Route by email domain to different CRMs"
    
  merge_paths:
    node: "Merge"
    mode: "chooseBranch"
    use: "Rejoin after IF/Switch branches"
```

**Switch èŠ‚ç‚¹ â€” æ¸…æ™°çš„å¤šè·¯è·¯ç”±**  
```
Switch on: {{ $json.status }}
  Case "new"      â†’ Create record path
  Case "updated"  â†’ Update record path  
  Case "deleted"  â†’ Archive record path
  Default         â†’ Log unknown status + alert
```

### 4.4 å¾ªç¯æ¨¡å¼

**åˆ†æ‰¹å¤„ç†**  
```yaml
batch_processing:
  node: "Split In Batches"
  batch_size: 10
  use_cases:
    - "API with rate limits (process 10, wait, next 10)"
    - "Database bulk inserts (batch of 100)"
    - "Email sending (batch of 50 to avoid spam filters)"
  
  pattern:
    1: "Split In Batches (size: 10)"
    2: "â†’ Process batch (HTTP Request / DB insert)"
    3: "â†’ Wait (1 second between batches)"
    4: "â†’ Loop back to Split In Batches"
```

**é€é¡¹å¤„ç†**  
```yaml
per_item_loop:
  node: "Loop Over Items"
  use_cases:
    - "Each item needs different API call"
    - "Sequential processing required (order matters)"
    - "Per-item error handling needed"
  
  anti_pattern: "Don't loop when batch/bulk API exists"
```

---

## ç¬¬äº”é˜¶æ®µï¼šé”™è¯¯å¤„ç†æ¶æ„

**æ¯ä¸ªç”Ÿäº§çº§å·¥ä½œæµéƒ½å¿…é¡»å…·å¤‡ä»¥ä¸‹é”™è¯¯å¤„ç†æœºåˆ¶ï¼š**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MAIN WORKFLOW                                   â”‚
â”‚                                                  â”‚
â”‚  Trigger â†’ Process â†’ Output                      â”‚
â”‚     â”‚                                            â”‚
â”‚     â””â”€â”€â”€ Error Trigger â”€â”€â†’ Error Handler â”€â”€â†’     â”‚
â”‚              â”‚                                   â”‚
â”‚              â”œâ”€â”€ Log error details                â”‚
â”‚              â”œâ”€â”€ Send alert (Slack/email)         â”‚
â”‚              â”œâ”€â”€ Retry logic (if applicable)      â”‚
â”‚              â””â”€â”€ Dead letter queue (if needed)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 é”™è¯¯è§¦å‘å™¨æ¨¡æ¿  

```yaml
error_workflow:
  nodes:
    - name: "Error Trigger"
      type: "n8n-nodes-base.errorTrigger"
      
    - name: "Extract Error Info"
      type: "n8n-nodes-base.code"
      code: |
        const error = $json;
        return [{
          json: {
            workflow_name: error.workflow?.name || 'Unknown',
            workflow_id: error.workflow?.id,
            execution_id: error.execution?.id,
            error_message: error.execution?.error?.message || 'No message',
            error_node: error.execution?.error?.node || 'Unknown node',
            timestamp: new Date().toISOString(),
            retry_url: `${$env.N8N_BASE_URL}/workflow/${error.workflow?.id}/executions/${error.execution?.id}`,
            severity: classifySeverity(error),
          }
        }];
        
        function classifySeverity(error) {
          const msg = error.execution?.error?.message || '';
          if (msg.includes('timeout') || msg.includes('ECONNREFUSED')) return 'WARNING';
          if (msg.includes('401') || msg.includes('403')) return 'CRITICAL';
          if (msg.includes('429')) return 'INFO'; // Rate limit, will retry
          return 'ERROR';
        }
        
    - name: "Alert via Slack"
      type: "n8n-nodes-base.slack"
      action: "Send message"
      channel: "#n8n-alerts"
      message: |
        ğŸš¨ *n8n Workflow Error*
        
        *Workflow:* {{ $json.workflow_name }}
        *Node:* {{ $json.error_node }}
        *Severity:* {{ $json.severity }}
        *Error:* {{ $json.error_message }}
        *Time:* {{ $json.timestamp }}
        
        <{{ $json.retry_url }}|View Execution>
```

### 5.3 é‡è¯•æ¨¡å¼  

```yaml
retry_strategies:
  http_retry:
    description: "Built-in HTTP Request retry"
    config:
      max_retries: 3
      retry_interval: 1000  # ms
      retry_on_timeout: true
      retry_on_status: [429, 500, 502, 503, 504]
    
  custom_retry_with_backoff:
    description: "Code node implementing exponential backoff"
    pattern: |
      const maxRetries = 3;
      const attempt = $json._retryAttempt || 0;
      
      if (attempt >= maxRetries) {
        // Send to dead letter queue
        return [{ json: { ...item.json, _failed: true, _attempts: attempt } }];
      }
      
      const delay = Math.pow(2, attempt) * 1000; // 1s, 2s, 4s
      await new Promise(r => setTimeout(r, delay));
      
      return [{ json: { ...item.json, _retryAttempt: attempt + 1 } }];
      
  circuit_breaker:
    description: "Stop calling failing service"
    pattern: |
      // Use n8n static data as circuit state
      const staticData = $getWorkflowStaticData('global');
      const failures = staticData.failures || 0;
      const lastFailure = staticData.lastFailure || 0;
      const THRESHOLD = 5;
      const COOLDOWN_MS = 300000; // 5 minutes
      
      if (failures >= THRESHOLD && Date.now() - lastFailure < COOLDOWN_MS) {
        // Circuit OPEN â€” skip API call, use fallback
        return [{ json: { _circuitOpen: true, _fallback: true } }];
      }
```

### 5.4 æ­»ä¿¡é˜Ÿåˆ—æ¨¡å¼  

```yaml
dead_letter_queue:
  purpose: "Store failed items for manual review/reprocessing"
  implementation:
    - node: "Google Sheets / Airtable / Database"
      columns: [workflow, execution_id, item_data, error, timestamp, status]
    - status_values: [pending, retrying, resolved, abandoned]
    - review: "Check DLQ daily, resolve or abandon stale items"
```

---

## ç¬¬å…­é˜¶æ®µï¼šæ•°æ®è½¬æ¢ä¸é›†æˆæ¨¡å¼

### 6.1 å¸¸è§é›†æˆæ¨¡å¼

**æ¨¡å¼ï¼šCRM æ•°æ®åŒæ­¥ï¼ˆåŒå‘ï¼‰**  
```yaml
crm_sync:
  inbound:
    trigger: "Webhook from CRM (new/updated contact)"
    steps:
      1: "Validate payload schema"
      2: "Map fields to internal format"
      3: "Deduplicate (check by email)"
      4: "Upsert to database"
      5: "Trigger downstream workflows"
      
  outbound:
    trigger: "Database change or schedule"
    steps:
      1: "Query changed records since last sync"
      2: "Map internal format to CRM fields"
      3: "Batch upsert to CRM API"
      4: "Store sync timestamp"
      5: "Log sync results"
      
  conflict_resolution:
    strategy: "Last write wins with audit trail"
    timestamp_field: "updated_at"
    audit: "Log both versions before overwrite"
```

**æ¨¡å¼ï¼šç”µå­é‚®ä»¶å¤„ç†æµç¨‹**  
```yaml
email_pipeline:
  trigger: "IMAP Email (polling every 5 min)"
  steps:
    1: "Read new emails"
    2: "Classify intent (AI/rules)"
    3: "Extract structured data (sender, subject, key fields)"
    4: "Route by classification"
    5_support: "Create ticket in helpdesk"
    5_sales: "Add to CRM as lead"
    5_billing: "Forward to accounting"
    5_spam: "Archive and skip"
    6: "Send auto-acknowledgment"
    7: "Log to audit trail"
```

**æ¨¡å¼ï¼šå¤šæ­¥éª¤å®¡æ‰¹**  
```yaml
approval_workflow:
  trigger: "Form/webhook (new request)"
  steps:
    1: "Create request record (status: pending)"
    2: "Send Slack message with Approve/Reject buttons"
    3: "Wait for webhook callback (button click)"
    4_approved: "Execute action + notify requester"
    4_rejected: "Notify requester with reason"
    5: "Update request status"
    6: "Log to audit trail"
  timeout: "48 hours â†’ auto-escalate to manager"
```

**æ¨¡å¼ï¼šåŸºäº AI çš„å¤„ç†**  
```yaml
ai_pipeline:
  trigger: "Webhook or schedule"
  steps:
    1: "Receive raw data (text, email, document)"
    2: "Pre-process (clean, chunk if needed)"
    3: "Send to LLM (OpenAI/Anthropic/local)"
    4: "Parse structured response"
    5: "Validate LLM output (check required fields, format)"
    6: "Route based on classification"
    7: "Human review if confidence < threshold"
    8: "Store result + feedback for improvement"
  
  llm_node_config:
    model: "gpt-4o-mini for classification, gpt-4o for generation"
    temperature: 0 for extraction/classification, 0.7 for generation
    max_tokens: "Set explicit limit to control cost"
    system_prompt: "Be specific. Include output format. Add examples."
    
  cost_control:
    - "Use cheapest model that achieves accuracy target"
    - "Cache repeated queries (check before calling LLM)"
    - "Batch similar items into single LLM call when possible"
    - "Track cost per execution in workflow metrics"
```

### 6.2 æ•°æ®æ˜ å°„å‚è€ƒè¡¨  

```javascript
// Common field mapping patterns in Code nodes

// Dates â€” always normalize to ISO
const isoDate = new Date(data.date_field).toISOString();
const dateOnly = new Date(data.date_field).toISOString().split('T')[0];

// Names
const fullName = `${data.firstName || ''} ${data.lastName || ''}`.trim();
const [firstName, ...rest] = data.fullName.split(' ');
const lastName = rest.join(' ');

// Currency â€” always store as cents/minor units
const amountCents = Math.round(parseFloat(data.amount) * 100);
const amountDisplay = (data.amount_cents / 100).toFixed(2);

// Phone â€” normalize
const phone = data.phone?.replace(/\D/g, '');

// Email â€” normalize
const email = data.email?.toLowerCase().trim();

// Null safety
const value = data.field ?? 'default';
const nested = data.parent?.child?.value ?? null;

// Array handling
const tags = Array.isArray(data.tags) ? data.tags : [data.tags].filter(Boolean);
const csvToArray = data.csv_field?.split(',').map(s => s.trim()) || [];
const arrayToCsv = data.array_field?.join(', ') || '';
```

---

## ç¬¬ä¸ƒé˜¶æ®µï¼šå­å·¥ä½œæµæ¶æ„

### 7.1 ä½•æ—¶æå–å­å·¥ä½œæµ

| é€‚ç”¨åœºæ™¯ | å¤„ç†æ–¹å¼ |
|--------|--------|
| å¤šä¸ªå·¥ä½œæµä¸­æœ‰ç›¸åŒçš„é€»è¾‘ | æå–ä¸ºå­å·¥ä½œæµ |
| å·¥ä½œæµèŠ‚ç‚¹è¶…è¿‡ 30 ä¸ª | åˆ†è§£ä¸ºä¸»å·¥ä½œæµå’Œå­å·¥ä½œæµ |
| éœ€è¦ä¸åŒçš„é”™è¯¯å¤„ç†æ–¹å¼ | åˆ†åˆ«å¤„ç†ä¸åŒçš„é”™è¯¯æƒ…å†µ |
| å›¢é˜Ÿå¸Œæœ›é‡ç”¨æŸä¸ªå¤„ç†æµç¨‹ | å°†å…¶è®¾è®¡ä¸ºå¯è°ƒç”¨çš„å­å·¥ä½œæµ |
| éœ€è¦ç‹¬ç«‹æµ‹è¯•æŸä¸ªéƒ¨åˆ† | å•ç‹¬æå–å¹¶æµ‹è¯•è¯¥éƒ¨åˆ† |

### 7.2 å­å·¥ä½œæµè®¾è®¡è§„åˆ™  

```yaml
sub_workflow_rules:
  naming: "[SUB] Description â€” Input/Output"
  interface:
    - "Define clear input schema (what data it expects)"
    - "Define clear output schema (what it returns)"
    - "Document side effects (external API calls, DB writes)"
  
  input_validation:
    - "First node: validate required fields exist"
    - "Return clear error if validation fails"
    
  output_contract:
    - "Always return consistent structure"
    - "Include success/failure status"
    - "Include execution metadata (duration, items processed)"
    
  example_output:
    success: true
    items_processed: 42
    errors: []
    duration_ms: 1234
```

### 7.3 ç¼–æ’å™¨æ¨¡å¼  

```
[PROCESS] Order Fulfillment â€” Orchestrator (v1.0)
  â”‚
  â”œâ”€â”€ [SUB] Validate Order â€” Input Check
  â”‚     â””â”€â”€ Returns: { valid: true/false, errors: [] }
  â”‚
  â”œâ”€â”€ [SUB] Check Inventory â€” Stock Verification  
  â”‚     â””â”€â”€ Returns: { inStock: true/false, items: [] }
  â”‚
  â”œâ”€â”€ [SUB] Process Payment â€” Stripe Charge
  â”‚     â””â”€â”€ Returns: { charged: true/false, chargeId: "" }
  â”‚
  â”œâ”€â”€ [SUB] Create Shipment â€” Shipping Label
  â”‚     â””â”€â”€ Returns: { trackingNumber: "", labelUrl: "" }
  â”‚
  â””â”€â”€ [SUB] Send Confirmations â€” Email + SMS
        â””â”€â”€ Returns: { emailSent: true, smsSent: true }

Orchestrator handles:
  - Sequential execution order
  - Rollback on failure (reverse previous steps)
  - Status tracking (store state between steps)
  - Timeout management (overall SLA)
```

---

## ç¬¬å…«é˜¶æ®µï¼šn8n é™æ€æ•°æ®ä¸çŠ¶æ€ç®¡ç†

### 8.1 é™æ€æ•°æ®æ¨¡å¼  

```javascript
// Global static data (persists across executions)
const staticData = $getWorkflowStaticData('global');

// Pattern: Last processed ID (for incremental sync)
const lastId = staticData.lastProcessedId || 0;
// ... process items where id > lastId ...
staticData.lastProcessedId = maxProcessedId;

// Pattern: Rate limit tracking
staticData.apiCalls = (staticData.apiCalls || 0) + 1;
staticData.windowStart = staticData.windowStart || Date.now();
if (Date.now() - staticData.windowStart > 3600000) {
  staticData.apiCalls = 1;
  staticData.windowStart = Date.now();
}

// Pattern: Deduplication cache
const cache = staticData.processedIds || {};
const newItems = items.filter(item => {
  if (cache[item.json.id]) return false;
  cache[item.json.id] = Date.now();
  return true;
});
// Prune cache entries older than 24h
for (const [id, ts] of Object.entries(cache)) {
  if (Date.now() - ts > 86400000) delete cache[id];
}
staticData.processedIds = cache;
```

### 8.2 å½“é™æ€æ•°æ®ä¸è¶³æ—¶  

```yaml
state_management:
  static_data:
    capacity: "~1MB per workflow"
    persistence: "Survives restarts"
    use_for: "Counters, last-processed IDs, small caches"
    dont_use_for: "Large datasets, shared state between workflows"
    
  database:
    use_for: "Shared state, large datasets, audit trails"
    options: ["Postgres", "SQLite", "Redis"]
    pattern: "Read state â†’ Process â†’ Write state (in same execution)"
    
  google_sheets:
    use_for: "Human-readable state, manual override capability"
    pattern: "Config sheet = feature flags, processing rules"
    
  redis:
    use_for: "High-speed counters, distributed locks, pub/sub"
    pattern: "Rate limiting, dedup across multiple workflows"
```

---

## ç¬¬ä¹é˜¶æ®µï¼šå®‰å…¨ä¸å‡­è¯ç®¡ç†

### 9.1 å‡­æ®ç®¡ç†è§„åˆ™  

```yaml
credential_rules:
  DO:
    - "Use n8n Credential Store for ALL secrets"
    - "Use environment variables for config (URLs, feature flags)"
    - "Rotate API keys on schedule (quarterly minimum)"
    - "Use OAuth2 over API keys when available"
    - "Limit credential scope (least privilege)"
    - "Audit credential usage quarterly"
    
  NEVER:
    - "Hardcode secrets in Code nodes"
    - "Put API keys in webhook URLs"
    - "Log full request/response bodies (may contain secrets)"
    - "Share credentials between dev/staging/prod"
    - "Use personal API keys for production workflows"
```

### 9.2 Webhook å®‰å…¨æ€§å®ç°  

```javascript
// HMAC signature verification (Stripe, GitHub, etc.)
const crypto = require('crypto');

const signature = $request.headers['x-hub-signature-256'];
const secret = $env.WEBHOOK_SECRET;
const body = JSON.stringify($json);

const expected = 'sha256=' + crypto
  .createHmac('sha256', secret)
  .update(body)
  .digest('hex');

if (signature !== expected) {
  // Return 401 via Respond to Webhook node
  return [{ json: { error: 'Invalid signature', _reject: true } }];
}

return items;
```

### 9.3 æ•°æ®éšç§æ£€æŸ¥  

```yaml
privacy_checklist:
  pii_handling:
    - "Identify PII fields in every workflow (email, name, phone, IP)"
    - "Minimize PII: only pass fields actually needed"
    - "Mask PII in logs (email â†’ j***@example.com)"
    - "Set execution data pruning (don't keep PII forever)"
    
  execution_data:
    - "Save execution data: Only on error (production)"
    - "Save execution data: Always (development only)"
    - "Prune executions older than 30 days"
    - "Don't store full response bodies from external APIs"
    
  compliance:
    - "GDPR: Can you delete a user's data from all workflow states?"
    - "Audit trail: Can you prove what data was processed and when?"
    - "Data residency: Are API calls going to correct region?"
```

---

## ç¬¬åé˜¶æ®µï¼šæ€§èƒ½ä¸ä¼˜åŒ–

### 10.1 æ€§èƒ½ä¼˜åŒ–ä¼˜å…ˆçº§

| ä¼˜å…ˆçº§ | æŠ€æœ¯æ‰‹æ®µ | å½±å“æ•ˆæœ |
|----------|-----------|--------|
| 1 | æ‰¹é‡ API è°ƒç”¨ï¼ˆæ‰¹é‡å¤„ç†ï¼‰ | å‡å°‘ API è°ƒç”¨æ¬¡æ•°ï¼ˆ10-100 å€ï¼‰ |
| 2 | å¹¶è¡Œæ‰§è¡Œï¼ˆæ‹†åˆ†å’Œåˆå¹¶ï¼‰ | æé«˜å¤„ç†é€Ÿåº¦ï¼ˆ2-5 å€ï¼‰ |
| 3 | æå‰è¿‡æ»¤æ•°æ®ï¼ˆåœ¨å¤æ‚å¤„ç†å‰å‰”é™¤æ— æ•ˆæ•°æ®ï¼‰ | å‡å°‘è®¡ç®—è´Ÿæ‹… |
| 4 | ç¼“å­˜é‡å¤æŸ¥è¯¢ï¼ˆå°¤å…¶æ˜¯é™æ€æ•°æ®ï¼‰ | å‡å°‘ API è°ƒç”¨æ¬¡æ•° |
| 5 | å‡å°‘èŠ‚ç‚¹é—´ä¼ é€’çš„æ•°æ®é‡ | é™ä½å†…å­˜æ¶ˆè€— |
| 6 | ä½¿ç”¨å­å·¥ä½œæµå¤„ç†å¤æ‚éƒ¨åˆ† | æ›´å¥½åœ°ç®¡ç†èµ„æº |
| 7 | åœ¨éé«˜å³°æ—¶æ®µæ‰§è¡Œä»»åŠ¡ | å‡å°‘ç³»ç»Ÿç«äº‰ |
| 8 | ä¼˜åŒ–ä»£ç èŠ‚ç‚¹ç®—æ³• | é™ä½ CPU ä½¿ç”¨æ—¶é—´ |

### 10.2 æ‰¹é‡å¤„ç†æ¨¡æ¿  

```yaml
batch_template:
  step_1: "Collect all items (trigger / query)"
  step_2: "Split In Batches (size based on API limit)"
  step_3: "Process batch (use bulk/batch API endpoint)"
  step_4: "Wait node (respect rate limit between batches)"
  step_5: "Aggregate results"
  step_6: "Report summary"
  
  sizing_guide:
    stripe_api: 100  # Stripe list limit
    hubspot_api: 100  # HubSpot batch limit
    postgres_insert: 1000  # Comfortable batch insert
    email_send: 50  # Avoid spam filters
    slack_api: 20  # Rate limit friendly
    openai_api: 1  # Usually per-request
```

### 10.3 å†…å­˜ä¼˜åŒ–  

```javascript
// Anti-pattern: Passing full objects through entire workflow
// âŒ BAD
return items; // Each item has 50 fields, only need 3

// âœ… GOOD: Extract only needed fields early
return items.map(item => ({
  json: {
    id: item.json.id,
    email: item.json.email,
    status: item.json.status,
  }
}));

// Anti-pattern: Accumulating in memory
// âŒ BAD: Loading 100K records into Code node
// âœ… GOOD: Use database queries with LIMIT/OFFSET, process in batches
```

---

## ç¬¬åä¸€é˜¶æ®µï¼šæµ‹è¯•ä¸è°ƒè¯•

### 11.1 æµ‹è¯•æ–¹æ³•è®º  

```yaml
testing_levels:
  unit_test:
    what: "Individual nodes with sample data"
    how: "Pin test data on trigger node, execute single node"
    when: "Building each node"
    
  integration_test:
    what: "Full workflow with test data"
    how: "Manual trigger with test payload, verify all outputs"
    when: "Before activating"
    
  smoke_test:
    what: "Quick check that workflow still works"
    how: "Trigger with minimal valid payload, check success"
    when: "After any change, weekly health check"
    
  load_test:
    what: "Performance under volume"
    how: "Send 100+ items through, measure time and errors"
    when: "Before scaling to production volume"
```

### 11.2 è°ƒè¯•æ£€æŸ¥æ¸…å•  

```yaml
debugging_steps:
  1_reproduce:
    - "Find the failed execution in execution list"
    - "Check which node failed (red highlight)"
    - "Read the error message carefully"
    
  2_inspect:
    - "Check input data to failed node (is it what you expected?)"
    - "Check node configuration (expressions resolving correctly?)"
    - "Check credentials (still valid? permissions?)"
    
  3_common_fixes:
    expression_error: "Wrap in try/catch or use ?? for null safety"
    timeout: "Increase timeout, check if API is actually up"
    auth_error: "Re-authenticate credential, check token expiry"
    rate_limit: "Add Wait node, reduce batch size"
    json_parse: "Check response is actually JSON (not HTML error page)"
    missing_field: "Data shape changed â€” update field mapping"
    
  4_isolate:
    - "Pin input data on the failing node"
    - "Execute just that node"
    - "If it works in isolation, problem is upstream data"
```

### 11.3 ç›‘æ§ä»ªè¡¨ç›˜  

```yaml
monitoring:
  metrics_to_track:
    - name: "Execution success rate"
      target: ">99%"
      alert_threshold: "<95%"
      
    - name: "Average execution time"
      target: "Under SLA"
      alert_threshold: ">2x normal"
      
    - name: "Items processed per run"
      target: "Expected range"
      alert_threshold: "0 items (nothing processed) or >10x normal"
      
    - name: "Error frequency by type"
      target: "Decreasing trend"
      alert_threshold: "Same error >3 times in 24h"
      
    - name: "API quota usage"
      target: "<80% of limit"
      alert_threshold: ">90% of limit"
      
  health_check_workflow:
    schedule: "Every 30 minutes"
    checks:
      - "Can reach external APIs? (HEAD request)"
      - "Database connection alive?"
      - "Disk space for execution data?"
      - "Any workflows stuck in 'running' >1 hour?"
    alert_channel: "Slack #n8n-alerts"
```

---

## ç¬¬åäºŒé˜¶æ®µï¼šç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ä¸ç»´æŠ¤

### 12.1 éƒ¨ç½²æ£€æŸ¥æ¸…å•  

```yaml
pre_activation:
  workflow:
    - [ ] "Workflow description filled in (purpose, owner, version)"
    - [ ] "All nodes named descriptively (not 'HTTP Request 1')"
    - [ ] "Sticky notes explain complex sections"
    - [ ] "Error trigger workflow connected"
    - [ ] "Test data pins removed"
    - [ ] "No hardcoded secrets or URLs"
    - [ ] "Environment variables used for config"
    
  testing:
    - [ ] "Happy path tested with real-shape data"
    - [ ] "Error paths tested (bad data, API failure, timeout)"
    - [ ] "Edge cases tested (empty array, null fields, special chars)"
    - [ ] "Load tested at expected volume"
    
  operations:
    - [ ] "Execution data retention configured"
    - [ ] "Alert channel receiving error notifications"
    - [ ] "Runbook written for common failure scenarios"
    - [ ] "Owner documented (who to page at 3 AM)"
```

### 12.2 å·¥ä½œæµç‰ˆæœ¬æ§åˆ¶ç­–ç•¥  

```yaml
versioning:
  format: "vMAJOR.MINOR (in workflow name + description)"
  
  major_bump: "Breaking changes â€” new trigger, changed output format"
  minor_bump: "Improvements â€” new fields, better error handling"
  
  changelog_location: "Workflow description field"
  changelog_format: |
    ## v2.1 (2024-03-15)
    - Added retry logic for Stripe API calls
    - Fixed timezone conversion for EU customers
    
    ## v2.0 (2024-02-01)
    - Migrated from REST to GraphQL API
    - Breaking: output format changed
    
  backup_strategy:
    - "Export workflow JSON before major changes"
    - "Store in git repo: workflows/[category]/[name].json"
    - "Tag with version: git tag workflow-name-v2.1"
```

### 12.3 ç»´æŠ¤è®¡åˆ’  

```yaml
maintenance:
  daily:
    - "Check error notifications channel"
    - "Review failed executions (>0 = investigate)"
    
  weekly:
    - "Review execution volume trends"
    - "Check API quota usage"
    - "Process dead letter queue items"
    
  monthly:
    - "Review and prune old executions"
    - "Audit credential usage"
    - "Update workflow documentation"
    - "Review performance (any slow workflows?)"
    
  quarterly:
    - "Rotate API keys and tokens"
    - "Review all active workflows â€” still needed?"
    - "Update n8n version (test in staging first)"
    - "Archive unused workflows"
```

---

## ç¬¬åä¸‰é˜¶æ®µï¼šå®Œæ•´çš„å·¥ä½œæµæ¨¡æ¿

### 13.1 æ¨¡æ¿ï¼šå®¢æˆ·ä¿¡æ¯æ”¶é›† â†’ CRM â†’ é€šçŸ¥  

```yaml
name: "[INGEST] Web Lead â†’ HubSpot + Slack Alert (v1.0)"
trigger: Webhook (form submission)
nodes:
  1_webhook:
    type: Webhook
    path: "/lead-capture"
    method: POST
    response: "Respond to Webhook (immediate 200)"
    
  2_validate:
    type: IF
    condition: "email exists AND email contains @"
    false_path: "â†’ Log invalid submission â†’ End"
    
  3_enrich:
    type: HTTP Request
    url: "Clearbit/Apollo enrichment API"
    fallback: "Continue without enrichment"
    
  4_dedupe:
    type: Code
    logic: "Check HubSpot for existing contact by email"
    
  5_create_or_update:
    type: HubSpot
    action: "Create/update contact"
    fields: [email, name, company, source, enrichment_data]
    
  6_notify:
    type: Slack
    channel: "#sales-leads"
    message: "ğŸ¯ New lead: {name} from {company} â€” {source}"
    
  7_auto_reply:
    type: Email (SMTP)
    to: "{{ $json.email }}"
    template: "Thanks for your interest, we'll be in touch within 24h"
```

### 13.2 æ¨¡æ¿ï¼šå®šæ—¶æŠ¥å‘Šç”Ÿæˆå™¨  

```yaml
name: "[EXPORT] Weekly Sales Report â€” Email (v1.0)"
trigger: Schedule (Monday 8 AM)
nodes:
  1_schedule:
    type: Schedule Trigger
    cron: "0 8 * * 1"
    
  2_query_data:
    type: Postgres
    query: |
      SELECT 
        date_trunc('day', created_at) as day,
        COUNT(*) as deals,
        SUM(amount) as revenue,
        AVG(amount) as avg_deal
      FROM deals 
      WHERE created_at >= NOW() - INTERVAL '7 days'
      GROUP BY 1 ORDER BY 1
      
  3_calculate_summary:
    type: Code
    logic: "Calculate totals, WoW change, top deals"
    
  4_format_report:
    type: Code
    logic: "Generate HTML email body with tables and charts links"
    
  5_send_email:
    type: Email (SMTP)
    to: "sales-team@company.com"
    subject: "ğŸ“Š Weekly Sales Report â€” W{{ weekNumber }}"
    html: "{{ $json.reportHtml }}"
```

### 13.3 æ¨¡æ¿ï¼šAI æ”¯æŒå·¥å•åˆ†ç±»å™¨  

```yaml
name: "[AI] Support Ticket â€” Classify + Route (v1.0)"
trigger: Webhook (helpdesk new ticket)
nodes:
  1_webhook:
    type: Webhook
    
  2_classify:
    type: OpenAI Chat
    model: "gpt-4o-mini"
    system: |
      Classify this support ticket. Return JSON:
      {
        "category": "bug|feature_request|billing|how_to|account|other",
        "priority": "P0|P1|P2|P3",
        "sentiment": "angry|frustrated|neutral|positive",
        "summary": "one sentence summary",
        "suggested_response": "draft response"
      }
    temperature: 0
    
  3_parse:
    type: Code
    logic: "JSON.parse response, validate required fields"
    
  4_route:
    type: Switch
    on: "{{ $json.category }}"
    cases:
      bug: "â†’ Assign to engineering team"
      billing: "â†’ Assign to finance team"
      feature_request: "â†’ Add to product backlog"
      default: "â†’ Assign to general support"
      
  5_priority_alert:
    type: IF
    condition: "priority == P0"
    true_path: "â†’ Slack alert to on-call"
    
  6_update_ticket:
    type: HTTP Request
    action: "Update ticket with classification tags"
    
  7_auto_respond:
    type: IF
    condition: "category == how_to AND confidence > 0.9"
    true_path: "â†’ Send suggested_response as reply"
    false_path: "â†’ Save draft for human review"
```

### 13.4 æ¨¡æ¿ï¼šå¤šç³»ç»Ÿæ•°æ®åŒæ­¥  

```yaml
name: "[SYNC] Stripe â†’ Postgres â†’ HubSpot â€” Payments (v1.0)"
trigger: Webhook (Stripe payment_intent.succeeded)
nodes:
  1_webhook:
    type: Webhook
    security: "HMAC signature verification"
    
  2_verify_signature:
    type: Code
    logic: "Stripe HMAC verification"
    
  3_extract_payment:
    type: Code
    logic: "Extract customer, amount, metadata from Stripe event"
    
  4_upsert_db:
    type: Postgres
    action: "INSERT ON CONFLICT UPDATE"
    table: "payments"
    
  5_update_crm:
    type: HubSpot
    action: "Update deal stage to 'Closed Won'"
    
  6_notify_team:
    type: Slack
    message: "ğŸ’° Payment received: ${{ amount }} from {{ customer }}"
    
  7_send_receipt:
    type: Email (SMTP)
    to: "{{ customer_email }}"
    template: "Payment confirmation"
```

---

## ç¬¬åå››é˜¶æ®µï¼šé«˜çº§æ¨¡å¼

### 14.1 å¹¶è¡Œå¤„ç†ï¼ˆæ‰‡å‡º/æ‰‡å…¥ï¼‰  

```yaml
pattern: "Split work across parallel paths, merge results"
use_case: "Enrich contacts from 3 APIs simultaneously"
implementation:
  1: "Trigger with batch of contacts"
  2: "Split into 3 parallel HTTP Request nodes"
  3: "Each calls different API (Clearbit, Apollo, LinkedIn)"
  4: "Merge node (Combine mode) joins results"
  5: "Code node merges enrichment data per contact"
  
benefit: "3x faster than sequential API calls"
caveat: "All 3 branches must handle their own errors"
```

### 14.2 åŸºäºäº‹ä»¶çš„æ¶æ„  

```yaml
pattern: "Workflows trigger other workflows via internal webhooks"
implementation:
  producer: |
    [PROCESS] Order Created
    â†’ Process order
    â†’ HTTP Request to internal webhook: /event/order-created
    
  consumers:
    - "[NOTIFY] Order Confirmation â†’ Email"
    - "[SYNC] Order â†’ Inventory Update"  
    - "[SYNC] Order â†’ Accounting System"
    - "[AI] Order â†’ Fraud Detection"
    
benefit: "Loose coupling â€” add new consumers without changing producer"
caveat: "Need to handle consumer failures independently"
```

### 14.3 ç‰¹æ€§å¼€å…³æ¨¡å¼  

```yaml
pattern: "Control workflow behavior without editing"
implementation:
  config_source: "Google Sheet or database table"
  columns: [feature_name, enabled, percentage, notes]
  
  in_workflow:
    1: "Read config at start of workflow"
    2: "IF node checks feature flag"
    3: "true â†’ new behavior, false â†’ old behavior"
    
  examples:
    - feature: "use_gpt4o_mini"
      check: "Route to cheaper model when enabled"
    - feature: "skip_enrichment"
      check: "Bypass API calls during outage"
    - feature: "double_check_mode"
      check: "Add human approval step"
```

### 14.4 é«˜æµé‡å¤„ç†æ¨¡å¼  

```yaml
pattern: "Buffer incoming items, process at controlled rate"
use_case: "1000 webhook events/minute, API limit 10/minute"
implementation:
  ingestion_workflow:
    1: "Webhook receives event"
    2: "Write to queue (database table: status=pending)"
    3: "Return 200 immediately"
    
  processing_workflow:
    1: "Schedule trigger (every minute)"
    2: "Query: SELECT * FROM queue WHERE status='pending' LIMIT 10"
    3: "Process batch"
    4: "UPDATE status='completed'"
    5: "On error: UPDATE status='failed', retry_count++"
    
benefit: "Never lose events, process at sustainable rate"
```

---

## ç¬¬åäº”é˜¶æ®µï¼šn8n å®ä¾‹ç®¡ç†

### 15.1 ç¯å¢ƒç­–ç•¥  

```yaml
environments:
  development:
    purpose: "Building and testing new workflows"
    data: "Test/mock data only"
    execution_saving: "All executions"
    
  staging:
    purpose: "Pre-production validation"
    data: "Anonymized production-like data"
    execution_saving: "All executions"
    
  production:
    purpose: "Live workflows"
    data: "Real data"
    execution_saving: "Errors only (save disk)"
    
  promotion_process:
    1: "Build in dev"
    2: "Export workflow JSON"
    3: "Import to staging, test with realistic data"
    4: "Export again (staging may have fixes)"
    5: "Import to production"
    6: "Activate and monitor first 24h"
```

### 15.2 n8n æ€§èƒ½è°ƒä¼˜  

```yaml
tuning:
  execution_mode: "queue"  # For high volume (requires Redis)
  
  environment_variables:
    EXECUTIONS_DATA_SAVE_ON_ERROR: "all"
    EXECUTIONS_DATA_SAVE_ON_SUCCESS: "none"  # Save disk in production
    EXECUTIONS_DATA_SAVE_MANUAL_EXECUTIONS: "true"
    EXECUTIONS_DATA_MAX_AGE: 720  # Hours (30 days)
    EXECUTIONS_DATA_PRUNE: "true"
    GENERIC_TIMEZONE: "UTC"  # Always UTC internally
    N8N_CONCURRENCY_PRODUCTION_LIMIT: 20  # Parallel executions
    
  scaling:
    vertical: "More CPU/RAM for the n8n instance"
    horizontal: "Queue mode + multiple workers"
    webhook_scaling: "Separate webhook processor from main"
```

---

## å·¥ä½œæµè´¨é‡è¯„ä¼°æ ‡å‡†

å¯¹ä»»ä½• n8n å·¥ä½œæµè¿›è¡Œ 0-100 åˆ†çš„è¯„åˆ†ï¼ˆå…± 8 ä¸ªç»´åº¦ï¼‰ï¼š

| ç»´åº¦ | æƒé‡ | 0ï¼ˆè¾ƒå·®ï¼‰ | 5ï¼ˆåˆæ ¼ï¼‰ | 10ï¼ˆä¼˜ç§€ï¼‰ |
|-----------|--------|-----------|---------------|-----------------|
| **å¯é æ€§** | 20% | æ— é”™è¯¯å¤„ç†æœºåˆ¶ | ä»…æœ‰åŸºæœ¬é”™è¯¯è§¦å‘ | å…¨é¢é‡è¯• + æ­»ä¿¡é˜Ÿåˆ— + è­¦æŠ¥æœºåˆ¶ |
| **å®‰å…¨æ€§** | 15% | å¯†é’¥ç¡¬ç¼–ç  | ä½¿ç”¨å‡­è¯å­˜å‚¨åº“ | ä½¿ç”¨ HMAC åŠ å¯† + éªŒè¯ + å®¡è®¡ |
| **æ€§èƒ½** | 15% | é¡ºåºæ‰§è¡Œï¼ˆæ— æ‰¹é‡å¤„ç†ï¼‰ | æœ‰éƒ¨åˆ†æ‰¹é‡å¤„ç† | ä¼˜åŒ–å¤„ç† + æ•°æ®ç¼“å­˜ + å¹¶è¡Œæ‰§è¡Œ |
| **å¯ç»´æŠ¤æ€§** | 15% | èŠ‚ç‚¹æ— å‘½åã€æ— æ–‡æ¡£ | èŠ‚ç‚¹æœ‰å‘½å | æœ‰å®Œæ•´æ–‡æ¡£ + ç‰ˆæœ¬æ§åˆ¶ + æœ‰æ³¨é‡Šè¯´æ˜ |
| **æ•°æ®è´¨é‡** | 10% | æ— æ•°æ®éªŒè¯ | ä»…æœ‰åŸºæœ¬æ£€æŸ¥ | æ•°æ®ç»“æ„éªŒè¯ + å»é‡ + æ•°æ®è½¬æ¢ |
| **å¯è§‚æµ‹æ€§** | 10% | æ— ç›‘æ§æœºåˆ¶ | æ— æ•…éšœæç¤º | æœ‰é”™è¯¯è­¦æŠ¥ + ç›‘æ§æŒ‡æ ‡ + æ—¥å¿—è®°å½• |
| **å¯æ‰©å±•æ€§** | 10% | å¤„ç†é‡è¶…è¿‡ 100 ä¸ªæ—¶æ€§èƒ½ä¸‹é™ | èƒ½å¤„ç† 1000 ä¸ªä»»åŠ¡ | æ”¯æŒæ‰¹é‡å¤„ç† + é˜Ÿåˆ— + æ°´å¹³æ‰©å±• |
| **å¯é‡ç”¨æ€§** | 5% | ä»£ç ç»“æ„å•ä¸€ | æœ‰éƒ¨åˆ†å­å·¥ä½œæµ | ä»£ç æ¨¡å—åŒ– + æ¥å£æ–‡æ¡£åŒ– |

**è¯„åˆ†èŒƒå›´ï¼š**
- **0-30:** åŸå‹é˜¶æ®µâ€”â€”æœªè¾¾åˆ°ç”Ÿäº§æ ‡å‡† |
- **31-60:** åŠŸèƒ½åŸºæœ¬å®ç°â€”â€”ä½†ç¨³å®šæ€§ä¸è¶³ |
- **61-80:** è¾¾åˆ°ç”Ÿäº§æ ‡å‡†â€”â€”ä½†ä»å¯æ”¹è¿› |
- **81-100:** ä¼ä¸šçº§åº”ç”¨â€”â€”å…·å¤‡é«˜ç¨³å®šæ€§ã€å¯è§‚æµ‹æ€§å’Œå¯æ‰©å±•æ€§ |

---

## n8n å·¥ä½œæµå·¥ç¨‹çš„åå¤§åŸåˆ™

1. **æ¯ä¸ªç”Ÿäº§çº§å·¥ä½œæµéƒ½å¿…é¡»æœ‰é”™è¯¯å¤„ç†æœºåˆ¶** â€” æ— ä¸€ä¾‹å¤–ã€‚
2. **åˆ‡å‹¿ç¡¬ç¼–ç æ•æ„Ÿä¿¡æ¯** â€” ä»…ä½¿ç”¨å‡­è¯å­˜å‚¨åº“æˆ–ç¯å¢ƒå˜é‡ã€‚
3. **ä¸ºæ¯ä¸ªèŠ‚ç‚¹å‘½å** â€” ä½¿ç”¨æ— æ„ä¹‰çš„åç§°ï¼ˆå¦‚â€œHTTP Request 4â€ï¼‰å±äºæŠ€æœ¯å€ºåŠ¡ã€‚
4. **æå‰è¿‡æ»¤æ•°æ®ï¼ŒåæœŸå†è¿›è¡Œå¤„ç†** â€” åœ¨å¤æ‚å¤„ç†å‰å‰”é™¤æ— æ•ˆæ•°æ®ã€‚
5. **å°½å¯èƒ½è¿›è¡Œæ‰¹é‡å¤„ç†** â€” å¯¹ 100 ä¸ªæ•°æ®ä½¿ç”¨ä¸€æ¬¡ API è°ƒç”¨ï¼Œæ¯”å¤šæ¬¡å•ç‹¬è°ƒç”¨æ›´é«˜æ•ˆã€‚
6. **ä½¿ç”¨çœŸå®æ•°æ®è¿›è¡Œæµ‹è¯•** â€” æ¨¡æ‹Ÿæ•°æ®å¯èƒ½æ©ç›–å®é™…é—®é¢˜ã€‚
7. **ä¸ºå·¥ä½œæµè®¾ç½®ç‰ˆæœ¬å·** â€” åœ¨åç§°å’Œæè¿°ä¸­ä½“ç°ç‰ˆæœ¬ä¿¡æ¯ã€‚
8. **è¯¦ç»†è®°å½•è®¾è®¡å†³ç­–** â€” ç”¨æ³¨é‡Šè§£é‡Šè®¾è®¡æ€è·¯ï¼Œè€Œä¸ä»…ä»…æ˜¯æ­¥éª¤è¯´æ˜ã€‚
9. **ç§¯æè¿›è¡Œç›‘æ§** â€” ä¸è¦ç­‰åˆ°ç”¨æˆ·æŠ•è¯‰æ‰å‘ç°é—®é¢˜ã€‚
10. **ä¿æŒç®€å•æ€§** â€” å¦‚æœéœ€è¦å›¾è¡¨æ¥è§£é‡Šæµç¨‹ï¼Œè¯´æ˜è¯´æ˜è¯´æ˜å…¶æ‹†åˆ†é€»è¾‘çš„å¿…è¦æ€§ã€‚

---

## è‡ªç„¶è¯­è¨€å‘½ä»¤

å½“ç”¨æˆ·è¯·æ±‚æ‚¨çš„å¸®åŠ©æ—¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹å‘½ä»¤æ‰§è¡Œç›¸åº”æ“ä½œï¼š

| å‘½ä»¤ | åŠ¨ä½œ |
|---------|--------|
| â€œä¸º [ä»»åŠ¡] è®¾è®¡ä¸€ä¸ªå·¥ä½œæµâ€ | ä½¿ç”¨ä¸Šè¿°æ¨¡æ¿è®¾è®¡å®Œæ•´çš„å·¥ä½œæµ |
| â€œå®¡æŸ¥è¿™ä¸ªå·¥ä½œæµâ€ | æ ¹æ®è¯„ä¼°æ ‡å‡†è¯„åˆ†å¹¶æå‡ºæ”¹è¿›å»ºè®® |
| â€œè°ƒè¯• [å·¥ä½œæµ/é”™è¯¯]â€ | æŒ‰ç…§è°ƒè¯•æ£€æŸ¥æ¸…å•è¿›è¡Œæ“ä½œ |
| â€œä¼˜åŒ– [å·¥ä½œæµ]â€ | åº”ç”¨æ€§èƒ½ä¼˜åŒ–ç­–ç•¥ |
| â€œä¸º [å·¥ä½œæµ] æ·»åŠ é”™è¯¯å¤„ç†â€ | å®ç°é”™è¯¯è§¦å‘ã€é‡è¯•å’Œè­¦æŠ¥æœºåˆ¶ |
| â€œä¸º [é€»è¾‘] åˆ›å»ºå­å·¥ä½œæµâ€ | å°†ç›¸å…³é€»è¾‘æå–å¹¶è®¾è®¡ä¸ºç‹¬ç«‹çš„å¯è°ƒç”¨å­å·¥ä½œæµ |
| â€œè®¾ç½®ç›‘æ§æœºåˆ¶â€ | å®ç°å¥åº·æ£€æŸ¥ä¸è­¦æŠ¥åŠŸèƒ½ |
| â€œå°†å·¥ä½œæµéƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒâ€ | æŒ‰ç…§éƒ¨ç½²æµç¨‹æ“ä½œ |
| â€œè®¾è®¡ [A] åˆ° [B] çš„é›†æˆæµç¨‹â€ | ä»é›†æˆæ¨¡å¼åº“ä¸­é€‰æ‹©åˆé€‚çš„æ–¹æ¡ˆ |
| â€œä¸º [å·¥ä½œæµ] æ·»åŠ  AI åŠŸèƒ½â€ | å®ç° AI å¤„ç†æµç¨‹ |
| â€œé™åˆ¶ [API] çš„è¯·æ±‚é€Ÿç‡â€ | å®æ–½æ‰¹é‡å¤„ç† + é™æµæœºåˆ¶ |
| â€œå®¡è®¡æˆ‘çš„ n8n è®¾ç½®â€ | è¿›è¡Œå¿«é€Ÿå¥åº·æ£€æŸ¥ï¼Œè¯„åˆ†å¹¶ç¡®å®šä¼˜å…ˆä¿®å¤äº‹é¡¹ |

---