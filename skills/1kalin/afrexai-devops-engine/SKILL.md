---
name: afrexai-devops-engine
description: å®Œæ•´çš„ DevOps ä¸å¹³å°å·¥ç¨‹ç³»ç»Ÿï¼šåŒ…æ‹¬æŒç»­é›†æˆ/æŒç»­äº¤ä»˜ï¼ˆCI/CDï¼‰ç®¡é“ã€åŸºç¡€è®¾æ–½å³ä»£ç ï¼ˆInfrastructure as Codeï¼‰ç®¡ç†ã€å®¹å™¨ç¼–æ’ï¼ˆContainer Orchestrationï¼‰æœºåˆ¶ã€å¯è§‚æµ‹æ€§ï¼ˆObservabilityï¼‰å·¥å…·ã€äº‹ä»¶å“åº”ï¼ˆIncident Responseï¼‰æµç¨‹ï¼Œä»¥åŠç«™ç‚¹å¯é æ€§å·¥ç¨‹ï¼ˆSite Reliability Engineering, SREï¼‰æœ€ä½³å®è·µâ€”â€”é€‚ç”¨äºæ‰€æœ‰å¹³å°å’Œæ‰€æœ‰äº‘ç¯å¢ƒã€‚
metadata: {"clawdbot":{"emoji":"ğŸ”§","os":["linux","darwin","win32"]}}
---
# DevOpsä¸å¹³å°å·¥ç¨‹å¼•æ“

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ç³»ç»Ÿï¼Œç”¨äºæ„å»ºã€éƒ¨ç½²ã€è¿è¥å’Œç›‘æ§ç”Ÿäº§ç¯å¢ƒä¸­çš„è½¯ä»¶ã€‚å®ƒæ¶µç›–äº†æ•´ä¸ªDevOpsç”Ÿå‘½å‘¨æœŸâ€”â€”ä¸ä»…ä»…æ˜¯æŒç»­é›†æˆ/æŒç»­éƒ¨ç½²ï¼ˆCI/CDï¼‰ï¼Œä¹Ÿä¸ä»…ä»…å±€é™äºå•ä¸€äº‘å¹³å°ã€‚

## ç¬¬ä¸€é˜¶æ®µï¼šä»“åº“ä¸åˆ†æ”¯ç­–ç•¥

### Git Flowå†³ç­–çŸ©é˜µ

| å›¢é˜Ÿè§„æ¨¡ | å‘å¸ƒé¢‘ç‡ | ç­–ç•¥ | åˆ†æ”¯ç»“æ„ |
|-----------|----------------|----------|----------|
| 1-3äºº | æŒç»­é›†æˆ | åŸºäºä¸»åˆ†æ”¯ï¼ˆTrunk-basedï¼‰ | main + çŸ­æœŸåŠŸèƒ½åˆ†æ”¯ |
| 4-15äºº | æ¯å‘¨/åŒå‘¨ | GitHub Flow | main + åŠŸèƒ½åˆ†æ”¯ + æäº¤è¯·æ±‚ï¼ˆPRï¼‰ |
| 15äººä»¥ä¸Š | å®šæœŸå‘å¸ƒ | Git Flow | main + å¼€å‘åˆ†æ”¯ + åŠŸèƒ½åˆ†æ”¯ + å‘å¸ƒåˆ†æ”¯ + ç´§æ€¥ä¿®å¤åˆ†æ”¯ |
| å—ç›‘ç®¡çš„å›¢é˜Ÿ | ç»è¿‡å®¡æ ¸çš„å‘å¸ƒ | Git Flow + æ ‡ç­¾ï¼ˆtagsï¼‰ | ä¸Šè¿°åˆ†æ”¯ç»“æ„ + ç­¾åçš„æ ‡ç­¾ + å®¡è®¡è¿½è¸ª |

### åˆ†æ”¯ä¿æŠ¤è§„åˆ™ï¼ˆè¯·éµå¾ªè¿™äº›è§„åˆ™ï¼‰

```yaml
# branch-protection.yml â€” document your rules
main:
  required_reviews: 2
  dismiss_stale_reviews: true
  require_codeowners: true
  require_status_checks:
    - ci/test
    - ci/lint
    - ci/security
  require_linear_history: true  # No merge commits
  restrict_pushes: true         # Only via PR
  require_signed_commits: false # Enable for regulated

develop:
  required_reviews: 1
  require_status_checks:
    - ci/test
```

### æäº¤è§„èŒƒ

æ ¼å¼ï¼š`<ç±»å‹>(<èŒƒå›´>): <æè¿°>`

ç±»å‹ç¤ºä¾‹ï¼š`feat`ï¼ˆæ–°å¢åŠŸèƒ½ï¼‰ã€`fix`ï¼ˆä¿®å¤é—®é¢˜ï¼‰ã€`docs`ï¼ˆæ–‡æ¡£æ›´æ–°ï¼‰ã€`style`ï¼ˆä»£ç é£æ ¼è°ƒæ•´ï¼‰ã€`refactor`ï¼ˆä»£ç é‡æ„ï¼‰ã€`perf`ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰ã€`test`ï¼ˆæµ‹è¯•ï¼‰ã€`build`ï¼ˆæ„å»ºï¼‰ã€`ci`ï¼ˆæŒç»­é›†æˆç›¸å…³ï¼‰ã€`chore`ï¼ˆæ‚åŠ¡ï¼‰

é‡è¦æç¤ºï¼šæ¶‰åŠé‡å¤§å˜æ›´çš„æäº¤éœ€ä½¿ç”¨`feat!: remove legacy API`ä½œä¸ºå‰ç¼€ï¼Œæˆ–æ·»åŠ `BREAKING CHANGE: æè¿°å˜æ›´å†…å®¹`çš„æ³¨é‡Šã€‚é€šè¿‡`commitlint`å’Œ`husky`ï¼ˆNode.jsæ’ä»¶ï¼‰æˆ–é¢„æäº¤é’©å­æ¥å¼ºåˆ¶æ‰§è¡Œè¿™äº›è§„èŒƒã€‚

## ç¬¬äºŒé˜¶æ®µï¼šCI/CDç®¡é“æ¶æ„

### ç®¡é“è®¾è®¡åŸåˆ™

1. **ä¸€æ¬¡æ„å»ºï¼Œåˆ°å¤„éƒ¨ç½²**â€”â€”ç›¸åŒçš„æ„å»ºäº§ç‰©åº”ç”¨äºå¼€å‘ã€æµ‹è¯•å’Œç”Ÿäº§ç¯å¢ƒã€‚
2. **å¿«é€Ÿå¤±è´¥æ£€æµ‹**â€”â€”ä¼˜å…ˆæ‰§è¡Œæˆæœ¬æœ€ä½çš„æ£€æŸ¥æ­¥éª¤ï¼ˆä»£ç æ ¼å¼æ£€æŸ¥ã€å•å…ƒæµ‹è¯•ã€é›†æˆæµ‹è¯•ã€ç«¯åˆ°ç«¯æµ‹è¯•ï¼‰ã€‚
3. **å°é—­å¼æ„å»º**â€”â€”æ„å»ºè¿‡ç¨‹ä¸­ä¸ä¾èµ–å¤–éƒ¨çŠ¶æ€ï¼Œä¸”å¯ä»¥ä»æäº¤å“ˆå¸Œå€¼ï¼ˆcommit SHAï¼‰é‡ç°æ„å»ºè¿‡ç¨‹ã€‚
4. **æ„å»ºäº§ç‰©ä¸å¯ä¿®æ”¹**â€”â€”æ„å»ºå®Œæˆåä¸å…è®¸ä¿®æ”¹ï¼›ä½¿ç”¨Gitå“ˆå¸Œå€¼è¿›è¡Œæ ‡è®°ã€‚
5. **å¹¶è¡Œå¤„ç†ç‹¬ç«‹é˜¶æ®µ**â€”â€”åŒæ—¶æ‰§è¡Œæµ‹è¯•ã€ä»£ç æ ¼å¼æ£€æŸ¥å’Œå®‰å…¨æ‰«æç­‰æ­¥éª¤ã€‚

### é€šç”¨ç®¡é“æ¨¡æ¿

```yaml
# pipeline-stages.yml â€” adapt to your CI system
stages:
  # Stage 1: Quality Gate (parallel, <2 min)
  lint:
    run: lint
    parallel: true
    timeout: 2m
  typecheck:
    run: tsc --noEmit
    parallel: true
    timeout: 2m
  security_scan:
    run: trivy, snyk, or semgrep
    parallel: true
    timeout: 3m

  # Stage 2: Test (parallel by type, <10 min)
  unit_tests:
    run: test --unit
    parallel: true
    coverage_threshold: 80%
    timeout: 5m
  integration_tests:
    run: test --integration
    parallel: true
    needs: [database_service]
    timeout: 10m

  # Stage 3: Build (<5 min)
  build:
    needs: [lint, typecheck, unit_tests]
    outputs: [docker_image, release_artifact]
    tag: "${GIT_SHA}"
    cache: [node_modules, .next/cache, target/]

  # Stage 4: Deploy Staging (auto)
  deploy_staging:
    needs: [build]
    environment: staging
    strategy: rolling
    smoke_test: true
    auto: true

  # Stage 5: E2E on Staging (<15 min)
  e2e_tests:
    needs: [deploy_staging]
    timeout: 15m
    retry: 1
    artifacts: [screenshots, videos]

  # Stage 6: Deploy Production (manual gate or auto)
  deploy_prod:
    needs: [e2e_tests]
    environment: production
    strategy: canary  # or blue-green
    approval: required  # manual gate
    rollback_on_failure: true
    monitoring_window: 15m
```

### CIå¹³å°å¿«é€Ÿå‚è€ƒ

| åŠŸèƒ½ | GitHub Actions | GitLab CI | CircleCI | Jenkins |
|---------|---------------|-----------|----------|---------|
| é…ç½®æ–‡ä»¶ | `.github/workflows/*.yml` | `.gitlab-ci.yml` | `.circleci/config.yml` | `Jenkinsfile` |
| å¹¶è¡Œæ€§ | `jobs.<id>`ï¼ˆè‡ªåŠ¨è®¾ç½®ï¼‰ | `stages` + `parallel` | `workflows` | `parallel`æ­¥éª¤ |
| ç¼“å­˜ç­–ç•¥ | `actions/cache` | `cache:`é”® | `save_cache/restore_cache` | Stash/unstash` |
| æœºå¯†ç®¡ç† | è®¾ç½® â†’ æœºå¯†ç®¡ç†åŠŸèƒ½ | è®¾ç½® â†’ CI/CD â†’ å˜é‡ç®¡ç† | é¡¹ç›®è®¾ç½® â†’ ç¯å¢ƒå˜é‡ | å‡­æ®ç®¡ç†æ’ä»¶ |
| å¤šç¯å¢ƒéƒ¨ç½² | `strategy.matrix` | `parallel:matrix` | `workflows`ä¸­çš„`matrix`é…ç½® | ç®¡é“ä¸­çš„`matrix`è®¾ç½® |
| è‡ªæ‰˜ç®¡ç¯å¢ƒ | `runs-on: self-hosted` | GitLab Runner | `resource_class` | é»˜è®¤é…ç½® |
| ä½¿ç”¨OIDC/æ— å¯†é’¥è®¤è¯ | `permissions: id-token: write` | `id_tokens:` | OIDCè®¤è¯æœºåˆ¶ |
| ç¼“å­˜ç­–ç•¥ | **å…·ä½“å®ç°æ–¹å¼è¯·å‚è€ƒç›¸åº”å¹³å°æ–‡æ¡£** |

### GitHub Actionsç‰¹å®šæ¨¡å¼

```yaml
# Reusable workflow (DRY across repos)
# .github/workflows/reusable-deploy.yml
on:
  workflow_call:
    inputs:
      environment:
        required: true
        type: string
    secrets:
      DEPLOY_KEY:
        required: true

# Caller workflow
jobs:
  deploy:
    uses: ./.github/workflows/reusable-deploy.yml
    with:
      environment: production
    secrets: inherit
```

### GitHub Actionså…¶ä»–ç›¸å…³å†…å®¹

```yaml
# Path-based triggers (monorepo)
on:
  push:
    paths:
      - 'packages/api/**'
      - 'shared/**'
  # Skip CI for docs-only changes
  pull_request:
    paths-ignore:
      - '**.md'
      - 'docs/**'
```

### GitHub Actionså…¶ä»–ç›¸å…³å†…å®¹

## ç¬¬ä¸‰é˜¶æ®µï¼šå®¹å™¨åŒ–ç­–ç•¥

### Dockerfileæœ€ä½³å®è·µ

```dockerfile
# Multi-stage build template
# Stage 1: Build
FROM node:20-alpine AS builder
WORKDIR /app
COPY package.json package-lock.json ./
RUN npm ci --production=false    # Install all deps for build
COPY . .
RUN npm run build

# Stage 2: Production
FROM node:20-alpine AS production
RUN addgroup -g 1001 app && adduser -u 1001 -G app -s /bin/sh -D app
WORKDIR /app
COPY --from=builder --chown=app:app /app/dist ./dist
COPY --from=builder --chown=app:app /app/node_modules ./node_modules
COPY --from=builder --chown=app:app /app/package.json ./

USER app
EXPOSE 3000
HEALTHCHECK --interval=30s --timeout=3s --retries=3 \
  CMD wget -qO- http://localhost:3000/health || exit 1
CMD ["node", "dist/index.js"]
```

### å›¾åƒå¤§å°ä¼˜åŒ–å»ºè®®

- ä½¿ç”¨alpineæˆ–distrolessåŸºç¡€é•œåƒã€‚
- é‡‡ç”¨å¤šé˜¶æ®µæ„å»ºæ–¹å¼ï¼ˆå°†ä¾èµ–é¡¹æ‰“åŒ…åˆ°å•ç‹¬çš„é•œåƒä¸­ï¼‰ã€‚
- ä½¿ç”¨`.dockerignore`æ–‡ä»¶æ’é™¤ä¸å¿…è¦çš„æ–‡ä»¶ï¼ˆå¦‚`.git`ã€`node_modules`ã€`.md`æ–‡ä»¶ä»¥åŠæµ‹è¯•å’Œæ–‡æ¡£æ–‡ä»¶ï¼‰ã€‚
- åˆå¹¶å¤šä¸ª`RUN`å‘½ä»¤ä»¥å‡å°‘é•œåƒå±‚æ•°ã€‚
- åœ¨æ„å»ºè¿‡ç¨‹ä¸­æ¸…ç†åŒ…ç®¡ç†å™¨çš„ç¼“å­˜ï¼ˆä¾‹å¦‚ï¼š`rm -rf /var/cache/apk/*`ï¼‰ã€‚
- ç¡®ä¿ç”Ÿäº§ç¯å¢ƒä¸åŒ…å«å¼€å‘é˜¶æ®µçš„ä¾èµ–é¡¹ã€‚
- å›ºå®šåŸºç¡€é•œåƒçš„å“ˆå¸Œå€¼ï¼ˆä¾‹å¦‚ï¼š`FROM node:20-alpine@sha256:abc123...`ï¼‰ã€‚

### å®¹å™¨å®‰å…¨æ‰«æ

```bash
# Trivy (recommended â€” free, fast)
trivy image myapp:latest --severity HIGH,CRITICAL
trivy fs . --security-checks vuln,secret,config

# Scan in CI before push
# Fail pipeline if CRITICAL vulnerabilities found
trivy image --exit-code 1 --severity CRITICAL myapp:${GIT_SHA}
```

### æœ¬åœ°å¼€å‘ç¯å¢ƒä¸‹çš„Docker Composeé…ç½®

```yaml
# docker-compose.yml â€” local development stack
services:
  app:
    build:
      context: .
      target: builder  # Use build stage for hot reload
    volumes:
      - .:/app
      - /app/node_modules  # Don't override node_modules
    ports:
      - "3000:3000"
    environment:
      - DATABASE_URL=postgres://user:pass@db:5432/app
      - REDIS_URL=redis://cache:6379
    depends_on:
      db:
        condition: service_healthy

  db:
    image: postgres:16-alpine
    volumes:
      - pgdata:/var/lib/postgresql/data
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: pass
      POSTGRES_DB: app
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U user"]
      interval: 5s
      timeout: 3s
      retries: 5

  cache:
    image: redis:7-alpine
    ports:
      - "6379:6379"

volumes:
  pgdata:
```

## ç¬¬å››é˜¶æ®µï¼šåŸºç¡€è®¾æ–½å³ä»£ç ï¼ˆInfrastructure as Code, IaCï¼‰

### IaCå·¥å…·é€‰æ‹©çŸ©é˜µ

| å·¥å…· | é€‚ç”¨åœºæ™¯ | æ•°æ®å­˜å‚¨æ–¹å¼ | ç¼–ç¨‹è¯­è¨€ | å­¦ä¹ æ›²çº¿ |
|------|----------|-------|----------|----------------|
| Terraform/OpenTofu | å¤šäº‘ç¯å¢ƒï¼Œä¸äº‘å¹³å°æ— å…³ | ä½¿ç”¨è¿œç¨‹å­˜å‚¨ï¼ˆå¦‚S3ã€GCSï¼‰ | HCLï¼ˆTerraformé…ç½®è¯­è¨€ï¼‰ | ä¸­ç­‰éš¾åº¦ |
| Pulumi | é€‚åˆå–œæ¬¢ç¼–å†™å®é™…ä»£ç çš„å¼€å‘è€… | ä½¿ç”¨è¿œç¨‹å­˜å‚¨ | ä½¿ç”¨ TypeScript/Python/Goè¯­è¨€ | å­¦ä¹ éš¾åº¦è¾ƒä½ï¼ˆå¦‚æœè‡ªå·±ç¼–å†™é…ç½®è„šæœ¬çš„è¯ï¼‰ |
| AWS CDK | ä»…é€‚ç”¨äºAWSç¯å¢ƒ | ä½¿ç”¨CloudFormation | ä½¿ç”¨ TypeScript/Pythonè¯­è¨€ | ä¸­ç­‰éš¾åº¦ |
| Ansible | ç”¨äºé…ç½®ç®¡ç†å’ŒæœåŠ¡å™¨è®¾ç½® | æ— çŠ¶æ€å­˜å‚¨ | ä½¿ç”¨YAMLè¯­è¨€ | å­¦ä¹ éš¾åº¦è¾ƒä½ |
| Helm | ç”¨äºKuberneteséƒ¨ç½² | æ”¯æŒTiller/OCIï¼ˆKubernetesæ“ä½œæ¡†æ¶ï¼‰ | ä½¿ç”¨YAMLå’ŒGoæ¨¡æ¿ | ä¸­ç­‰éš¾åº¦ |

### Terraformé¡¹ç›®ç»“æ„å»ºè®®

```
infrastructure/
â”œâ”€â”€ modules/                    # Reusable components
â”‚   â”œâ”€â”€ vpc/
â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â”œâ”€â”€ variables.tf
â”‚   â”‚   â””â”€â”€ outputs.tf
â”‚   â”œâ”€â”€ ecs-service/
â”‚   â””â”€â”€ rds/
â”œâ”€â”€ environments/
â”‚   â”œâ”€â”€ dev/
â”‚   â”‚   â”œâ”€â”€ main.tf            # Calls modules with dev params
â”‚   â”‚   â”œâ”€â”€ terraform.tfvars
â”‚   â”‚   â””â”€â”€ backend.tf         # Dev state bucket
â”‚   â”œâ”€â”€ staging/
â”‚   â””â”€â”€ prod/
â”œâ”€â”€ .terraform-version          # Pin terraform version
â””â”€â”€ .tflint.hcl
```

### Terraformå®‰å…¨ä½¿ç”¨è§„åˆ™

1. **å§‹ç»ˆåœ¨åº”ç”¨æ›´æ”¹å‰è¿›è¡Œè§„åˆ’**â€”â€”ä»”ç»†å®¡æŸ¥æ¯ä¸ªå˜æ›´ã€‚
2. **ä½¿ç”¨é”å®šæœºåˆ¶ç®¡ç†è¿œç¨‹çŠ¶æ€æ•°æ®**â€”â€”ä¾‹å¦‚ä½¿ç”¨S3å’ŒDynamoDBæˆ–GCSï¼Œå¹¶è®¾ç½®é”å®šæœºåˆ¶ã€‚
3. **ä¸è¦å°†æ•æ„Ÿä¿¡æ¯ï¼ˆå¦‚æ•°æ®åº“å¯†ç ï¼‰å­˜å‚¨åœ¨Gitä»“åº“ä¸­**ã€‚
4. **åœ¨ç®¡ç†èµ„æºä¹‹å‰å…ˆå¯¼å…¥ç°æœ‰èµ„æº**â€”â€”é¿å…é‡å¤åˆ›å»ºèµ„æºã€‚
5. **å¯¹å…³é”®èµ„æºï¼ˆå¦‚æ•°æ®åº“ã€S3å­˜å‚¨æ¡¶ï¼‰ä½¿ç”¨`prevent_destroy`é€‰é¡¹è¿›è¡Œä¿æŠ¤**ã€‚
6. **ä¸ºæ‰€æœ‰èµ„æºæ·»åŠ æ ‡ç­¾**â€”â€”ä¾‹å¦‚`environment`ã€`team`ã€`cost-center`ã€`managed-by: terraform`ç­‰ã€‚
7. åœ¨CIè¿‡ç¨‹ä¸­ä½¿ç”¨`terraform fmt`å·¥å…·ä¿æŒé…ç½®æ ¼å¼çš„ä¸€è‡´æ€§ã€‚

### ç¯å¢ƒå‘å¸ƒæµç¨‹å»ºè®®

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  terraform plan â”€â”€â–ºâ”‚  Review in PR    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ merge
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  auto-apply â”€â”€â”€â”€â”€â”€â–ºâ”‚  Dev             â”‚â”€â”€â–º smoke tests
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ promote
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  manual approve â”€â”€â–ºâ”‚  Staging         â”‚â”€â”€â–º integration tests
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ promote (manual gate)
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  manual approve â”€â”€â–ºâ”‚  Production      â”‚â”€â”€â–º monitoring window
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ç¬¬äº”é˜¶æ®µï¼šKubernetesæ“ä½œ

### Kubernetesèµ„æºæ¨¡æ¿

```yaml
# deployment.yml â€” production-ready template
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
  labels:
    app: myapp
    version: "1.0.0"
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0    # Zero-downtime
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
      containers:
        - name: myapp
          image: myregistry/myapp:abc123  # Git SHA tag
          ports:
            - containerPort: 3000
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
            limits:
              cpu: 500m
              memory: 512Mi
          livenessProbe:
            httpGet:
              path: /health
              port: 3000
            initialDelaySeconds: 10
            periodSeconds: 10
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /ready
              port: 3000
            initialDelaySeconds: 5
            periodSeconds: 5
          env:
            - name: DATABASE_URL
              valueFrom:
                secretKeyRef:
                  name: myapp-secrets
                  key: database-url
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: DoNotSchedule
```

### Helmå›¾è¡¨é…ç½®å»ºè®®

- åœ¨`values.yaml`æ–‡ä»¶ä¸­è®¾ç½®åˆç†çš„é»˜è®¤å€¼ï¼ˆä»¥ä¾¿å¿«é€Ÿé…ç½®ï¼‰ã€‚
- æ˜ç¡®èµ„æºè¯·æ±‚å’Œé™åˆ¶ã€‚
- å®šä¹‰å¥åº·æ£€æŸ¥/å°±ç»ªæ£€æŸ¥æœºåˆ¶ã€‚
- è®¾ç½®PodDisruptionBudgetå‚æ•°ï¼ˆä¾‹å¦‚`minAvailable: 1`è¡¨ç¤ºå…è®¸1%çš„æ•…éšœç‡ï¼Œ`maxUnavailable: 25%`è¡¨ç¤ºå…è®¸25%çš„æ•…éšœç‡ï¼‰ã€‚
- é…ç½®ç½‘ç»œç­–ç•¥ï¼ˆä¾‹å¦‚`deny all`è¡¨ç¤ºæ‹’ç»æ‰€æœ‰æµé‡ï¼Œ`allow specific`è¡¨ç¤ºä»…å…è®¸ç‰¹å®šæµé‡ï¼‰ã€‚
- ä½¿ç”¨`ServiceAccount`è¿›è¡Œèº«ä»½éªŒè¯ï¼ˆéé»˜è®¤è®¾ç½®ï¼‰ã€‚
- é€šè¿‡`external-secrets-operator`æˆ–`sealed-secrets`æœºåˆ¶ç®¡ç†æœºå¯†ä¿¡æ¯ã€‚
- åœ¨CIè¿‡ç¨‹ä¸­ä½¿ç”¨`helm lint`å’Œ`helm template`è¿›è¡Œä»£ç æ£€æŸ¥ã€‚
- åœ¨`NOTES.txt`æ–‡ä»¶ä¸­è®°å½•éƒ¨ç½²åçš„æ“ä½œæŒ‡å—ã€‚

### kubectlåŸºæœ¬æ“ä½œæŒ‡å—

```bash
# Debugging
kubectl get pods -l app=myapp -o wide          # Pod status + node
kubectl describe pod <pod>                      # Events, conditions
kubectl logs <pod> --tail=100 -f               # Stream logs
kubectl logs <pod> --previous                   # Crashed container logs
kubectl exec -it <pod> -- /bin/sh              # Shell into pod
kubectl top pods -l app=myapp                  # Resource usage

# Rollouts
kubectl rollout status deployment/myapp        # Watch rollout
kubectl rollout history deployment/myapp       # Revision history
kubectl rollout undo deployment/myapp          # Rollback to previous
kubectl rollout undo deployment/myapp --to-revision=3  # Specific

# Scaling
kubectl scale deployment/myapp --replicas=5    # Manual scale
kubectl autoscale deployment/myapp --min=3 --max=10 --cpu-percent=70

# Context management
kubectl config get-contexts                     # List clusters
kubectl config use-context prod-cluster         # Switch
kubectl config set-context --current --namespace=myapp  # Set namespace
```

## ç¬¬å…­é˜¶æ®µï¼šéƒ¨ç½²ç­–ç•¥

### éƒ¨ç½²ç­–ç•¥é€‰æ‹©çŸ©é˜µ

| éƒ¨ç½²ç­–ç•¥ | é£é™© | éƒ¨ç½²é€Ÿåº¦ | å›æ»šèƒ½åŠ› | æˆæœ¬ | é€‚ç”¨åœºæ™¯ |
|----------|------|-------|----------|------|----------|
| æ»šåŠ¨éƒ¨ç½²ï¼ˆRollingï¼‰ | é£é™©è¾ƒä½ | éƒ¨ç½²é€Ÿåº¦å¿« | å›æ»šé€Ÿåº¦è¾ƒæ…¢ | é€‚ç”¨äºæ ‡å‡†éƒ¨ç½²åœºæ™¯ |
| è“ç»¿éƒ¨ç½²ï¼ˆBlue-Greenï¼‰ | é£é™©è¾ƒä½ | éƒ¨ç½²é€Ÿåº¦å¿« | å¯ç«‹å³åˆ‡æ¢åˆ°æ–°ç‰ˆæœ¬ | é€‚ç”¨äºå…³é”®æœåŠ¡ï¼Œè¦æ±‚é›¶åœæœºæ—¶é—´ |
| ç“œç‰›éƒ¨ç½²ï¼ˆCanaryï¼‰ | é£é™©æä½ | éƒ¨ç½²é€Ÿåº¦å¿« | å¯ç«‹å³åˆ‡æ¢åˆ°æ–°ç‰ˆæœ¬ | é€‚ç”¨äºé«˜æµé‡ã€é£é™©è¾ƒé«˜çš„åœºæ™¯ |
| ç‰¹æ€§å¼€å…³ï¼ˆFeature Flagï¼‰ | é£é™©æä½ | éƒ¨ç½²é€Ÿåº¦å¿« | å¯ç«‹å³åˆ‡æ¢ | é€‚ç”¨äºé€æ­¥æ¨å¹¿æ–°ç‰¹æ€§æˆ–è¿›è¡ŒA/Bæµ‹è¯• |
| é‡æ–°åˆ›å»ºéƒ¨ç½²ï¼ˆRecreateï¼‰ | é£é™©è¾ƒé«˜ | éƒ¨ç½²é€Ÿåº¦å¿« | å›æ»šé€Ÿåº¦æ…¢ | é€‚ç”¨äºéœ€è¦é‡å»ºèµ„æºçš„åœºæ™¯ï¼ˆå¦‚çŠ¶æ€åŒ–åº”ç”¨ï¼‰ |

### ç“œç‰›éƒ¨ç½²å·¥ä½œæµç¨‹

å½“éƒ¨ç½²å‡ºç°é—®é¢˜æ—¶ï¼š
1. **ç«‹å³é‡‡å–æªæ–½**ï¼šå°†æµé‡ä»æ–°ç‰ˆæœ¬è·¯ç”±åˆ°æ—§ç‰ˆæœ¬ã€‚
2. **å¦‚æœä½¿ç”¨æ»šåŠ¨éƒ¨ç½²**ï¼šä½¿ç”¨`kubectl rollout undo`å‘½ä»¤æ’¤é”€éƒ¨ç½²æˆ–é‡æ–°éƒ¨ç½²ä¹‹å‰çš„ç‰ˆæœ¬ã€‚
3. **æ£€æŸ¥**ï¼šæ•°æ®åº“è¿ç§»æ˜¯å¦å‘åå…¼å®¹ï¼Ÿ
4. **éªŒè¯**ï¼šå›æ»šæ“ä½œæ˜¯å¦æˆåŠŸï¼Ÿæ£€æŸ¥é”™è¯¯ç‡å’Œç³»ç»Ÿæ€§èƒ½ã€‚
5. **æ²Ÿé€š**ï¼šåœ¨#incidentsé€šé“ä¸­æŠ¥å‘Šé—®é¢˜ï¼Œå¹¶æ›´æ–°çŠ¶æ€é¡µé¢ã€‚
6. **è°ƒæŸ¥**ï¼šåœ¨æ‰¾åˆ°æ ¹æœ¬åŸå› ä¹‹å‰ä¸è¦é‡æ–°éƒ¨ç½²ã€‚

### æ•°æ®åº“è¿ç§»å®‰å…¨æ³¨æ„äº‹é¡¹

```
RULE: Migrations must be backward-compatible with the PREVIOUS version.
      (Because during rolling deploy, both versions run simultaneously)

Safe migration pattern:
  v1: Add new column (nullable, with default)
  v2: Backfill data, start writing to new column
  v3: Make new column required, stop writing old column
  v4: Drop old column (after v3 is fully deployed)

NEVER in one deploy:
  âŒ Rename column
  âŒ Change column type
  âŒ Drop column still read by current version
  âŒ Add NOT NULL without default
```

## ç¬¬ä¸ƒé˜¶æ®µï¼šç›‘æ§ä¸å¯è§‚æµ‹æ€§

### ç›‘æ§ä¸å¯è§‚æµ‹æ€§å·¥å…·ç»„åˆ

| å·¥å…· | åŠŸèƒ½ | ä¼˜å…ˆçº§ |
|--------|------|-------|
| **æŒ‡æ ‡ï¼ˆMetricsï¼‰** | éšæ—¶é—´å˜åŒ–çš„æ•°å€¼æ•°æ® | Prometheusã€Datadogã€CloudWatch | é¦–é€‰å·¥å…· |
| **æ—¥å¿—ï¼ˆLogsï¼‰** | äº‹ä»¶è®°å½• | ELKã€Lokiã€CloudWatch Logs | é‡è¦å·¥å…· |
| **è¿½è¸ªï¼ˆTracesï¼‰** | æœåŠ¡é—´çš„è¯·æ±‚æµç¨‹ | Jaegerã€Tempoã€X-Rayã€Honeycomb | é«˜çº§å·¥å…· |
| **æ€§èƒ½åˆ†æï¼ˆProfilingï¼‰** | CPU/å†…å­˜ä½¿ç”¨æƒ…å†µåˆ†æ | Pyroscopeã€Parca | å¯é€‰å·¥å…·ï¼ˆé’ˆå¯¹æ€§èƒ½ä¼˜åŒ–éœ€æ±‚ï¼‰ |

### éœ€è¦è·Ÿè¸ªçš„å…³é”®æŒ‡æ ‡

```yaml
# RED Method (request-driven services)
rate:     # Requests per second
errors:   # Failed requests per second
duration: # Latency distribution (p50, p95, p99)

# USE Method (infrastructure/resources)
utilization:  # % of resource in use (CPU, memory, disk)
saturation:   # Queue depth, pending work
errors:       # Resource errors (OOM, disk full)

# Business Metrics (most important!)
signups_per_hour:
checkout_completion_rate:
api_calls_by_customer:
revenue_per_minute:
```

### è­¦æŠ¥è§„åˆ™è®¾ç½®å»ºè®®

```yaml
# alerting-rules.yml
alerts:
  # Symptom-based (good â€” tells you users are impacted)
  - name: HighErrorRate
    condition: "error_rate_5xx > 1% for 5m"
    severity: critical
    runbook: docs/runbooks/high-error-rate.md
    notify: [pagerduty, slack-incidents]

  - name: HighLatency
    condition: "p99_latency > 2s for 5m"
    severity: warning
    runbook: docs/runbooks/high-latency.md
    notify: [slack-incidents]

  # Cause-based (supplementary â€” helps diagnose)
  - name: PodCrashLooping
    condition: "pod_restart_count increase > 3 in 10m"
    severity: warning
    notify: [slack-platform]

  - name: DiskSpaceWarning
    condition: "disk_usage > 80%"
    severity: warning
    notify: [slack-platform]

  - name: CertificateExpiring
    condition: "cert_expiry_days < 14"
    severity: warning
    notify: [slack-platform]

# Alert rules:
# 1. Every alert must have a runbook link
# 2. Every alert must be actionable (if you can't do anything, remove it)
# 3. Critical = wake someone up. Warning = check next business day.
# 4. Review alerts monthly â€” archive unused, tune noisy ones
```

### ç»“æ„åŒ–æ—¥å¿—è®°å½•æ ‡å‡†

**æ—¥å¿—çº§åˆ«è¯´æ˜ï¼š**
- `error`ï¼šè¡¨ç¤ºç³»ç»Ÿå‡ºç°æ•…éšœï¼Œéœ€è¦ç«‹å³å¤„ç†ã€‚
- `warn`ï¼šè¡¨ç¤ºå‡ºç°æ„å¤–æƒ…å†µï¼Œä½†ç³»ç»Ÿå·²å¤„ç†ï¼ˆä¾‹å¦‚é‡è¯•æˆåŠŸæˆ–ä½¿ç”¨äº†å¤‡ç”¨æ–¹æ¡ˆï¼‰ã€‚
- `info`ï¼šè¡¨ç¤ºä¸šåŠ¡ç›¸å…³äº‹ä»¶ï¼ˆä¾‹å¦‚è®¢å•æ”¾ç½®ã€ç”¨æˆ·æ³¨å†Œã€éƒ¨ç½²å¼€å§‹ç­‰ï¼‰ã€‚
- `debug`ï¼šè¡¨ç¤ºæŠ€æœ¯ç»†èŠ‚ï¼ˆä¾‹å¦‚æŸ¥è¯¢æ‰§è¡Œç»“æœã€ç¼“å­˜å‘½ä¸­/æœªå‘½ä¸­ç­‰ï¼‰â€”â€”åœ¨ç”Ÿäº§ç¯å¢ƒä¸­é€šå¸¸å…³é—­è¿™äº›æ—¥å¿—ã€‚

### ä»ªè¡¨æ¿è®¾è®¡å»ºè®®

æ¯ä¸ªæœåŠ¡çš„ä»ªè¡¨æ¿éƒ½åº”åŒ…å«å¿…è¦çš„ç›‘æ§æŒ‡æ ‡ã€‚

```
Row 1: Traffic Overview
  - Request rate (per endpoint)
  - Error rate (4xx, 5xx separate)
  - Active users / connections

Row 2: Performance
  - p50, p95, p99 latency
  - Throughput
  - Apdex score

Row 3: Resources
  - CPU utilization (per pod/instance)
  - Memory usage (vs limit)
  - Disk I/O / Network I/O

Row 4: Business
  - Revenue per minute (if applicable)
  - Conversion funnel
  - Queue depth / processing lag

Row 5: Dependencies
  - Database query latency + connection pool
  - External API latency + error rate
  - Cache hit rate
```

## ç¬¬å…«é˜¶æ®µï¼šäº‹ä»¶å“åº”æœºåˆ¶

### äº‹ä»¶ä¸¥é‡ç¨‹åº¦åˆ†çº§

| ä¸¥é‡ç¨‹åº¦ | å®šä¹‰ | å“åº”æ—¶é—´ | ç¤ºä¾‹ |
|-------|-----------|---------------|---------|
| SEV-1 | æœåŠ¡å®Œå…¨ä¸­æ–­ï¼Œå½±å“æ”¶å…¥ | 15åˆ†é’Ÿ | ç½‘ç«™ç˜«ç—ªï¼Œæ”¯ä»˜åŠŸèƒ½å¤±æ•ˆ |
| SEV-2 | ä¸»è¦åŠŸèƒ½æ•…éšœï¼Œä½†æœ‰ä¸´æ—¶è§£å†³æ–¹æ¡ˆ | 30åˆ†é’Ÿ | æœç´¢åŠŸèƒ½å¤±æ•ˆï¼Œé¡µé¢åŠ è½½ç¼“æ…¢ |
| SEV-3 | è¾ƒå°åŠŸèƒ½æ•…éšœï¼Œå½±å“è¾ƒå° | 4å°æ—¶ | ç®¡ç†é¢æ¿å‡ºç°é”™è¯¯ï¼Œéå…³é”®APIæ•…éšœ |
| SEV-4 | ä»…å½±å“å¤–è§‚æˆ–ç”¨æˆ·ä½“éªŒ | ä¸‹ä¸€ä¸ªå¼€å‘å‘¨æœŸå¤„ç† | æ–‡æœ¬æ ¼å¼é”™è¯¯ï¼ŒUIç•Œé¢æœ‰å°é—®é¢˜ |

### äº‹ä»¶å¤„ç†å·¥ä½œæµç¨‹

```
1. DETECT (automated or reported)
   â†’ Alert fires / user reports issue
   â†’ Create incident channel: #inc-YYYY-MM-DD-description

2. TRIAGE (first 5 minutes)
   â†’ Assign Incident Commander (IC)
   â†’ Determine severity level
   â†’ Post initial assessment in channel
   â†’ Update status page (if customer-facing)

3. MITIGATE (focus on stopping the bleeding)
   â†’ Can we rollback? â†’ Do it
   â†’ Can we scale up? â†’ Do it
   â†’ Can we feature-flag disable? â†’ Do it
   â†’ DON'T debug root cause yet â€” restore service first

4. RESOLVE
   â†’ Confirm service restored (metrics, customer reports)
   â†’ Communicate resolution to stakeholders
   â†’ Update status page

5. POST-MORTEM (within 48 hours)
   â†’ Blameless â€” focus on systems, not people
   â†’ Timeline of events
   â†’ Root cause analysis (5 Whys)
   â†’ Action items with owners and deadlines
   â†’ Share with team
```

### äº‹ä»¶äº‹ååˆ†ææ¨¡æ¿

```markdown
# Incident Post-Mortem: [Title]

**Date:** YYYY-MM-DD
**Duration:** Xh Ym
**Severity:** SEV-X
**Incident Commander:** [name]
**Author:** [name]

## Summary
[1-2 sentence summary of what happened and impact]

## Impact
- Users affected: [number/percentage]
- Revenue impact: [if applicable]
- Duration: [start to full resolution]

## Timeline (all times UTC)
| Time | Event |
|------|-------|
| 14:00 | Deploy v2.3.1 begins |
| 14:05 | Error rate spikes to 15% |
| 14:07 | Alert fires, IC paged |
| 14:12 | Rollback initiated |
| 14:15 | Service restored |

## Root Cause
[Technical explanation â€” what actually broke and why]

## Contributing Factors
- [Factor 1 â€” e.g., migration not tested with production data volume]
- [Factor 2 â€” e.g., canary deployment not configured for this service]

## What Went Well
- [Fast detection â€” alert fired within 2 minutes]
- [Clear runbook â€” IC knew rollback procedure]

## What Went Wrong
- [No canary â€” went straight to 100% rollout]
- [Migration was not backward-compatible]

## Action Items
| Action | Owner | Due | Priority |
|--------|-------|-----|----------|
| Add canary to deployment | @engineer | YYYY-MM-DD | P1 |
| Add migration backward-compat check | @engineer | YYYY-MM-DD | P1 |
| Update runbook for this service | @sre | YYYY-MM-DD | P2 |

## Lessons Learned
[Key takeaways for the team]
```

### ç´§æ€¥æƒ…å†µä¸‹çš„åº”å¯¹æœ€ä½³å®è·µ

```yaml
on_call:
  rotation: weekly
  handoff: Monday 10:00 (overlap 1h with previous)
  escalation:
    - primary: respond within 15 min
    - secondary: auto-page if no ack in 15 min
    - manager: auto-page if no ack in 30 min

  expectations:
    - Laptop + internet within reach
    - Respond to page within 15 minutes
    - Follow runbook first, improvise second
    - Escalate early â€” "I don't know" is fine
    - Update incident channel every 15 min during active incident

  wellness:
    - No more than 1 week in 4 on-call
    - Comp time after major incidents
    - Toil budget: <30% of on-call time should be toil
    - Quarterly review: are we paging too much?
```

## ç¬¬ä¹é˜¶æ®µï¼šå®‰å…¨åŠ å›ºæªæ–½

### CIç®¡é“ä¸­çš„å®‰å…¨æ£€æŸ¥

```yaml
security_gates:
  # Pre-commit
  - tool: gitleaks / trufflehog
    what: Secret detection in code
    block: true

  # Build
  - tool: semgrep / CodeQL
    what: Static analysis (SAST)
    block: critical findings

  - tool: npm audit / pip audit / cargo audit
    what: Dependency vulnerabilities (SCA)
    block: critical/high

  # Container
  - tool: trivy / grype
    what: Image vulnerability scan
    block: critical

  - tool: hadolint
    what: Dockerfile best practices
    block: error level

  # Deploy
  - tool: checkov / tfsec
    what: IaC security scan
    block: high findings

  # Runtime
  - tool: falco / sysdig
    what: Runtime anomaly detection
    alert: true
```

### æœºå¯†ä¿¡æ¯ç®¡ç†ç­–ç•¥

| æ–¹æ³• | å®‰å…¨æ€§ | å¤æ‚æ€§ | é€‚ç”¨åœºæ™¯ |
|--------|----------|------------|----------|
| CI/CDè¿‡ç¨‹ä¸­çš„ç¯å¢ƒå˜é‡ç®¡ç† | åŸºç¡€å®‰å…¨æªæ–½ | é€‚ç”¨äºå°å‹å›¢é˜Ÿå’Œéå…³é”®åœºæ™¯ |
| AWS Secrets Manager / GCP Secret Manager | é«˜çº§å®‰å…¨æªæ–½ | é€‚ç”¨äºå¤æ‚ç¯å¢ƒæˆ–å¯¹å®‰å…¨æ€§è¦æ±‚è¾ƒé«˜çš„åœºæ™¯ |
| HashiCorp Vault | æœ€é«˜çº§å®‰å…¨æªæ–½ | é€‚ç”¨äºå¤šäº‘ç¯å¢ƒæˆ–å¯¹åˆè§„æ€§è¦æ±‚ä¸¥æ ¼çš„åº”ç”¨ |
| SOPS + git | è‰¯å¥½çš„å®‰å…¨å®è·µ | é€‚ç”¨äºGitOpså·¥ä½œæµç¨‹ |

**å®‰å…¨å»ºè®®ï¼š**
- è‡³å°‘æ¯90å¤©æ›´æ–°ä¸€æ¬¡æœºå¯†ä¿¡æ¯ã€‚
- ä¸åŒç¯å¢ƒï¼ˆå¼€å‘ç¯å¢ƒã€æµ‹è¯•ç¯å¢ƒã€ç”Ÿäº§ç¯å¢ƒï¼‰ä½¿ç”¨ä¸åŒçš„æœºå¯†ä¿¡æ¯ã€‚
- å®¡è®¡æ‰€æœ‰å¯¹æœºå¯†ä¿¡æ¯çš„è®¿é—®è¡Œä¸ºã€‚
- åœ¨CIè¾“å‡ºä¸­éšè—æœºå¯†ä¿¡æ¯ã€‚
- å°½å¯èƒ½ä½¿ç”¨OIDC/æ— å¯†é’¥è®¤è¯æœºåˆ¶ï¼ˆé¿å…ä½¿ç”¨é•¿æœŸæœ‰æ•ˆçš„ä»¤ç‰Œï¼‰ã€‚

### ç½‘ç»œå®‰å…¨åŸºæœ¬è¦æ±‚

```
1. Default deny all â€” explicitly allow what's needed
2. TLS everywhere â€” including internal service-to-service
3. No public IPs on internal services â€” use load balancers / API gateways
4. WAF on public endpoints â€” OWASP Top 10 rules minimum
5. Rate limiting on all APIs â€” prevent abuse and DDoS
6. DNS for service discovery â€” never hardcode IPs
7. VPN or zero-trust for admin access â€” no SSH from internet
8. Network policies in K8s â€” pods can't talk to everything
9. Egress control â€” services should only reach what they need
10. Certificate auto-renewal â€” cert-manager or ACM
```

## ç¬¬åé˜¶æ®µï¼šè¿ç»´æœ€ä½³å®è·µï¼ˆSREï¼‰

### SLOï¼ˆService Level Objectiveï¼‰æ¡†æ¶

```yaml
# Define SLOs for every user-facing service
service: checkout-api
slos:
  availability:
    target: 99.95%        # 4.38 hours downtime/year
    window: 30d rolling
    measurement: "successful_requests / total_requests"

  latency:
    target: 99%           # 99% of requests under threshold
    threshold: 500ms      # p99 < 500ms
    window: 30d rolling

  freshness:
    target: 99.9%         # Data updated within SLA
    threshold: 5m
    window: 30d rolling

error_budget:
  monthly_budget: 0.05%   # ~21.6 minutes
  burn_rate_alert:
    fast: 14.4x           # Budget consumed in 1 hour â†’ page
    slow: 3x              # Budget consumed in 10 hours â†’ ticket
  policy:
    budget_exhausted:
      - freeze non-critical deploys
      - redirect eng effort to reliability
      - review in weekly SRE sync
```

### é™ä½è¿ç»´å·¥ä½œé‡çš„æ–¹æ³•

```
Toil = manual, repetitive, automatable, reactive, no lasting value

Track toil:
  - Log manual interventions for 2 weeks
  - Categorize: deployment, scaling, cert renewal, data fixes, permissions
  - Prioritize: frequency Ã— time Ã— frustration

Target: <30% of engineering time on toil
If toil > 50%: stop feature work, automate the top 3 toil items

Common toil automation:
  Manual deploys         â†’ CI/CD pipeline
  Certificate renewal    â†’ cert-manager / ACM
  Scaling up/down        â†’ HPA / auto-scaling groups
  Permission requests    â†’ Self-service IAM with approval
  Data fixes             â†’ Admin API / scripts
  Dependency updates     â†’ Renovate / Dependabot
  Flaky test management  â†’ Auto-quarantine + ticket
```

### å®¹é‡è§„åˆ’å»ºè®®

```yaml
capacity_review:
  frequency: monthly
  inputs:
    - current_utilization: "CPU, memory, disk, network per service"
    - growth_rate: "request rate trend over 90 days"
    - planned_events: "launches, marketing campaigns, seasonal peaks"
    - headroom_target: 30%  # Don't run above 70% sustained

  formula:
    needed_capacity: "current_usage Ã— (1 + growth_rate) Ã— (1 + headroom)"
    lead_time: "14 days for cloud, 60+ days for hardware"

  actions:
    - "If utilization > 70%: plan scaling within 2 weeks"
    - "If utilization > 85%: emergency scaling NOW"
    - "If utilization < 30%: rightsize down (save money)"
```

## ç¬¬åä¸€é˜¶æ®µï¼šæˆæœ¬ä¼˜åŒ–

### äº‘æœåŠ¡æˆæœ¬ç®¡ç†

### äº‘æœåŠ¡æˆæœ¬ç®¡ç†è§„åˆ™

```
1. Right-size first â€” most instances are overprovisioned
   Check: actual CPU/memory usage vs provisioned (CloudWatch, Datadog)
   Action: downsize to next tier that maintains 70% headroom

2. Reserved capacity for baseline â€” spot/preemptible for burst
   Pattern: 60% reserved + 30% on-demand + 10% spot
   Savings: 40-70% on reserved vs on-demand

3. Auto-scale to zero when possible
   - Dev/staging environments: scale down nights + weekends
   - Serverless for bursty workloads (Lambda, Cloud Functions)

4. Delete zombie resources monthly
   - Unattached EBS volumes
   - Old snapshots (>90 days, not tagged for retention)
   - Unused load balancers
   - Orphaned Elastic IPs

5. Storage tiering
   - Hot: SSD (frequently accessed)
   - Warm: HDD (monthly access)
   - Cold: S3 Glacier / Archive (yearly access)
   - Auto-lifecycle policies on S3 buckets

6. Tag everything â€” untagged = untracked = wasted
   Required tags: environment, team, service, cost-center
   Weekly report: cost by tag, highlight untagged resources
```

### æœˆåº¦æˆæœ¬å®¡æŸ¥æ¨¡æ¿

```markdown
## Cloud Cost Review â€” [Month YYYY]

### Summary
- Total spend: $X,XXX (vs budget: $X,XXX)
- MoM change: +X% ($XXX)
- Top 3 cost drivers: [service1, service2, service3]

### By Service
| Service | Cost | % of Total | MoM Change | Action |
|---------|------|-----------|------------|--------|
| EKS | $XXX | XX% | +X% | Right-size node group |
| RDS | $XXX | XX% | 0% | Consider reserved |
| S3 | $XXX | XX% | +X% | Add lifecycle rules |

### Optimization Actions Taken
- [Action 1]: Saved $XXX/mo
- [Action 2]: Saved $XXX/mo

### Next Month Actions
- [ ] [Action with estimated savings]
```

### DevOpsæˆç†Ÿåº¦è¯„ä¼°

è¯„ä¼°å›¢é˜Ÿçš„DevOpsæˆç†Ÿåº¦ï¼ˆæ¯ä¸ªç»´åº¦è¯„åˆ†1-5åˆ†ï¼‰ï¼š

| ç»´åº¦ | 1ï¼ˆåˆæ­¥é˜¶æ®µï¼‰ | 3ï¼ˆåŸºæœ¬å®Œå–„ï¼‰ | 5ï¼ˆé«˜åº¦ä¼˜åŒ–ï¼‰ |
|--------|-----------|-------------|----------------|
| **CI/CD** | æ‰‹åŠ¨éƒ¨ç½² | è‡ªåŠ¨åŒ–ç®¡é“ï¼Œä½†éœ€è¦äººå·¥å®¡æ ¸ | å®Œå…¨è‡ªåŠ¨åŒ–éƒ¨ç½²ï¼Œæ”¯æŒå¿«é€Ÿåˆ‡æ¢åˆ°æ–°ç‰ˆæœ¬ï¼ˆ<15åˆ†é’Ÿï¼‰ |
| **åŸºç¡€è®¾æ–½å³ä»£ç ï¼ˆIaCï¼‰** | é€šè¿‡ç‚¹å‡»æ“ä½œç®¡ç†é…ç½® | éƒ¨åˆ†ä½¿ç”¨Terraformï¼Œéœ€è¦æ‰‹åŠ¨è°ƒæ•´ | 100%è‡ªåŠ¨åŒ–éƒ¨ç½²ï¼Œå…·å¤‡è‡ªåŠ¨æ¢å¤æœºåˆ¶ |
| **ç›‘æ§** | å‘ç”Ÿé—®é¢˜æ—¶æ‰è¿›è¡Œæ£€æŸ¥ | é€šè¿‡ä»ªè¡¨æ¿å’ŒåŸºæœ¬è­¦æŠ¥è¿›è¡Œç›‘æ§ | è®¾å®šæœåŠ¡æ°´å¹³ç›®æ ‡ï¼ˆSLOsï¼‰ï¼Œå…·å¤‡è‡ªåŠ¨æ¢å¤æœºåˆ¶ |
| **äº‹ä»¶å“åº”** | ä¾èµ–ç´§æ€¥å“åº”æœºåˆ¶å’ŒSSHè¿œç¨‹ç™»å½• | æœ‰å®Œå–„çš„äº‹ä»¶å¤„ç†æµç¨‹å’Œå€¼ç­åˆ¶åº¦ | è¿›è¡Œäº‹ååˆ†æï¼Œé‡‡ç”¨æ··æ²Œå·¥ç¨‹ï¼ˆchaos engineeringï¼‰ |
| **å®‰å…¨æ€§** | å®šæœŸè¿›è¡Œå®‰å…¨å®¡è®¡ | ä½¿ç”¨CIæ‰«æå·¥å…·å’Œæœºå¯†ä¿¡æ¯ç®¡ç†å·¥å…· | é‡‡ç”¨é¢„é˜²æ€§å®‰å…¨æªæ–½ |

**è¯„åˆ†è¯´æ˜ï¼š**
- 6-12åˆ†ï¼šå¤„äºåŸºç¡€é˜¶æ®µï¼Œéœ€è¦é‡ç‚¹æå‡CI/CDå’ŒåŸºæœ¬ç›‘æ§èƒ½åŠ›ã€‚
- 13-20åˆ†ï¼šæ­£åœ¨å‘å±•ä¸­ï¼Œéœ€è¦å¼•å…¥åŸºç¡€è®¾æ–½å³ä»£ç ï¼ˆIaCï¼‰å’Œäº‹ä»¶å“åº”æœºåˆ¶ã€‚
- 21-26åˆ†ï¼šå·²ç»è¾ƒä¸ºæˆç†Ÿï¼Œå…·å¤‡SREæœ€ä½³å®è·µå’Œæˆæœ¬ç®¡ç†èƒ½åŠ›ã€‚
- 27-30åˆ†ï¼šå¤„äºé«˜çº§é˜¶æ®µï¼Œæ³¨é‡æ··æ²Œå·¥ç¨‹å’Œæå‡å¼€å‘è€…çš„å·¥ä½œä½“éªŒã€‚

### å¸¸ç”¨å‘½ä»¤ç¤ºä¾‹

- â€œä¸ºæˆ‘çš„Node.jsé¡¹ç›®è®¾ç½®CI/CDæµç¨‹ã€‚â€
- â€œä¸ºæˆ‘çš„Python APIåˆ›å»ºä¸€ä¸ªDockerfileã€‚â€
- â€œä¸ºä½¿ç”¨RDSçš„ECSæœåŠ¡ç¼–å†™Terraformé…ç½®ã€‚â€
- â€œä¸ºæˆ‘çš„æœåŠ¡è®¾è®¡ä¸€ä¸ªç›‘æ§ä»ªè¡¨æ¿ã€‚â€
- â€œå¸®æˆ‘åˆ†ææ˜¨å¤©çš„ç³»ç»Ÿæ•…éšœã€‚â€
- â€œè¯„ä¼°æˆ‘çš„Kuberneteséƒ¨ç½²æ˜¯å¦å…·å¤‡ç”Ÿäº§ç¯å¢ƒæ‰€éœ€çš„å‡†å¤‡å°±ç»ªçŠ¶æ€ã€‚â€
- â€œæˆ‘åº”è¯¥é€‰æ‹©å“ªç§éƒ¨ç½²ç­–ç•¥ï¼Ÿâ€
- â€œå¸®æˆ‘è®¾ç½®è­¦æŠ¥è§„åˆ™ã€‚â€
- â€œä¸ºæ•°æ®åº“æ•…éšœåˆ¶å®šäº‹ä»¶å“åº”æµç¨‹ã€‚â€
- â€œå®¡è®¡æˆ‘çš„äº‘æœåŠ¡æˆæœ¬å¹¶æå‡ºä¼˜åŒ–å»ºè®®ã€‚â€
- â€œè¯„ä¼°æˆ‘ä»¬çš„DevOpsæˆç†Ÿåº¦ã€‚â€
- â€œä¸ºæˆ‘ä»¬çš„CIç®¡é“é…ç½®æœºå¯†ä¿¡æ¯ç®¡ç†æœºåˆ¶ã€‚â€