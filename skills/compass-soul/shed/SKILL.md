---
name: shed
description: >
  针对长时间运行的大型语言模型（LLM）代理，需要关注“上下文窗口的维护”问题。具体来说，需要制定决策规则来决定何时以及如何压缩、屏蔽、切换或委托上下文数据——这些规则基于相关研究（JetBrains/NeurIPS 2025、OpenHands、Letta/MemGPT、LLMLingua）。这些规则在以下情况下尤为适用：  
  1. 当代理长时间运行、生成大量输出数据时；  
  2. 当上下文数据量接近系统容量限制时；  
  3. 当代理因数据压缩或溢出问题而出现性能下降时；  
  4. 在设计需要长期管理上下文数据的代理架构时。
---
# Shed — 代理的上下文管理工具

*舍弃不需要的内容，保留重要的信息。*

该工具的名称来源于“蜕皮”这一过程——通过蜕去旧的外层来促进成长。你的上下文窗口就如同你的“皮肤”；当它变得过于臃肿时，就需要及时“蜕去”那些无用的信息。

## 核心原则

**工具的输出占你上下文信息的84%，但却是价值最低的部分。**（Lindenbauer等人，2025年NeurIPS DL4C研讨会，基于SWE-agent实验得出）所有操作都应围绕这一原则展开。

## 规则

### 每次使用工具后

1. **提取关键信息，切勿累积。** 当工具返回大量输出（文件内容、搜索结果、日志、API响应等）时，立即将关键事实写入文件或以列表形式压缩保存。原始输出可以视为“一次性使用”的数据。
2. **问自己：“我以后还需要这些原始数据吗？”** 几乎不需要。真正重要的是你提取出的关键信息，而不是那些包含原始数据的500行内容。

### 当上下文信息达到约70%时

3. **主动进行压缩处理。** 不要等待平台自动压缩——否则你会失去对自身内存使用的控制权。当上下文信息达到70%时，应立即进行压缩。
4. **优先屏蔽旧的工具输出**（无需调用大型语言模型LLM）。保留你的推理过程和操作历史记录——你需要的是决策链，而不是20分钟前生成的`ls -la`命令的输出。
5. **仅将推理过程作为备份进行压缩。** 如果屏蔽还不够，再对旧的推理过程进行压缩。但这样做会有一定的信息损失，并且需要消耗额外的LLM调用资源——因此要谨慎使用。
6. **切勿重新压缩已压缩过的内容。** 如果你已经对信息进行了压缩，而上下文信息仍在增长，应切换上下文或创建一个新的代理。递归压缩可能会导致错误累积。

### 完成任务后

7. **将结果写入文件，然后立即切换上下文。** 旧的任务上下文会对新任务产生负面影响，因此不要将其保留。
8. **留下“线索”。** 在切换上下文之前，记录下你做了什么、下一步该做什么以及文件的位置（格式为`memory/YYYY-MM-DD.md`）。未来的你需要这些“线索”，而不仅仅是原始的记录。

### 委派任务时

9. **为复杂的子任务创建新的代理。** 你的上下文信息可能对它们来说是无用的“噪音”。为它们提供清晰、必要的提示。
10. **不要将父代理的上下文信息传递给子代理。** 自动生成（AutoGen）模式要求每个代理都有自己的信息使用预算。继承过多的上下文信息只会降低它们的工作效率。

### 架构（针对代理构建者）

11. **将上下文信息结构化为有明确大小限制的块。** 所有的生产框架都应遵循这一原则——Letta使用带有大小标签的块（如“人类信息”、“角色信息”、“知识信息”等）。过于庞大的上下文信息难以管理。
12. **区分工作内存（当前上下文中的信息）和参考内存（文件/数据库中的信息）。** 你的有效上下文信息通常比窗口显示的范围要小得多。在长篇上下文中，模型很容易丢失重要信息。
13. **将关键信息放在上下文的开头或结尾，切勿放在中间。** 根据Hsieh等人的研究（2024年），位置性注意力机制会使得中间部分的信息被低估（最多低15个百分点）。

## 复杂性陷阱

不要认为复杂的压缩方法（如LLM生成的摘要）一定优于简单的处理方式（如信息屏蔽）。JetBrains的“复杂性陷阱”论文（2025年）在SWE-bench平台上对5种模型配置进行了测试，结果如下：
- 简单的信息屏蔽方法**将成本降低了一半**；
- 信息屏蔽方法**与LLM生成的摘要方法的效果相当或更优**；
- 例如：使用信息屏蔽后，Qwen3-Coder的效率从53.8%提升到了54.8%。

**结论**：从简单的方法开始尝试。首先屏蔽工具的输出，只有在信息屏蔽无法满足需求时，再考虑使用摘要功能。

## 成本模型

如果不进行干预，每轮操作的成本会**呈二次方增长**（每轮操作不仅会生成新的信息，还会重新处理之前的所有信息）。定期进行压缩处理可以将成本增长变为**线性增长**。OpenHands的研究表明，使用他们的压缩工具可以将成本降低一半。

## 快速参考

| 情况 | 应采取的行动 |
|-----------|--------|
| 工具返回大量输出 | 提取关键信息并保存到文件中，丢弃原始数据 |
| 上下文信息达到约70% | 屏蔽旧的工具输出 |
| 信息屏蔽后上下文仍在增长 | 对最旧的推理过程进行压缩 |
| 任务完成 | 将结果写入文件并立即切换上下文 |
| 需要处理复杂的子任务 | 创建新的代理 |
| 信息已经压缩但仍需继续处理 | 切换上下文或创建新的代理 |
| 需要保留的关键信息 | 将其放在上下文的开头或结尾 |

## 参考文献

- Lindenbauer等人，《复杂性陷阱》（NeurIPS 2025 DL4C）：https://arxiv.org/abs/2508.21433
- OpenHands的上下文压缩技术（2025年）：https://openhands.dev/blog/openhands-context-condensation-for-more-efficient-ai-agents
- Letta/MemGPT的记忆块机制：https://www.letta.com/blog/memory-blocks
- LLMLingua-2（ACL 2024年）：https://aclanthology.org/2024.acl-long.91/
- Liu等人，《迷失在中间》（2023年）：https://arxiv.org/abs/2307.03172
- Hsieh等人，《发现关键信息的位置》（2024年）：https://arxiv.org/abs/2406.16008
- MEM1动态状态管理技术（2025年）：https://arxiv.org/abs/2506.15841