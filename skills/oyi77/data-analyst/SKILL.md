---
name: data-analyst
version: 1.0.0
description: "æ•°æ®å¯è§†åŒ–ã€æŠ¥å‘Šç”Ÿæˆã€SQLæŸ¥è¯¢ä»¥åŠç”µå­è¡¨æ ¼è‡ªåŠ¨åŒ–åŠŸèƒ½â€”â€”å°†æ‚¨çš„äººå·¥æ™ºèƒ½ä»£ç†è½¬å˜ä¸ºä¸€ä¸ªç²¾é€šæ•°æ®çš„åˆ†æå¸ˆï¼Œèƒ½å¤Ÿå°†åŸå§‹æ•°æ®è½¬åŒ–ä¸ºå¯æ“ä½œçš„æ´å¯Ÿã€‚"
author: openclaw
---

# æ•°æ®åˆ†æå¸ˆæŠ€èƒ½ ğŸ“Š

**å°†æ‚¨çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹æ‰“é€ æˆå¼ºå¤§çš„æ•°æ®åˆ†æå·¥å…·ã€‚**

èƒ½å¤ŸæŸ¥è¯¢æ•°æ®åº“ã€åˆ†æç”µå­è¡¨æ ¼ã€åˆ›å»ºå¯è§†åŒ–å›¾è¡¨ï¼Œå¹¶ç”Ÿæˆæœ‰åŠ©äºå†³ç­–çš„æ´å¯Ÿã€‚

---

## è¯¥æŠ€èƒ½çš„åŠŸèƒ½

âœ… **SQLæŸ¥è¯¢** â€” ç¼–å†™å¹¶æ‰§è¡Œé’ˆå¯¹æ•°æ®åº“çš„æŸ¥è¯¢  
âœ… **ç”µå­è¡¨æ ¼åˆ†æ** â€” å¤„ç†CSVã€Excelã€Google Sheetsä¸­çš„æ•°æ®  
âœ… **æ•°æ®å¯è§†åŒ–** â€” åˆ›å»ºå›¾è¡¨ã€å›¾å½¢å’Œä»ªè¡¨æ¿  
âœ… **æŠ¥å‘Šç”Ÿæˆ** â€” è‡ªåŠ¨ç”ŸæˆåŒ…å«æ´å¯Ÿçš„æŠ¥å‘Š  
âœ… **æ•°æ®æ¸…æ´—** â€” å¤„ç†ç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼å’Œæ•°æ®æ ¼å¼é—®é¢˜  
âœ… **ç»Ÿè®¡åˆ†æ** â€” è¿›è¡Œæè¿°æ€§ç»Ÿè®¡åˆ†æã€è¶‹åŠ¿åˆ†æåŠç›¸å…³æ€§åˆ†æ  

---

## å¿«é€Ÿå…¥é—¨

1. åœ¨ `TOOLS.md` ä¸­é…ç½®æ‚¨çš„æ•°æ®æºï¼š  
```markdown
### Data Sources
- Primary DB: [Connection string or description]
- Spreadsheets: [Google Sheets URL / local path]
- Data warehouse: [BigQuery/Snowflake/etc.]
```  

2. è®¾ç½®æ‚¨çš„å·¥ä½œç¯å¢ƒï¼š  
```bash
./scripts/data-init.sh
```  

3. å¼€å§‹åˆ†æå§ï¼  

---

## SQLæŸ¥è¯¢æ¨¡å¼

### å¸¸è§æŸ¥è¯¢æ¨¡æ¿

**åŸºç¡€æ•°æ®æ¢ç´¢**  
```sql
-- Row count
SELECT COUNT(*) FROM table_name;

-- Sample data
SELECT * FROM table_name LIMIT 10;

-- Column statistics
SELECT 
    column_name,
    COUNT(*) as count,
    COUNT(DISTINCT column_name) as unique_values,
    MIN(column_name) as min_val,
    MAX(column_name) as max_val
FROM table_name
GROUP BY column_name;
```  

**åŸºäºæ—¶é—´çš„æ•°æ®åˆ†æ**  
```sql
-- Daily aggregation
SELECT 
    DATE(created_at) as date,
    COUNT(*) as daily_count,
    SUM(amount) as daily_total
FROM transactions
GROUP BY DATE(created_at)
ORDER BY date DESC;

-- Month-over-month comparison
SELECT 
    DATE_TRUNC('month', created_at) as month,
    COUNT(*) as count,
    LAG(COUNT(*)) OVER (ORDER BY DATE_TRUNC('month', created_at)) as prev_month,
    (COUNT(*) - LAG(COUNT(*)) OVER (ORDER BY DATE_TRUNC('month', created_at))) / 
        NULLIF(LAG(COUNT(*)) OVER (ORDER BY DATE_TRUNC('month', created_at)), 0) * 100 as growth_pct
FROM transactions
GROUP BY DATE_TRUNC('month', created_at)
ORDER BY month;
```  

**ç¾¤ä½“åˆ†æ**  
```sql
-- User cohort by signup month
SELECT 
    DATE_TRUNC('month', u.created_at) as cohort_month,
    DATE_TRUNC('month', o.created_at) as activity_month,
    COUNT(DISTINCT u.id) as users
FROM users u
LEFT JOIN orders o ON u.id = o.user_id
GROUP BY cohort_month, activity_month
ORDER BY cohort_month, activity_month;
```  

**æ¼æ–—åˆ†æ**  
```sql
-- Conversion funnel
WITH funnel AS (
    SELECT
        COUNT(DISTINCT CASE WHEN event = 'page_view' THEN user_id END) as views,
        COUNT(DISTINCT CASE WHEN event = 'signup' THEN user_id END) as signups,
        COUNT(DISTINCT CASE WHEN event = 'purchase' THEN user_id END) as purchases
    FROM events
    WHERE date >= CURRENT_DATE - INTERVAL '30 days'
)
SELECT 
    views,
    signups,
    ROUND(signups * 100.0 / NULLIF(views, 0), 2) as signup_rate,
    purchases,
    ROUND(purchases * 100.0 / NULLIF(signups, 0), 2) as purchase_rate
FROM funnel;
```  

---

## æ•°æ®æ¸…æ´—

### å¸¸è§çš„æ•°æ®è´¨é‡é—®é¢˜

| é—®é¢˜ | æ£€æµ‹æ–¹æ³• | è§£å†³æ–¹æ¡ˆ |
|-------|-----------|----------|
| **ç¼ºå¤±å€¼** | `IS NULL` æˆ–ç©ºå­—ç¬¦ä¸² | ç”¨é»˜è®¤å€¼å¡«å……ã€åˆ é™¤æˆ–æ ‡è®°ä¸ºç¼ºå¤±å€¼ |
| **é‡å¤å€¼** | ä½¿ç”¨ `GROUP BY` å’Œ `HAVING COUNT(*) > 1` è¿›è¡Œå»é‡ |
| **å¼‚å¸¸å€¼** | Zåˆ†æ•° > 3 æˆ– IQR æ–¹æ³• | è°ƒæŸ¥å¹¶å¤„ç†æˆ–æ’é™¤å¼‚å¸¸å€¼ |
| **æ ¼å¼ä¸ä¸€è‡´** | é€šè¿‡æŠ½æ ·å’Œæ¨¡å¼åŒ¹é…è¿›è¡Œæ ‡å‡†åŒ– |
| **æ— æ•ˆå€¼** | è¿›è¡ŒèŒƒå›´æ£€æŸ¥å¹¶éªŒè¯æ•°æ®çš„æœ‰æ•ˆæ€§ |

### æ•°æ®æ¸…æ´—ç›¸å…³çš„SQLè¯­å¥  

```sql
-- Find duplicates
SELECT email, COUNT(*)
FROM users
GROUP BY email
HAVING COUNT(*) > 1;

-- Find nulls
SELECT 
    COUNT(*) as total,
    SUM(CASE WHEN email IS NULL THEN 1 ELSE 0 END) as null_emails,
    SUM(CASE WHEN name IS NULL THEN 1 ELSE 0 END) as null_names
FROM users;

-- Standardize text
UPDATE products
SET category = LOWER(TRIM(category));

-- Remove outliers (IQR method)
WITH stats AS (
    SELECT 
        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY value) as q1,
        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY value) as q3
    FROM data
)
SELECT * FROM data, stats
WHERE value BETWEEN q1 - 1.5*(q3-q1) AND q3 + 1.5*(q3-q1);
```  

### æ•°æ®æ¸…æ´—æ£€æŸ¥æ¸…å•  

```markdown
# Data Quality Audit: [Dataset]

## Row-Level Checks
- [ ] Total row count: [X]
- [ ] Duplicate rows: [X]
- [ ] Rows with any null: [X]

## Column-Level Checks
| Column | Type | Nulls | Unique | Min | Max | Issues |
|--------|------|-------|--------|-----|-----|--------|
| [col] | [type] | [n] | [n] | [v] | [v] | [notes] |

## Data Lineage
- Source: [Where data came from]
- Last updated: [Date]
- Known issues: [List]

## Cleaning Actions Taken
1. [Action and reason]
2. [Action and reason]
```  

---

## ç”µå­è¡¨æ ¼åˆ†æ

### ä½¿ç”¨Pythonå¤„ç†CSV/Excelæ–‡ä»¶  

```python
import pandas as pd

# Load data
df = pd.read_csv('data.csv')  # or pd.read_excel('data.xlsx')

# Basic exploration
print(df.shape)  # (rows, columns)
print(df.info())  # Column types and nulls
print(df.describe())  # Numeric statistics

# Data cleaning
df = df.drop_duplicates()
df['date'] = pd.to_datetime(df['date'])
df['amount'] = df['amount'].fillna(0)

# Analysis
summary = df.groupby('category').agg({
    'amount': ['sum', 'mean', 'count'],
    'quantity': 'sum'
}).round(2)

# Export
summary.to_csv('analysis_output.csv')
```  

### å¸¸ç”¨çš„Pandasæ“ä½œ  

```python
# Filtering
filtered = df[df['status'] == 'active']
filtered = df[df['amount'] > 1000]
filtered = df[df['date'].between('2024-01-01', '2024-12-31')]

# Aggregation
by_category = df.groupby('category')['amount'].sum()
pivot = df.pivot_table(values='amount', index='month', columns='category', aggfunc='sum')

# Window functions
df['running_total'] = df['amount'].cumsum()
df['pct_change'] = df['amount'].pct_change()
df['rolling_avg'] = df['amount'].rolling(window=7).mean()

# Merging
merged = pd.merge(df1, df2, on='id', how='left')
```  

---

## æ•°æ®å¯è§†åŒ–

### å›¾è¡¨é€‰æ‹©æŒ‡å—

| æ•°æ®ç±»å‹ | æœ€é€‚åˆçš„å›¾è¡¨ | é€‚ç”¨åœºæ™¯ |
|-----------|------------|----------|
| éšæ—¶é—´å˜åŒ–çš„è¶‹åŠ¿ | æŠ˜çº¿å›¾ | æ˜¾ç¤ºéšæ—¶é—´çš„å˜åŒ–æ¨¡å¼ |
| ç±»åˆ«æ¯”è¾ƒ | æ¡å½¢å›¾ | æ¯”è¾ƒä¸åŒç±»åˆ«çš„æ•°æ® |
| éƒ¨åˆ†ä¸æ•´ä½“çš„å…³ç³» | é¥¼å›¾/åœ†ç¯å›¾ | æ˜¾ç¤ºæ¯”ä¾‹ï¼ˆâ‰¤5ä¸ªç±»åˆ«ï¼‰ |
| æ•°æ®åˆ†å¸ƒ | ç›´æ–¹å›¾ | äº†è§£æ•°æ®åˆ†å¸ƒæƒ…å†µ |
| å˜é‡ç›¸å…³æ€§ | æ•£ç‚¹å›¾ | åˆ†æä¸¤ä¸ªå˜é‡ä¹‹é—´çš„å…³ç³» |
| å¤šä¸ªç±»åˆ«çš„æ•°æ® | æ°´å¹³æ¡å½¢å›¾ | å¯¹å¤šä¸ªé¡¹ç›®è¿›è¡Œæ’åæˆ–æ¯”è¾ƒ |
| åœ°ç†æ•°æ® | åœ°å›¾ | æ˜¾ç¤ºåœ°ç†ä½ç½®ç›¸å…³çš„æ•°æ® |

### ä½¿ç”¨Matplotlib/Seabornè¿›è¡ŒPythonå¯è§†åŒ–  

```python
import matplotlib.pyplot as plt
import seaborn as sns

# Set style
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("husl")

# Line chart (trends)
plt.figure(figsize=(10, 6))
plt.plot(df['date'], df['value'], marker='o')
plt.title('Trend Over Time')
plt.xlabel('Date')
plt.ylabel('Value')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('trend.png', dpi=150)

# Bar chart (comparisons)
plt.figure(figsize=(10, 6))
sns.barplot(data=df, x='category', y='amount')
plt.title('Amount by Category')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('comparison.png', dpi=150)

# Heatmap (correlations)
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Matrix')
plt.tight_layout()
plt.savefig('correlation.png', dpi=150)
```  

### ASCIIå›¾è¡¨ï¼ˆå¿«é€Ÿç»ˆç«¯å¯è§†åŒ–ï¼‰

å½“æ— æ³•ç”Ÿæˆå›¾åƒæ—¶ï¼Œå¯ä»¥ä½¿ç”¨ASCIIå›¾è¡¨ï¼š  
```
Revenue by Month (in $K)
========================
Jan: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 160
Feb: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 180
Mar: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 240
Apr: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 220
May: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 260
Jun: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 280
```  

---

## æŠ¥å‘Šç”Ÿæˆ

### æ ‡å‡†æŠ¥å‘Šæ¨¡æ¿  

```markdown
# [Report Name]
**Period:** [Date range]
**Generated:** [Date]
**Author:** [Agent/Human]

## Executive Summary
[2-3 sentences with key findings]

## Key Metrics

| Metric | Current | Previous | Change |
|--------|---------|----------|--------|
| [Metric] | [Value] | [Value] | [+/-X%] |

## Detailed Analysis

### [Section 1]
[Analysis with supporting data]

### [Section 2]
[Analysis with supporting data]

## Visualizations
[Insert charts]

## Insights
1. **[Insight]**: [Supporting evidence]
2. **[Insight]**: [Supporting evidence]

## Recommendations
1. [Actionable recommendation]
2. [Actionable recommendation]

## Methodology
- Data source: [Source]
- Date range: [Range]
- Filters applied: [Filters]
- Known limitations: [Limitations]

## Appendix
[Supporting data tables]
```  

### è‡ªåŠ¨åŒ–æŠ¥å‘Šè„šæœ¬  

```bash
#!/bin/bash
# generate-report.sh

# Pull latest data
python scripts/extract_data.py --output data/latest.csv

# Run analysis
python scripts/analyze.py --input data/latest.csv --output reports/

# Generate report
python scripts/format_report.py --template weekly --output reports/weekly-$(date +%Y-%m-%d).md

echo "Report generated: reports/weekly-$(date +%Y-%m-%d).md"
```  

---

## ç»Ÿè®¡åˆ†æ

### æè¿°æ€§ç»Ÿè®¡

| ç»Ÿè®¡é‡ | å«ä¹‰ | ä½¿ç”¨åœºæ™¯ |
|-----------|-------------------|----------|
| **å¹³å‡å€¼** | æ•°æ®çš„ä¸­é—´å€¼ | è¡¡é‡æ•°æ®çš„ä¸­å¿ƒè¶‹åŠ¿ |
| **ä¸­ä½æ•°** | æ•°æ®çš„ä¸­é—´å€¼ | å¯¹å¼‚å¸¸å€¼å…·æœ‰è¾ƒå¥½çš„é²æ£’æ€§ |
| **ä¼—æ•°** | å‡ºç°é¢‘ç‡æœ€é«˜çš„å€¼ | é€‚ç”¨äºåˆ†ç±»æ•°æ® |
| **æ ‡å‡†å·®** | æ•°æ®å›´ç»•å¹³å‡å€¼çš„ç¦»æ•£ç¨‹åº¦ | è¡¡é‡æ•°æ®çš„æ³¢åŠ¨æ€§ |
| **æœ€å°å€¼/æœ€å¤§å€¼** | æ•°æ®çš„èŒƒå›´ | è¡¨ç¤ºæ•°æ®çš„è¾¹ç•Œ |
| **ç™¾åˆ†ä½æ•°** | æ•°æ®åˆ†å¸ƒçš„å½¢çŠ¶ | ç”¨äºåŸºå‡†æµ‹è¯• |

### ä½¿ç”¨Pythonè¿›è¡Œå¿«é€Ÿç»Ÿè®¡åˆ†æ  

```python
# Full descriptive statistics
stats = df['amount'].describe()
print(stats)

# Additional stats
print(f"Median: {df['amount'].median()}")
print(f"Mode: {df['amount'].mode()[0]}")
print(f"Skewness: {df['amount'].skew()}")
print(f"Kurtosis: {df['amount'].kurtosis()}")

# Correlation
correlation = df['sales'].corr(df['marketing_spend'])
print(f"Correlation: {correlation:.3f}")
```  

### å¸¸è§ç»Ÿè®¡æ£€éªŒ

| æ£€éªŒæ–¹æ³• | ä½¿ç”¨åœºæ™¯ | Pythonå‡½æ•° |
|------|----------|--------|
| Tæ£€éªŒ | æ¯”è¾ƒä¸¤ä¸ªæ ·æœ¬çš„å¹³å‡å€¼ | `scipy.stats.ttest_ind(a, b)` |
| å¡æ–¹æ£€éªŒ | æ£€éªŒç±»åˆ«é—´çš„ç‹¬ç«‹æ€§ | `scipy.stats.chi2_contingency(table)` |
| æ–¹å·®åˆ†æï¼ˆANOVAï¼‰ | æ¯”è¾ƒä¸‰ä¸ªåŠä»¥ä¸Šæ ·æœ¬çš„å¹³å‡å€¼ | `scipy.stats.f_oneway(a, b, c)` |
| çš®å°”é€Šç›¸å…³ç³»æ•° | æµ‹é‡ä¸¤ä¸ªå˜é‡ä¹‹é—´çš„çº¿æ€§ç›¸å…³æ€§ | `scipy.stats.pearsonr(x, y)` |

---

## åˆ†æå·¥ä½œæµç¨‹

### æ ‡å‡†åˆ†ææµç¨‹

1. **æ˜ç¡®é—®é¢˜**  
   - æˆ‘ä»¬è¯•å›¾å›ç­”ä»€ä¹ˆé—®é¢˜ï¼Ÿ  
   - è¿™äº›åˆ†æç»“æœå°†ç”¨äºåšå‡ºå“ªäº›å†³ç­–ï¼Ÿ  

2. **ç†è§£æ•°æ®**  
   - æœ‰å“ªäº›å¯ç”¨æ•°æ®ï¼Ÿ  
   - æ•°æ®çš„ç»“æ„å’Œè´¨é‡å¦‚ä½•ï¼Ÿ  

3. **æ•°æ®æ¸…æ´—ä¸å‡†å¤‡**  
   - å¤„ç†ç¼ºå¤±å€¼  
   - è°ƒæ•´æ•°æ®ç±»å‹  
   - åˆ é™¤é‡å¤æ•°æ®  

4. **æ•°æ®æ¢ç´¢**  
   - è¿›è¡Œæè¿°æ€§ç»Ÿè®¡åˆ†æ  
   - åˆæ­¥ç”Ÿæˆå¯è§†åŒ–ç»“æœ  
   - å‘ç°æ•°æ®ä¸­çš„æ¨¡å¼  

5. **æ·±å…¥åˆ†æ**  
   - å¯¹åˆ†æç»“æœè¿›è¡Œæ·±å…¥ç ”ç©¶  
   - å¦‚æœ‰éœ€è¦ï¼Œè¿›è¡Œç»Ÿè®¡æ£€éªŒ  
   - éªŒè¯å‡è®¾  

6. **ç»“æœæ²Ÿé€š**  
   - ä½¿ç”¨æ¸…æ™°çš„å¯è§†åŒ–å›¾è¡¨å±•ç¤ºç»“æœ  
   - æä¾›å¯æ“ä½œçš„æ´å¯Ÿå’Œå»ºè®®  

### åˆ†æè¯·æ±‚æ¨¡æ¿  

```markdown
# Analysis Request

## Question
[What are we trying to answer?]

## Context
[Why does this matter? What decision will it inform?]

## Data Available
- [Dataset 1]: [Description]
- [Dataset 2]: [Description]

## Expected Output
- [Deliverable 1]
- [Deliverable 2]

## Timeline
[When is this needed?]

## Notes
[Any constraints or considerations]
```  

---

## è„šæœ¬

### data-init.sh  
åˆå§‹åŒ–æ‚¨çš„æ•°æ®åˆ†æå·¥ä½œç¯å¢ƒã€‚  

### query.sh  
å¿«é€Ÿæ‰§è¡ŒSQLæŸ¥è¯¢ã€‚  
```bash
# Run query from file
./scripts/query.sh --file queries/daily-report.sql

# Run inline query
./scripts/query.sh "SELECT COUNT(*) FROM users"

# Save output to file
./scripts/query.sh --file queries/export.sql --output data/export.csv
```  

### analyze.py  
Pythonæ•°æ®åˆ†æå·¥å…·åŒ…ã€‚  
```bash
# Basic analysis
python scripts/analyze.py --input data/sales.csv

# With specific analysis type
python scripts/analyze.py --input data/sales.csv --type cohort

# Generate report
python scripts/analyze.py --input data/sales.csv --report weekly
```  

---

## é›†æˆå»ºè®®

### ä¸å…¶ä»–æŠ€èƒ½çš„é›†æˆ

| æŠ€èƒ½ | é›†æˆæ–¹å¼ |
|-------|-------------|
| **å¸‚åœºè¥é”€** | åˆ†æè¥é”€æ´»åŠ¨çš„æ•ˆæœå’Œå†…å®¹æŒ‡æ ‡ |
| **é”€å”®** | åˆ†æé”€å”®æµç¨‹å’Œè½¬åŒ–ç‡ |
| **ä¸šåŠ¡å¼€å‘** | è¿›è¡Œå¸‚åœºç ”ç©¶å’Œç«äº‰å¯¹æ‰‹åˆ†æ |

### å¸¸è§çš„æ•°æ®æ¥æº

- **æ•°æ®åº“**ï¼šPostgreSQLã€MySQLã€SQLite  
- **æ•°æ®ä»“åº“**ï¼šBigQueryã€Snowflakeã€Redshift  
- **ç”µå­è¡¨æ ¼**ï¼šGoogle Sheetsã€Excelã€CSV  
- **API**ï¼šRESTæ¥å£ã€GraphQL  
- **æ–‡ä»¶æ ¼å¼**ï¼šJSONã€Parquetã€XML  

---

## æœ€ä½³å®è·µ

1. **ä»é—®é¢˜å‡ºå‘** â€” æ˜ç¡®æ‚¨æƒ³è¦è§£å†³çš„é—®é¢˜  
2. **éªŒè¯æ•°æ®è´¨é‡** â€” æ•°æ®è´¨é‡ç›´æ¥å½±å“åˆ†æç»“æœ  
3. **è¯¦ç»†è®°å½•æ‰€æœ‰æ­¥éª¤** â€” åŒ…æ‹¬æŸ¥è¯¢å†…å®¹ã€å‡è®¾å’Œå†³ç­–è¿‡ç¨‹  
4. **é€‰æ‹©åˆé€‚çš„å¯è§†åŒ–æ–¹å¼** â€” æ ¹æ®æ•°æ®ç±»å‹é€‰æ‹©åˆé€‚çš„å›¾è¡¨  
5. **æ¸…æ™°å±•ç¤ºåˆ†æè¿‡ç¨‹** â€” æ–¹æ³•è®ºåŒæ ·é‡è¦  
6. **ä»¥æ´å¯Ÿä¸ºä¸»** â€” ä¸ä»…ä»…æ˜¯æä¾›åŸå§‹æ•°æ®  
7. **ç¡®ä¿ç»“æœå…·æœ‰å®é™…æ„ä¹‰** â€” éœ€è¦æ˜ç¡®ä¸‹ä¸€æ­¥è¡ŒåŠ¨æ–¹æ¡ˆ  
8. **å¯¹æŸ¥è¯¢è¿›è¡Œç‰ˆæœ¬æ§åˆ¶** â€” è·Ÿè¸ªä»£ç çš„å˜åŒ–  

---

## å¸¸è§é”™è¯¯

âŒ **ç¡®è®¤åè¯¯** â€” åªå¯»æ‰¾æ”¯æŒå·²æœ‰ç»“è®ºçš„æ•°æ®  
âŒ **ç›¸å…³æ€§â‰ å› æœå…³ç³»** â€” åœ¨å¾—å‡ºç»“è®ºæ—¶è¦è°¨æ…  
âŒ **é€‰æ‹©æ€§ä½¿ç”¨æ•°æ®** â€” åªé€‰æ‹©æœ‰åˆ©çš„æ•°æ®  
âŒ **å¿½è§†å¼‚å¸¸å€¼** â€” åœ¨åˆ é™¤å¼‚å¸¸å€¼å‰å…ˆè¿›è¡Œè°ƒæŸ¥  
âŒ **è¿‡åº¦å¤æ‚åŒ–** â€” ç®€å•çš„åˆ†æå¾€å¾€æ›´æœ‰æ•ˆ  
âŒ **ç¼ºä¹èƒŒæ™¯ä¿¡æ¯** â€” æœªç»å¯¹æ¯”çš„æ•°å­—æ¯«æ— æ„ä¹‰  

---

## è®¸å¯è¯

**è®¸å¯è¯**ï¼šMITè®¸å¯è¯ â€” å¯è‡ªç”±ä½¿ç”¨ã€ä¿®æ”¹å’Œåˆ†å‘ã€‚  

â€”â€”â€œç›®æ ‡æ˜¯å°†æ•°æ®è½¬åŒ–ä¸ºä¿¡æ¯ï¼Œå†å°†ä¿¡æ¯è½¬åŒ–ä¸ºæœ‰ä»·å€¼çš„æ´å¯Ÿã€‚â€ â€” Carly Fiorina