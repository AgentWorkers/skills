---
name: log-analyzer
description: è§£æã€æœç´¢å¹¶åˆ†æå„ç§æ ¼å¼çš„åº”ç”¨ç¨‹åºæ—¥å¿—ã€‚é€‚ç”¨äºä»æ—¥å¿—æ–‡ä»¶ä¸­è¿›è¡Œè°ƒè¯•ã€è®¾ç½®ç»“æ„åŒ–æ—¥å¿—è®°å½•ã€åˆ†æé”™è¯¯æ¨¡å¼ã€è·¨æœåŠ¡å…³è”äº‹ä»¶ã€è§£æå †æ ˆè·Ÿè¸ªï¼Œæˆ–å®æ—¶ç›‘æ§æ—¥å¿—è¾“å‡ºç­‰åœºæ™¯ã€‚
metadata: {"clawdbot":{"emoji":"ğŸ“‹","requires":{"anyBins":["grep","awk","jq","python3"]},"os":["linux","darwin","win32"]}}
---

# æ—¥å¿—åˆ†æå™¨

ç”¨äºè§£æã€æœç´¢å’Œè°ƒè¯•åº”ç”¨ç¨‹åºæ—¥å¿—ã€‚æ”¯æŒå¤„ç†çº¯æ–‡æœ¬æ—¥å¿—ã€ç»“æ„åŒ–JSONæ—¥å¿—ã€å †æ ˆè·Ÿè¸ªä¿¡æ¯ï¼Œä»¥åŠå®ç°å¤šæœåŠ¡é—´çš„æ—¥å¿—å…³è”å’Œå®æ—¶ç›‘æ§åŠŸèƒ½ã€‚

## ä½¿ç”¨åœºæ™¯

- ä»æ—¥å¿—æ–‡ä»¶ä¸­è°ƒè¯•åº”ç”¨ç¨‹åºé”™è¯¯
- åœ¨æ—¥å¿—ä¸­æœç´¢ç‰¹å®šæ¨¡å¼ã€é”™è¯¯æˆ–è¯·æ±‚ID
- è§£æå’Œåˆ†æå †æ ˆè·Ÿè¸ªä¿¡æ¯
- åœ¨åº”ç”¨ç¨‹åºä¸­é…ç½®ç»“æ„åŒ–æ—¥å¿—è®°å½•ï¼ˆJSONæ ¼å¼ï¼‰
- å…³è”å¤šä¸ªæœåŠ¡æˆ–æ—¥å¿—æ–‡ä»¶ä¸­çš„äº‹ä»¶
- åœ¨å¼€å‘è¿‡ç¨‹ä¸­å®æ—¶ç›‘æ§æ—¥å¿—
- ç”Ÿæˆé”™è¯¯é¢‘ç‡æŠ¥å‘Šæˆ–æ‘˜è¦

## å¿«é€Ÿæœç´¢æ¨¡å¼

### æŸ¥æ‰¾é”™è¯¯å’Œå¼‚å¸¸

```bash
# All errors in a log file
grep -i 'error\|exception\|fatal\|panic\|fail' app.log

# Errors with 3 lines of context
grep -i -C 3 'error\|exception' app.log

# Errors in the last hour (ISO timestamps)
HOUR_AGO=$(date -u -d '1 hour ago' '+%Y-%m-%dT%H:%M' 2>/dev/null || date -u -v-1H '+%Y-%m-%dT%H:%M')
awk -v t="$HOUR_AGO" '$0 ~ /^[0-9]{4}-[0-9]{2}-[0-9]{2}T/ && $1 >= t' app.log | grep -i 'error'

# Count errors by type
grep -oP '(?:Error|Exception): \K[^\n]+' app.log | sort | uniq -c | sort -rn | head -20

# HTTP 5xx errors from access logs
awk '$9 >= 500' access.log
```

### æŒ‰è¯·æ±‚IDæˆ–å…³è”IDæœç´¢

```bash
# Trace a single request across log entries
grep 'req-abc123' app.log

# Across multiple files
grep -r 'req-abc123' /var/log/myapp/

# Across multiple services (with filename prefix)
grep -rH 'correlation-id-xyz' /var/log/service-a/ /var/log/service-b/ /var/log/service-c/
```

### æ—¶é—´èŒƒå›´è¿‡æ»¤

```bash
# Between two timestamps (ISO format)
awk '$0 >= "2026-02-03T10:00" && $0 <= "2026-02-03T11:00"' app.log

# Last N lines (tail)
tail -1000 app.log | grep -i error

# Since a specific time (GNU date)
awk -v start="$(date -d '30 minutes ago' '+%Y-%m-%dT%H:%M')" '$1 >= start' app.log
```

## JSON / ç»“æ„åŒ–æ—¥å¿—

### ä½¿ç”¨jqè¿›è¡Œè§£æ

```bash
# Pretty-print JSON logs
cat app.log | jq '.'

# Filter by level
cat app.log | jq 'select(.level == "error")'

# Filter by time range
cat app.log | jq 'select(.timestamp >= "2026-02-03T10:00:00Z")'

# Extract specific fields
cat app.log | jq -r '[.timestamp, .level, .message] | @tsv'

# Count by level
cat app.log | jq -r '.level' | sort | uniq -c | sort -rn

# Filter by nested field
cat app.log | jq 'select(.context.userId == "user-123")'

# Group errors by message
cat app.log | jq -r 'select(.level == "error") | .message' | sort | uniq -c | sort -rn

# Extract request duration stats
cat app.log | jq -r 'select(.duration != null) | .duration' | awk '{sum+=$1; count++; if($1>max)max=$1} END {print "count="count, "avg="sum/count, "max="max}'
```

### è§£ææ··åˆæ ¼å¼çš„æ—¥å¿—ï¼ˆJSONè¡Œä¸çº¯æ–‡æœ¬æ··åˆï¼‰

```bash
# Extract only valid JSON lines
while IFS= read -r line; do
  echo "$line" | jq '.' 2>/dev/null && continue
done < app.log

# Or with grep for lines starting with {
grep '^\s*{' app.log | jq '.'
```

## å †æ ˆè·Ÿè¸ªåˆ†æ

### æå–å¹¶å»é‡å †æ ˆè·Ÿè¸ªä¿¡æ¯

```bash
# Extract Java/Kotlin stack traces (starts with Exception/Error, followed by \tat lines)
awk '/Exception|Error/{trace=$0; while(getline && /^\t/) trace=trace"\n"$0; print trace"\n---"}' app.log

# Extract Python tracebacks
awk '/^Traceback/{p=1} p{print} /^[A-Za-z].*Error/{if(p) print "---"; p=0}' app.log

# Extract Node.js stack traces (Error + indented "at" lines)
awk '/Error:/{trace=$0; while(getline && /^    at /) trace=trace"\n"$0; print trace"\n---"}' app.log

# Deduplicate: group by root cause (first line of trace)
awk '/Exception|Error:/{cause=$0} /^\tat|^    at /{next} cause{print cause; cause=""}' app.log | sort | uniq -c | sort -rn
```

### ä½¿ç”¨Pythonè¿›è¡Œå †æ ˆè·Ÿè¸ªè§£æ

```python
#!/usr/bin/env python3
"""Parse Python tracebacks from log files and group by root cause."""
import sys
import re
from collections import Counter

def extract_tracebacks(filepath):
    tracebacks = []
    current = []
    in_trace = False

    with open(filepath) as f:
        for line in f:
            if line.startswith('Traceback (most recent call last):'):
                in_trace = True
                current = [line.rstrip()]
            elif in_trace:
                current.append(line.rstrip())
                # Exception line ends the traceback
                if re.match(r'^[A-Za-z]\w*(Error|Exception|Warning)', line):
                    tracebacks.append('\n'.join(current))
                    in_trace = False
                    current = []
    return tracebacks

if __name__ == '__main__':
    filepath = sys.argv[1] if len(sys.argv) > 1 else '/dev/stdin'
    traces = extract_tracebacks(filepath)

    # Group by exception type and message
    causes = Counter()
    for trace in traces:
        lines = trace.split('\n')
        cause = lines[-1] if lines else 'Unknown'
        causes[cause] += 1

    print(f"Found {len(traces)} tracebacks, {len(causes)} unique causes:\n")
    for cause, count in causes.most_common(20):
        print(f"  {count:4d}x  {cause}")
```

## å®æ—¶ç›‘æ§

### ç›‘æ§æ—¥å¿—æµå¹¶è¿‡æ»¤æ•°æ®

```bash
# Follow log file, highlight errors in red
tail -f app.log | grep --color=always -i 'error\|warn\|$'

# Follow and filter to errors only
tail -f app.log | grep --line-buffered -i 'error\|exception'

# Follow JSON logs, pretty-print errors
tail -f app.log | while IFS= read -r line; do
  level=$(echo "$line" | jq -r '.level // empty' 2>/dev/null)
  if [ "$level" = "error" ] || [ "$level" = "fatal" ]; then
    echo "$line" | jq '.'
  fi
done

# Follow multiple files
tail -f /var/log/service-a/app.log /var/log/service-b/app.log

# Follow with timestamps (useful when log doesn't include them)
tail -f app.log | while IFS= read -r line; do
  echo "$(date '+%H:%M:%S') $line"
done
```

### ç›‘æ§ç‰¹å®šæ¨¡å¼å¹¶è§¦å‘è­¦æŠ¥

```bash
# Beep on error (terminal bell)
tail -f app.log | grep --line-buffered -i 'error' | while read line; do
  echo -e "\a$line"
done

# Count errors per minute
tail -f app.log | grep --line-buffered -i 'error' | while read line; do
  echo "$(date '+%Y-%m-%d %H:%M') ERROR"
done | uniq -c
```

## æ—¥å¿—æ ¼å¼è§£æ

### å¸¸è§è®¿é—®æ—¥å¿—ï¼ˆApache/Nginxï¼‰

```bash
# Parse fields: IP, date, method, path, status, size
awk '{print $1, $9, $7}' access.log

# Top IPs by request count
awk '{print $1}' access.log | sort | uniq -c | sort -rn | head -20

# Top paths by request count
awk '{print $7}' access.log | sort | uniq -c | sort -rn | head -20

# Slow requests (response time in last field, microseconds)
awk '{if ($NF > 1000000) print $0}' access.log

# Requests per minute
awk '{split($4,a,":"); print a[1]":"a[2]":"a[3]}' access.log | uniq -c

# Status code distribution
awk '{print $9}' access.log | sort | uniq -c | sort -rn

# 4xx and 5xx with paths
awk '$9 >= 400 {print $9, $7}' access.log | sort | uniq -c | sort -rn | head -20
```

### è‡ªå®šä¹‰åˆ†éš”ç¬¦çš„æ—¥å¿—æ–‡ä»¶

```bash
# Pipe-delimited: timestamp|level|service|message
awk -F'|' '{print $2, $3, $4}' app.log

# Tab-delimited
awk -F'\t' '$2 == "ERROR" {print $1, $4}' app.log

# CSV logs
python3 -c "
import csv, sys
with open(sys.argv[1]) as f:
    for row in csv.DictReader(f):
        if row.get('level') == 'error':
            print(f\"{row['timestamp']} {row['message']}\")
" app.csv
```

## é…ç½®ç»“æ„åŒ–æ—¥å¿—è®°å½•

### Node.jsï¼ˆä½¿ç”¨pinoä½œä¸ºå¿«é€ŸJSONæ—¥å¿—è®°å½•å·¥å…·ï¼‰

```javascript
// npm install pino
const pino = require('pino');
const logger = pino({
  level: process.env.LOG_LEVEL || 'info',
  // Add standard fields to every log line
  base: { service: 'my-api', version: '1.2.0' },
});

// Usage
logger.info({ userId: 'u123', action: 'login' }, 'User logged in');
logger.error({ err, requestId: req.id }, 'Request failed');

// Output: {"level":30,"time":1706900000000,"service":"my-api","userId":"u123","action":"login","msg":"User logged in"}

// Child logger with bound context
const reqLogger = logger.child({ requestId: req.id, userId: req.user?.id });
reqLogger.info('Processing order');
reqLogger.error({ err }, 'Order failed');
```

### Pythonï¼ˆä½¿ç”¨structlogï¼‰

```python
# pip install structlog
import structlog

structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.add_log_level,
        structlog.processors.JSONRenderer(),
    ],
)
logger = structlog.get_logger(service="my-api")

# Usage
logger.info("user_login", user_id="u123", ip="1.2.3.4")
logger.error("request_failed", request_id="req-abc", error=str(e))

# Output: {"event":"user_login","user_id":"u123","ip":"1.2.3.4","level":"info","timestamp":"2026-02-03T12:00:00Z","service":"my-api"}
```

### Goï¼ˆä½¿ç”¨zerologï¼‰

```go
import (
    "os"
    "github.com/rs/zerolog"
    "github.com/rs/zerolog/log"
)

func init() {
    zerolog.TimeFieldFormat = zerolog.TimeFormatUnix
    log.Logger = zerolog.New(os.Stdout).With().
        Timestamp().
        Str("service", "my-api").
        Logger()
}

// Usage
log.Info().Str("userId", "u123").Msg("User logged in")
log.Error().Err(err).Str("requestId", reqID).Msg("Request failed")
```

## é”™è¯¯æ¨¡å¼æŠ¥å‘Š

### ç”Ÿæˆé”™è¯¯é¢‘ç‡æŠ¥å‘Š

```bash
#!/bin/bash
# error-report.sh - Summarize errors from a log file
LOG="${1:?Usage: error-report.sh <logfile>}"

echo "=== Error Report: $(basename "$LOG") ==="
echo "Generated: $(date -u '+%Y-%m-%dT%H:%M:%SZ')"
echo ""

total=$(wc -l < "$LOG")
errors=$(grep -ci 'error\|exception\|fatal' "$LOG")
warns=$(grep -ci 'warn' "$LOG")

echo "Total lines:  $total"
echo "Errors:       $errors"
echo "Warnings:     $warns"
echo ""

echo "--- Top 15 Error Messages ---"
grep -i 'error\|exception' "$LOG" | \
  sed 's/^[0-9TZ:.+\-]* //' | \
  sed 's/\b[0-9a-f]\{8,\}\b/ID/g' | \
  sed 's/[0-9]\{1,\}/N/g' | \
  sort | uniq -c | sort -rn | head -15
echo ""

echo "--- Errors Per Hour ---"
grep -i 'error\|exception' "$LOG" | \
  grep -oP '\d{4}-\d{2}-\d{2}T\d{2}' | \
  sort | uniq -c
echo ""

echo "--- First Occurrence of Each Error Type ---"
grep -i 'error\|exception' "$LOG" | \
  sed 's/^[0-9TZ:.+\-]* //' | \
  sort -u | head -10
```

### ä½¿ç”¨Pythonç”ŸæˆJSONæ ¼å¼çš„é”™è¯¯æŠ¥å‘Š

```python
#!/usr/bin/env python3
"""Generate error summary from JSON log files."""
import json
import sys
from collections import Counter, defaultdict
from datetime import datetime

def analyze_logs(filepath):
    errors = []
    levels = Counter()
    errors_by_hour = defaultdict(int)

    with open(filepath) as f:
        for line in f:
            try:
                entry = json.loads(line.strip())
            except (json.JSONDecodeError, ValueError):
                continue

            level = entry.get('level', entry.get('severity', '')).lower()
            levels[level] += 1

            if level in ('error', 'fatal', 'critical'):
                msg = entry.get('message', entry.get('msg', entry.get('event', 'unknown')))
                ts = entry.get('timestamp', entry.get('time', ''))
                errors.append({'message': msg, 'timestamp': ts, 'entry': entry})

                # Group by hour
                try:
                    hour = ts[:13]  # "2026-02-03T12"
                    errors_by_hour[hour] += 1
                except (TypeError, IndexError):
                    pass

    # Group errors by message
    error_counts = Counter(e['message'] for e in errors)

    print(f"=== Log Analysis: {filepath} ===\n")
    print("Level distribution:")
    for level, count in levels.most_common():
        print(f"  {level:10s}  {count}")

    print(f"\nTotal errors: {len(errors)}")
    print(f"Unique error messages: {len(error_counts)}\n")

    print("Top 15 errors:")
    for msg, count in error_counts.most_common(15):
        print(f"  {count:4d}x  {msg[:100]}")

    if errors_by_hour:
        print("\nErrors by hour:")
        for hour in sorted(errors_by_hour):
            bar = '#' * min(errors_by_hour[hour], 50)
            print(f"  {hour}  {errors_by_hour[hour]:4d}  {bar}")

if __name__ == '__main__':
    analyze_logs(sys.argv[1])
```

## å¤šæœåŠ¡æ—¥å¿—å…³è”

### åˆå¹¶å¹¶æ’åºæ¥è‡ªå¤šä¸ªæœåŠ¡çš„æ—¥å¿—

```bash
# Merge multiple log files, sort by timestamp
sort -m -t'T' -k1,1 service-a.log service-b.log service-c.log > merged.log

# If files aren't individually sorted, use full sort
sort -t'T' -k1,1 service-*.log > merged.log

# Merge JSON logs, add source field
for f in service-*.log; do
  service=$(basename "$f" .log)
  jq --arg svc "$service" '. + {source: $svc}' "$f"
done | jq -s 'sort_by(.timestamp)[]'
```

### è·Ÿè¸ªè·¨æœåŠ¡çš„è¯·æ±‚æµç¨‹

```bash
# Find all log entries for a correlation/request ID across all services
REQUEST_ID="req-abc-123"
grep -rH "$REQUEST_ID" /var/log/services/ | sort -t: -k2

# With JSON logs
for f in /var/log/services/*.log; do
  jq --arg rid "$REQUEST_ID" 'select(.requestId == $rid or .correlationId == $rid)' "$f" 2>/dev/null
done | jq -s 'sort_by(.timestamp)[]'
```

## æ—¥å¿—è½®æ¢ä¸å¤§æ–‡ä»¶å¤„ç†

### å¤„ç†å·²è½®æ¢æˆ–å‹ç¼©çš„æ—¥å¿—

```bash
# Search across rotated logs (including .gz)
zgrep -i 'error' /var/log/app.log*

# Search today's and yesterday's logs
zgrep -i 'error' /var/log/app.log /var/log/app.log.1

# Decompress, filter, and recompress
zcat app.log.3.gz | grep 'ERROR' | gzip > errors-day3.gz
```

### ä»å¤§æ–‡ä»¶ä¸­é‡‡æ ·æ•°æ®

```bash
# Random sample of 1000 lines
shuf -n 1000 huge.log > sample.log

# Every 100th line
awk 'NR % 100 == 0' huge.log > sample.log

# First and last 500 lines
{ head -500 huge.log; echo "--- TRUNCATED ---"; tail -500 huge.log; } > excerpt.log
```

## ä½¿ç”¨æŠ€å·§

- é¦–å…ˆå§‹ç»ˆæœç´¢**è¯·æ±‚IDæˆ–å…³è”ID**â€”â€”è¿™æ¯”ä½¿ç”¨æ—¶é—´æˆ³æˆ–é”™è¯¯ä¿¡æ¯èƒ½æ›´å¿«åœ°å®šä½é—®é¢˜ã€‚
- å½“ä»`tail -f`å‘½ä»¤è¾“å‡ºæ—¥å¿—æ—¶ï¼Œä½¿ç”¨`--line-buffered`é€‰é¡¹å¯ä»¥é¿å…è¾“å‡ºå»¶è¿Ÿã€‚
- åœ¨å¯¹é”™è¯¯è¿›è¡Œåˆ†ç»„ä¹‹å‰ï¼Œå…ˆä½¿ç”¨`sed 's/[0-9a-f]\{8,\}/ID/g`å‘½ä»¤å¯¹IDå’Œæ•°å­—è¿›è¡Œè§„èŒƒåŒ–å¤„ç†ï¼Œä»¥æ¶ˆé™¤ä»…å› IDä¸åŒè€Œäº§ç”Ÿçš„é‡å¤è®°å½•ã€‚
- å¯¹äºJSONæ—¥å¿—ï¼Œ`jq`æ˜¯å¿…ä¸å¯å°‘çš„å·¥å…·ã€‚å¦‚æœå°šæœªå®‰è£…ï¼Œè¯·ä½¿ç”¨`apt install jq`æˆ–`brew install jq`è¿›è¡Œå®‰è£…ã€‚
- é…ç½®ç»“æ„åŒ–æ—¥å¿—è®°å½•ï¼ˆJSONæ ¼å¼ï¼‰æ˜¯éå¸¸å€¼å¾—çš„ã€‚å®ƒèƒ½è®©æ‰€æœ‰åˆ†æä»»åŠ¡å˜å¾—æ›´åŠ ç®€å•ï¼šè¿‡æ»¤ã€åˆ†ç»„ã€å…³è”å’Œè§¦å‘è­¦æŠ¥éƒ½å¯ä»¥é€šè¿‡`jq`çš„ä¸€è¡Œå‘½ä»¤å®Œæˆã€‚
- åœ¨è°ƒè¯•ç”Ÿäº§ç¯å¢ƒä¸­çš„é—®é¢˜æ—¶ï¼Œé¦–å…ˆç¡®å®š**æ—¶é—´èŒƒå›´**å’Œ**å—å½±å“çš„ç”¨æˆ·/è¯·æ±‚ID**ï¼Œç„¶åå†åœ¨è¯¥èŒƒå›´å†…è¿‡æ»¤æ—¥å¿—ã€‚
- å¯¹äºå¤§æ–‡ä»¶ï¼Œ`awk`æ¯”`grep | sort | uniq -c`çš„ç»„åˆå‘½ä»¤æ›´å¿«ï¼Œé€‚ç”¨äºè®¡æ•°å’Œèšåˆæ“ä½œã€‚