---
name: homelab-cluster
description: |
  Manage multi-tier AI inference clusters for homelabs. Health monitoring, expert MoE routing,
  automatic node recovery, and model deployment across Ollama and llama.cpp nodes. Covers GPU
  memory planning, Docker volume strategies for large models, sequential startup patterns to
  avoid CUDA deadlocks, and unified API gateways via LiteLLM.
version: "1.0.0"
license: MIT
metadata:
  author: mlesnews
  org: Lumina Homelab
  domain: luminahomelab.ai
  emoji: "ğŸ "
  tags:
    - homelab
    - infrastructure
    - llm
    - gpu
    - monitoring
    - ollama
    - llama-cpp
    - compute-cluster
    - litellm
    - docker
---

# å®¶åº­å®éªŒå®¤é›†ç¾¤ç®¡ç†

æœ¬æ–‡æ¡£ä»‹ç»äº†å¦‚ä½•ç®¡ç†ä¸€ä¸ªç”±å¤šå±‚GPUå’ŒCPUæ¨ç†èŠ‚ç‚¹ç»„æˆçš„AIè®¡ç®—é›†ç¾¤ã€‚è¯¥é›†ç¾¤ç”±[Lumina Homelab](https://luminahomelab.ai)å¼€å‘å¹¶ç»è¿‡å®é™…æµ‹è¯•ã€‚

## ä½¿ç”¨åœºæ™¯

å½“æ‚¨çš„åº”ç”¨ç¨‹åºéœ€è¦æ‰§è¡Œä»¥ä¸‹æ“ä½œæ—¶ï¼Œå¯ä»¥ä½¿ç”¨æ­¤åŠŸèƒ½ï¼š
- ç›‘æ§åˆ†å¸ƒå¼æ¨¡å‹ç«¯ç‚¹çš„è¿è¡ŒçŠ¶æ€
- å°†æ¨ç†è¯·æ±‚è·¯ç”±åˆ°æ€§èƒ½æœ€ä½³çš„æ¨¡å‹
- è‡ªåŠ¨æ¢å¤æ•…éšœèŠ‚ç‚¹
- è§„åˆ’æ¨¡å‹ä¹‹é—´çš„GPUå†…å­˜åˆ†é…
- åœ¨å¼‚æ„ç¡¬ä»¶ä¸Šéƒ¨ç½²æ¨¡å‹

## æ¶æ„æ¨¡å¼

ä¸€ä¸ªå®¶åº­å®éªŒå®¤é›†ç¾¤é€šå¸¸åŒ…å«2-3ä¸ªå±‚çº§ï¼š
| å±‚æ¬¡ | å…¸å‹ç¡¬ä»¶ | è¿è¡Œæ—¶ç¯å¢ƒ | åŠŸèƒ½ |
|------|-----------------|---------|------|
| **æœ¬åœ°å±‚** | ä¸»GPUï¼ˆRTX 4090/5090ï¼‰ | Ollama | å¿«é€Ÿæ¨ç†ã€åµŒå…¥è®¡ç®— |
| **è¿œç¨‹å±‚** | è¾…åŠ©GPUï¼ˆRTX 3090/4090ï¼‰ | llama.cppæˆ–Ollama | åˆ†å¸ƒå¼æ¨ç† |
| **NAS/CPUå±‚** | Synologyã€RPiæˆ–å…¶ä»–CPUèŠ‚ç‚¹ | Ollama | è½»é‡çº§æ¨¡å‹ã€å¤‡ç”¨æ–¹æ¡ˆ |

åœ¨é›†ç¾¤çš„å‰ç«¯éƒ¨ç½²äº†ä¸€ä¸ª**LiteLLMä»£ç†**ï¼Œå®ƒä¸ºæ‰€æœ‰å±‚çº§æä¾›ç»Ÿä¸€çš„OpenAIå…¼å®¹APIã€‚

## è¿è¡ŒçŠ¶æ€ç›‘æ§

æ‚¨å¯ä»¥é…ç½®é’ˆå¯¹æ¯ä¸ªç«¯ç‚¹çš„è¶…æ—¶æ—¶é—´æ¥æ£€æŸ¥æ‰€æœ‰ç«¯ç‚¹çš„è¿è¡ŒçŠ¶æ€ï¼š

```bash
# Define endpoints with tier labels
ENDPOINTS = {
    "local/ollama": {"url": "http://localhost:11434/api/tags", "tier": "LOCAL"},
    "remote/mark-i": {"url": "http://REMOTE_IP:3009/v1/models", "tier": "REMOTE", "timeout": 8},
    "gateway/litellm": {"url": "http://localhost:8080/health/liveliness", "tier": "GATEWAY"},
}

# For each endpoint: GET with timeout, check HTTP 200
# Classify: HEALTHY / DEGRADED / DOWN per tier
# Overall prognosis based on tier health
```

**é‡è¦æç¤ºï¼š** ä½¿ç”¨`/health/liveliness`æ¥æŸ¥è¯¢LiteLLMçš„è¿è¡ŒçŠ¶æ€ï¼Œè€Œä¸æ˜¯`/health`â€”â€”åè€…ä¼šå°è¯•è¿æ¥æ‰€æœ‰æ¨¡å‹èŠ‚ç‚¹ï¼Œå¦‚æœæŸä¸ªèŠ‚ç‚¹æ— æ³•è®¿é—®å¯èƒ½ä¼šå¯¼è‡´ç³»ç»ŸæŒ‚èµ·ã€‚

## åŸºäºä»»åŠ¡çš„æ¨¡å‹è·¯ç”±

æ ¹æ®ä»»åŠ¡ç±»å‹å°†è¯·æ±‚è·¯ç”±åˆ°æœ€åˆé€‚çš„æ¨¡å‹ï¼š

```
Task Categories:
  code     â†’ Coder model (Qwen2.5-Coder-7B or similar)
  reason   â†’ Reasoning model (DeepSeek-R1-Distill or similar)
  chat     â†’ General model (Qwen2.5-14B or similar)
  vision   â†’ Vision model (Qwen2.5-VL or similar)
  fast     â†’ Smallest available model for quick responses
  embed    â†’ Embedding model (nomic-embed-text or similar)

Router logic:
  1. Classify task from prompt
  2. Check health of preferred model
  3. Fallback to next-best if unavailable
  4. Return model endpoint + metadata
```

## Dockeréƒ¨ç½²ï¼ˆåœ¨è¿œç¨‹èŠ‚ç‚¹ä¸Šè¿è¡Œllama.cppï¼‰

**å…³é”®æç¤ºï¼š** ä½¿ç”¨Dockerå·è€Œéç»‘å®šæŒ‚è½½

å¯¹äºæ–‡ä»¶å¤§å°è¶…è¿‡1.5GBçš„æ¨¡å‹ï¼Œåœ¨Windows Dockerä¸»æœºä¸Šåº”ä½¿ç”¨Dockerå·ï¼š
```bash
# Create a Docker volume for model storage
docker volume create models-vol

# Copy models INTO the volume
docker run --rm -v models-vol:/models -v /host/path:/src alpine cp /src/model.gguf /models/

# Run container FROM volume (not bind mount)
docker run -d --gpus all -v models-vol:/models -p 3009:8000 \
  -e MODEL_PATH=/models/model.gguf your-llamacpp-image
```

**åŸå› ï¼š** Windowsçš„ç»‘å®šæŒ‚è½½æ–¹å¼ä¼šé€šè¿‡gRPC-FUSE/9Pæ¡¥æ¥æœºåˆ¶è¿›è¡Œæ•°æ®ä¼ è¾“ï¼Œåœ¨å¤„ç†å¤§å‹æ–‡ä»¶æ—¶å¯èƒ½å¯¼è‡´GPUå¼ é‡åŠ è½½å¤±è´¥ã€‚è€ŒDockerå·ä½¿ç”¨Linuxçš„åŸç”Ÿext4æ–‡ä»¶ç³»ç»Ÿï¼Œå¯ä»¥å®Œå…¨é¿å…è¿™ä¸ªé—®é¢˜ã€‚

## å®¹å™¨å¯åŠ¨é¡ºåº

åˆ‡å‹¿åŒæ—¶å¯åŠ¨å¤šä¸ªGPUå®¹å™¨ï¼š

```bash
# WRONG â€” causes CUDA initialization deadlock
docker start mark-i mark-iii mark-iv mark-vi &

# RIGHT â€” sequential with health check between each
for container in mark-v mark-iii mark-iv mark-vi mark-i; do
  docker restart $container
  sleep 5
  # Verify health before starting next
  curl -s http://localhost:PORT/v1/models || echo "Warning: $container slow to start"
done
```

## GPUå†…å­˜è§„åˆ’

ç¡®ä¿æ¨¡å‹çš„è¿è¡Œéœ€æ±‚ä¸ä¼šè¶…å‡ºç³»ç»Ÿçš„VRAMå®¹é‡ï¼š

```
Example for 24GB GPU:
  14B model (Q4_K_M)  â†’  9.0 GB, 28 GPU layers
  7B coder            â†’  4.4 GB, full GPU
  8B reasoning        â†’  4.6 GB, full GPU
  1.5B fast coder     â†’  1.1 GB, full GPU
  1.7B fast chat      â†’  1.0 GB, full GPU
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total:               20.1 GB (~84% utilized)

  Remaining: CPU-only containers for 32B+ models
```

## èŠ‚ç‚¹è‡ªåŠ¨æ¢å¤

å½“è¿œç¨‹èŠ‚ç‚¹å‡ºç°æ•…éšœï¼ˆå¦‚Dockeræ¡Œé¢å´©æºƒæˆ–é‡å¯ï¼‰æ—¶ï¼Œç³»ç»Ÿåº”èƒ½å¤Ÿè‡ªåŠ¨æ¢å¤èŠ‚ç‚¹æœåŠ¡ï¼š

```
Recovery sequence:
  1. Health check fails for remote tier
  2. Check if SSH is responsive (node is up but Docker is down)
  3. If SSH works: restart Docker Desktop via SSH
  4. If SSH fails: create RDP session to wake the machine
  5. Wait for Docker + sequential container restart
  6. Re-check health
```

**é‡è¦æç¤ºï¼š** ç»ä¸è¦ä»¥æ˜æ–‡å½¢å¼å­˜å‚¨æ¢å¤æ‰€éœ€çš„å‡­æ®ã€‚è¯·ä½¿ç”¨å®‰å…¨å­˜å‚¨è§£å†³æ–¹æ¡ˆï¼ˆå¦‚Azure Key Vaultã€HashiCorp Vaultç­‰ï¼‰ï¼Œå¹¶é€šè¿‡æ ‡å‡†è¾“å…¥ï¼ˆstdinï¼‰ä¼ é€’æ•æ„Ÿä¿¡æ¯ï¼Œåˆ‡å‹¿å°†å…¶ä½œä¸ºCLIå‚æ•°ä¼ é€’ã€‚

## LiteLLMç½‘å…³é…ç½®

æ‰€æœ‰å±‚çº§éƒ½ä½¿ç”¨ç»Ÿä¸€çš„APIæ¥å£ï¼š

```yaml
model_list:
  # Local Ollama models
  - model_name: local/chat
    litellm_params:
      model: ollama/qwen2.5:32b
      api_base: http://localhost:11434

  # Remote llama.cpp models (need openai/ prefix)
  - model_name: remote/mark-i
    litellm_params:
      model: openai/qwen2.5-14b-instruct
      api_base: http://REMOTE_IP:3009/v1
      api_key: "not-needed"

  # NAS Ollama models
  - model_name: nas/coder
    litellm_params:
      model: ollama/qwen2.5-coder:7b
      api_base: http://NAS_IP:11434
```

**å…³é”®æ³¨æ„äº‹é¡¹ï¼š** llama.cppç«¯ç‚¹çš„æ¨¡å‹åç§°å‰å¿…é¡»åŠ ä¸Š`openai/`å‰ç¼€ï¼ŒåŒæ—¶`api_base`è·¯å¾„ä¸­å¿…é¡»åŒ…å«`/v1`ï¼Œä»¥ç¡®ä¿ä¸LiteLLMçš„å…¼å®¹æ€§ã€‚

## å‚è€ƒé“¾æ¥ï¼š
- **Lumina Homelabï¼š** [luminahomelab.ai](https://luminahomelab.ai)
- **X/Twitterï¼š** [@HK47LUMINA](https://x.com/HK47LUMINA)
- **GitHubï¼š** [mlesnews](https://github.com/mlesnews)