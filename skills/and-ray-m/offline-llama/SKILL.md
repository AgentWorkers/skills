# offline-llama

该技能能够自主管理和使用本地的Ollama模型，实现无需互联网连接的持续运行。它具备模型健康监控、自动回退和自我修复等功能。

## 概述

该技能支持使用本地Ollama模型进行自主运行，能够监控模型状态，在模型出现问题时自动切换模型，并在断网情况下仍保持功能正常。此外，它还具备自我修复能力，可在需要时重启服务并清理资源。

## 核心功能

### 模型管理
- **健康监控**：持续检查模型的可用性和性能
- **自动回退**：当主模型故障时，自动切换到备用模型
- **模型选择**：动态选择最适合当前任务的模型

### 自我修复
- **服务重启**：当模型不可用时，自动重启Ollama服务
- **资源清理**：清理缓存和临时文件以释放系统资源
- **模型重新安装**：自动重新安装出现问题的模型

### 连接性检测
- **网络状态检测**：监控互联网连接状态
- **智能回退**：当本地模型不可用且网络可用时，自动切换到远程模型
- **离线模式**：在无网络连接的情况下仍能保持全部功能

## 配置

### 模型配置
- **主模型**：lama-3.1-8b-instruct（通用任务）
- **备用模型**：mistral-7b-instruct（响应更快）
- **专用模型**：code-llama-7b（用于编码任务）

### 健康检查
- **模型状态**：每30秒检查一次模型可用性
- **延迟监控**：每分钟监控一次响应时间
- **资源使用情况**：每5分钟监控一次GPU/CPU和内存使用情况

### 回退策略
1. **模型切换**：自动切换到其他可用模型
2. **请求重试**：对失败请求进行指数级退避重试
3. **降级模式**：当所有模型都不可用时，以有限的功能继续运行

## 使用方式

### 网络可用时
- 主要使用本地模型
- 当本地模型不可用时，切换到远程模型
- 保持最佳性能

### 网络不可用时
- 仅使用本地模型
- 无中断地继续所有操作
- 在必要时提供降级后的功能

## 命令

### 模型管理
- `model_status`：检查当前模型的健康状态
- `switch_model`：手动切换模型
- `restart_ollama`：重启Ollama服务

### 健康监控
- `check_health`：执行全面的健康检查
- `monitor_resources`：监控系统资源使用情况
- `clear_cache`：清理模型缓存和临时文件

## 自我修复

- **服务重启**：在模型不可用时自动触发
- **资源清理**：在检测到内存使用过高时自动触发
- **模型重新安装**：在模型持续出现故障时自动触发

### 手动干预
- **手动重启**：用户可以手动重启服务
- **缓存清理**：用户可以手动清理资源
- **模型更新**：用户可以根据需要更新模型

## 安全性考虑
- 所有操作均在本地执行
- 无需依赖外部服务
- 提供安全的模型管理机制
- 默认情况下保护用户隐私

## 性能优化
- **资源监控**：跟踪GPU/CPU使用情况和内存使用情况
- **延迟监控**：监控响应时间和性能
- **模型选择**：根据任务需求选择最佳模型

## 维护
- **定期检查**：定期执行健康检查
- **缓存管理**：定期清理未使用的缓存
- **模型更新**：尽可能更新模型

### 故障排除
- **日志分析**：监控日志以发现潜在问题
- **性能指标**：长期跟踪系统性能
- **错误处理**：实现优雅的错误处理和恢复机制

## 集成
该技能可与以下组件集成：
- **Ollama**：用于管理本地模型
- **系统资源**：监控和管理系统资源
- **网络**：检测互联网连接状态
- **OpenClaw**：与现有工具无缝集成

## 未来改进方向
- **模型训练**：支持自定义模型训练
- **智能路由**：根据任务需求智能选择模型
- **多GPU支持**：支持在多GPU上扩展运行
- **云同步**：提供可选的云备份和同步功能

## 许可证
该技能属于OpenClaw生态系统的一部分，遵循与OpenClaw相同的许可条款。