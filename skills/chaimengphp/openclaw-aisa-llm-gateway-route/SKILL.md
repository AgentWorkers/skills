---
name: llm-router
description: "ç»Ÿä¸€å¤§è¯­è¨€æ¨¡å‹ï¼ˆUnified LLMï¼‰ç½‘å…³ï¼šä¸€ä¸ªAPIï¼Œæ”¯æŒ70å¤šä¸ªAIæ¨¡å‹ã€‚åªéœ€ä½¿ç”¨ä¸€ä¸ªAPIå¯†é’¥ï¼Œå³å¯è®¿é—®GPTã€Claudeã€Geminiã€Grokç­‰ä¼—å¤šAIæ¨¡å‹ã€‚"
homepage: https://openclaw.ai
metadata: {"openclaw":{"emoji":"ğŸ§ ","requires":{"bins":["curl","python3"],"env":["AISA_API_KEY"]},"primaryEnv":"AISA_API_KEY"}}
---

# OpenClaw LLM Router ğŸ§ 

**ä¸“ä¸ºè‡ªä¸»ä»£ç†è®¾è®¡çš„ç»Ÿä¸€LLMç½‘å…³ï¼Œç”±AIsaæä¾›æ”¯æŒã€‚**

åªéœ€ä¸€ä¸ªAPIå¯†é’¥ï¼Œå³å¯ä½¿ç”¨70å¤šç§æ¨¡å‹ï¼Œä¸”å…¼å®¹OpenAIã€‚

ç”¨ä¸€ä¸ªAPIå¯†é’¥æ›¿ä»£åŸæœ‰çš„100å¤šä¸ªAPIå¯†é’¥ï¼Œé€šè¿‡ç»Ÿä¸€çš„ã€å…¼å®¹OpenAIçš„æ¥å£è®¿é—®GPT-4ã€Claude-3ã€Geminiã€Grokç­‰æ¨¡å‹ã€‚

## ğŸ”¥ æ‚¨èƒ½åšä»€ä¹ˆï¼Ÿ

### å¤šæ¨¡å‹èŠå¤©
```
"Chat with GPT-4 for reasoning, switch to Claude for creative writing"
```

### æ¨¡å‹æ¯”è¾ƒ
```
"Compare responses from GPT-4, Claude, and Gemini for the same question"
```

### è§†è§‰åˆ†æ
```
"Analyze this image with GPT-4o - what objects are in it?"
```

### æˆæœ¬ä¼˜åŒ–
```
"Route simple queries to fast/cheap models, complex queries to GPT-4"
```

### å›é€€ç­–ç•¥
```
"If GPT-4 fails, automatically try Claude, then Gemini"
```

## ä¸ºä»€ä¹ˆé€‰æ‹©LLM Routerï¼Ÿ

| ç‰¹æ€§ | LLM Router | ç›´æ¥API |
|---------|------------|-------------|
| APIå¯†é’¥ | 1ä¸ª | 10å¤šä¸ª |
| SDKå…¼å®¹æ€§ | OpenAI SDK | å¤šä¸ªSDK |
| è®¡è´¹æ–¹å¼ | ç»Ÿä¸€è®¡è´¹ | æŒ‰æœåŠ¡æä¾›å•†è®¡è´¹ |
| æ¨¡å‹åˆ‡æ¢ | é€šè¿‡å­—ç¬¦ä¸²é…ç½® | éœ€é‡æ–°ç¼–å†™ä»£ç  |
| å›é€€æœºåˆ¶ | å†…ç½® | éœ€è‡ªè¡Œå®ç° |
| æˆæœ¬è¿½è¸ª | ç»Ÿä¸€è¿½è¸ª | åˆ†æ•£å¼è¿½è¸ª |

## æ”¯æŒçš„æ¨¡å‹å®¶æ—

| æ¨¡å‹å®¶æ— | å¼€å‘è€… | ç¤ºä¾‹æ¨¡å‹ |
|--------|-----------|----------------|
| GPT | OpenAI | gpt-5.2, gpt-5, gpt-5-mini, gpt-4.1, gpt-4.1-mini, gpt-4o, gpt-4o-mini |
| Claude | Anthropic | claude-sonnet-4-5, claude-opus-4-1, claude-opus-4, claude-sonnet-4, claude-haiku-4-5 |
| Gemini | Google | gemini-2.5-pro, gemini-2.5-flash, gemini-2.5-flash-lite, gemini-3-pro-preview |
| Grok | xAI | grok-4, grok-3 |
| Llama | Meta | llama-3.1-405b, llama-3.1-70b, llama-3.1-8b |
| Mistral | Mistral AI | mistral-large, mistral-medium, mixtral-8x7b |

> **æ³¨æ„**ï¼šæ¨¡å‹å¯ç”¨æ€§å¯èƒ½æœ‰æ‰€å˜åŒ–ã€‚è¯·è®¿é—®[marketplace.aisa.one/pricing](https://marketplace.aisa.one/pricing)æŸ¥çœ‹å½“å‰å¯ç”¨æ¨¡å‹åŠä»·æ ¼åˆ—è¡¨ã€‚

## å¿«é€Ÿå…¥é—¨

```bash
export AISA_API_KEY="your-key"
```

## APIç«¯ç‚¹

### å…¼å®¹OpenAIçš„èŠå¤©åŠŸèƒ½

```
POST https://api.aisa.one/v1/chat/completions
```

#### è¯·æ±‚
```bash
curl -X POST "https://api.aisa.one/v1/chat/completions" \
  -H "Authorization: Bearer $AISA_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4.1",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "Explain quantum computing in simple terms."}
    ],
    "temperature": 0.7,
    "max_tokens": 1000
  }'
```

#### å‚æ•°

| å‚æ•° | ç±»å‹ | æ˜¯å¦å¿…å¡« | è¯´æ˜ |
|-----------|------|----------|-------------|
| `model` | å­—ç¬¦ä¸² | æ˜¯ | æ¨¡å‹æ ‡è¯†ç¬¦ï¼ˆä¾‹å¦‚ï¼š`gpt-4.1`, `claude-sonnet-4-5`ï¼‰ |
| `messages` | æ•°ç»„ | æ˜¯ | å¯¹è¯æ¶ˆæ¯ |
| `temperature` | æ•°å­— | å¦ | éšæœºæ€§ï¼ˆ0-2ï¼Œé»˜è®¤å€¼ï¼š1ï¼‰ |
| `max_tokens` | æ•´æ•° | å¦ | æœ€å¤§å“åº”å­—ç¬¦æ•° |
| `stream` | å¸ƒå°”å€¼ | å¦ | æ˜¯å¦å¯ç”¨æµå¼å“åº”ï¼ˆé»˜è®¤å€¼ï¼šfalseï¼‰ |
| `top_p` | æ•°å­— | å¦ | æ ¸å¿ƒé‡‡æ ·ç‡ï¼ˆ0-1ï¼‰ |
| `frequency_penalty` | æ•°å­— | å¦ | é¢‘ç‡æƒ©ç½šï¼ˆ-2è‡³2ï¼‰ |
| `presence Penalty` | æ•°å­— | å¦ | å‡ºç°æƒ©ç½šï¼ˆ-2è‡³2ï¼‰ |
| `stop` | å­—ç¬¦ä¸²/æ•°ç»„ | å¦ | åœæ­¢åºåˆ— |

#### æ¶ˆæ¯æ ¼å¼
```json
{
  "role": "user|assistant|system",
  "content": "message text or array for multimodal"
}
```

#### å“åº”
```json
{
  "id": "chatcmpl-xxx",
  "object": "chat.completion",
  "created": 1234567890,
  "model": "gpt-4.1",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Quantum computing uses..."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 50,
    "completion_tokens": 200,
    "total_tokens": 250,
    "cost": 0.0025
  }
}
```

### æµå¼å“åº”

æµå¼å“åº”ä¼šè¿”å›æœåŠ¡å™¨å‘é€çš„äº‹ä»¶ï¼ˆSSEæ ¼å¼ï¼‰ï¼š
```bash
curl -X POST "https://api.aisa.one/v1/chat/completions" \
  -H "Authorization: Bearer $AISA_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "claude-sonnet-4-5",
    "messages": [{"role": "user", "content": "Write a poem about AI."}],
    "stream": true
  }'
```

### è§†è§‰/å›¾åƒåˆ†æ

é€šè¿‡ä¼ é€’å›¾åƒURLæˆ–Base64æ•°æ®æ¥åˆ†æå›¾åƒï¼š
```bash
curl -X POST "https://api.aisa.one/v1/chat/completions" \
  -H "Authorization: Bearer $AISA_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o",
    "messages": [
      {
        "role": "user",
        "content": [
          {"type": "text", "text": "What is in this image?"},
          {"type": "image_url", "image_url": {"url": "https://example.com/image.jpg"}}
        ]
      }
    ]
  }'
```

### å‡½æ•°è°ƒç”¨

å¯ç”¨å·¥å…·/å‡½æ•°ä»¥è·å–ç»“æ„åŒ–è¾“å‡ºï¼š
```bash
curl -X POST "https://api.aisa.one/v1/chat/completions" \
  -H "Authorization: Bearer $AISA_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4.1",
    "messages": [{"role": "user", "content": "What is the weather in Tokyo?"}],
    "functions": [
      {
        "name": "get_weather",
        "description": "Get current weather for a location",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {"type": "string", "description": "City name"},
            "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
          },
          "required": ["location"]
        }
      }
    ],
    "function_call": "auto"
  }'
```

### Google Geminiæ ¼å¼

å¯¹äºGeminiæ¨¡å‹ï¼Œæ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨å…¶åŸç”Ÿæ ¼å¼ï¼š
```
POST https://api.aisa.one/v1/models/gemini-2.5-flash:generateContent
```

```bash
curl -X POST "https://api.aisa.one/v1/models/gemini-2.5-flash:generateContent" \
  -H "Authorization: Bearer $AISA_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [
      {
        "role": "user",
        "parts": [{"text": "Explain machine learning."}]
      }
    ],
    "generationConfig": {
      "temperature": 0.7,
      "maxOutputTokens": 1000
    }
  }'
```

## Pythonå®¢æˆ·ç«¯

### å®‰è£…

æ— éœ€å®‰è£…ï¼Œä»…ä½¿ç”¨æ ‡å‡†åº“ã€‚

### å‘½ä»¤è¡Œæ¥å£ï¼ˆCLIï¼‰ä½¿ç”¨æ–¹æ³•
```bash
# Basic completion
python3 {baseDir}/scripts/llm_router_client.py chat --model gpt-4.1 --message "Hello, world!"

# With system prompt
python3 {baseDir}/scripts/llm_router_client.py chat --model claude-sonnet-4-5 --system "You are a poet" --message "Write about the moon"

# Streaming
python3 {baseDir}/scripts/llm_router_client.py chat --model gpt-4o --message "Tell me a story" --stream

# Multi-turn conversation
python3 {baseDir}/scripts/llm_router_client.py chat --model gpt-4.1 --messages '[{"role":"user","content":"Hi"},{"role":"assistant","content":"Hello!"},{"role":"user","content":"How are you?"}]'

# Vision analysis
python3 {baseDir}/scripts/llm_router_client.py vision --model gpt-4o --image "https://example.com/image.jpg" --prompt "Describe this image"

# List supported models
python3 {baseDir}/scripts/llm_router_client.py models

# Compare models
python3 {baseDir}/scripts/llm_router_client.py compare --models "gpt-4.1,claude-sonnet-4-5,gemini-2.5-flash" --message "What is 2+2?"
```

### Python SDKä½¿ç”¨æ–¹æ³•
```python
from llm_router_client import LLMRouterClient

client = LLMRouterClient()  # Uses AISA_API_KEY env var

# Simple chat
response = client.chat(
    model="gpt-4.1",
    messages=[{"role": "user", "content": "Hello!"}]
)
print(response["choices"][0]["message"]["content"])

# With options
response = client.chat(
    model="claude-3-sonnet",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Explain relativity."}
    ],
    temperature=0.7,
    max_tokens=500
)

# Streaming
for chunk in client.chat_stream(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Write a story."}]
):
    print(chunk, end="", flush=True)

# Vision
response = client.vision(
    model="gpt-4o",
    image_url="https://example.com/image.jpg",
    prompt="What's in this image?"
)

# Compare models
results = client.compare_models(
    models=["gpt-4.1", "claude-sonnet-4-5", "gemini-2.5-flash"],
    message="Explain quantum computing"
)
for model, result in results.items():
    print(f"{model}: {result['response'][:100]}...")
```

## ä½¿ç”¨åœºæ™¯

### 1. æˆæœ¬ä¼˜åŒ–è·¯ç”±

å¯¹äºç®€å•ä»»åŠ¡ï¼Œä½¿ç”¨æˆæœ¬æ›´ä½çš„æ¨¡å‹ï¼š
```python
def smart_route(message: str) -> str:
    # Simple queries -> fast/cheap model
    if len(message) < 50:
        model = "gpt-3.5-turbo"
    # Complex reasoning -> powerful model
    else:
        model = "gpt-4.1"
    
    return client.chat(model=model, messages=[{"role": "user", "content": message}])
```

### 2. å›é€€ç­–ç•¥

åœ¨å‘ç”Ÿæ•…éšœæ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹ï¼š
```python
def chat_with_fallback(message: str) -> str:
    models = ["gpt-4.1", "claude-sonnet-4-5", "gemini-2.5-flash"]
    
    for model in models:
        try:
            return client.chat(model=model, messages=[{"role": "user", "content": message}])
        except Exception:
            continue
    
    raise Exception("All models failed")
```

### 3. æ¨¡å‹A/Bæµ‹è¯•

æ¯”è¾ƒä¸åŒæ¨¡å‹çš„è¾“å‡ºï¼š
```python
results = client.compare_models(
    models=["gpt-4.1", "claude-opus-4-1"],
    message="Analyze this quarterly report..."
)

# Log for analysis
for model, result in results.items():
    log_response(model=model, latency=result["latency"], cost=result["cost"])
```

### 4. é€‰æ‹©æœ€é€‚åˆä»»åŠ¡çš„æ¨¡å‹

ä¸ºæ¯ä¸ªä»»åŠ¡é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼š
```python
MODEL_MAP = {
    "code": "gpt-4.1",
    "creative": "claude-opus-4-1",
    "fast": "gemini-2.5-flash",
    "vision": "gpt-4o",
    "reasoning": "o1",
    "open_source": "llama-3.1-70b"
}

def route_by_task(task_type: str, message: str) -> str:
    model = MODEL_MAP.get(task_type, "gpt-4.1")
    return client.chat(model=model, messages=[{"role": "user", "content": message}])
```

## é”™è¯¯å¤„ç†

é”™è¯¯ä¼šä»¥JSONæ ¼å¼è¿”å›ï¼Œå…¶ä¸­åŒ…å«`error`å­—æ®µï¼š

```json
{
  "error": {
    "code": "model_not_found",
    "message": "Model 'xyz' is not available"
  }
}
```

å¸¸è§é”™è¯¯ä»£ç ï¼š
- `401` - APIå¯†é’¥æ— æ•ˆæˆ–ç¼ºå¤±
- `402` - ä¿¡ç”¨ä¸è¶³
- `404` - æ¨¡å‹æœªæ‰¾åˆ°
- `429` - è¶…è¿‡ä½¿ç”¨é¢‘ç‡é™åˆ¶
- `500` - æœåŠ¡å™¨é”™è¯¯

## æœ€ä½³å®è·µ

1. **ä½¿ç”¨æµå¼å“åº”**ä»¥æå‡ç”¨æˆ·ä½“éªŒ
2. **è®¾ç½®`max_tokens`ä»¥æ§åˆ¶æˆæœ¬
3. **å®ç°å›é€€æœºåˆ¶**ä»¥ç¡®ä¿ç³»ç»Ÿå¯é æ€§
4. **ç¼“å­˜å“åº”**ä»¥å‡å°‘é‡å¤è¯·æ±‚
5. **é€šè¿‡å“åº”å…ƒæ•°æ®ç›‘æ§ä½¿ç”¨æƒ…å†µ**
6. **æ ¹æ®ä»»åŠ¡é€‰æ‹©åˆé€‚çš„æ¨¡å‹**â€”â€”ä¸è¦å¯¹ç®€å•ä»»åŠ¡ä½¿ç”¨GPT-4

## OpenAI SDKå…¼å®¹æ€§

åªéœ€æ›´æ”¹åŸºç¡€URLå’ŒAPIå¯†é’¥å³å¯ï¼š
```python
import os
from openai import OpenAI

client = OpenAI(
    api_key=os.environ["AISA_API_KEY"],
    base_url="https://api.aisa.one/v1"
)

response = client.chat.completions.create(
    model="gpt-4.1",
    messages=[{"role": "user", "content": "Hello!"}]
)
print(response.choices[0].message.content)
```

## ä»·æ ¼

è´¹ç”¨æŒ‰æ¨¡å‹è®¡è´¹ï¼Œå…·ä½“ä»·æ ¼è¯·æŸ¥çœ‹[marketplace.aisa.one/pricing](https://marketplace.aisa.one/pricing)ã€‚

| æ¨¡å‹å®¶æ— | å¤§çº¦è´¹ç”¨ |
|--------------|------------------|
| GPT-4.1 / GPT-4o | çº¦0.01ç¾å…ƒ/1000ä¸ªtoken |
| Claude-3-Sonnet | çº¦0.01ç¾å…ƒ/1000ä¸ªtoken |
| Gemini-2.5-Flash | çº¦0.001ç¾å…ƒ/1000ä¸ªtoken |
| Grok-2 | çº¦0.01ç¾å…ƒ/1000ä¸ªtoken |
| Llama-3.1-70b | çº¦0.002ç¾å…ƒ/1000ä¸ªtoken |
| Mistral-Large | çº¦0.008ç¾å…ƒ/1000ä¸ªtoken |

æ¯ä¸ªå“åº”éƒ½ä¼šåŒ…å«`usage.cost`å’Œ`usage.credits_remaining`å­—æ®µã€‚

## å¼€å§‹ä½¿ç”¨

1. åœ¨[aisa.one](https://aisa.one)æ³¨å†Œ
2. ä»æ§åˆ¶é¢æ¿è·å–APIå¯†é’¥
3. è´­ä¹°ä¿¡ç”¨ï¼ˆæŒ‰éœ€ä»˜è´¹ï¼‰
4. è®¾ç½®ç¯å¢ƒå˜é‡ï¼š`export AISA_API_KEY="your-key"`

## å®Œæ•´APIå‚è€ƒ

è¯·å‚é˜…[APIå‚è€ƒ](https://aisa.mintlify.app/api-reference/introduction)ä»¥è·å–å®Œæ•´çš„ç«¯ç‚¹æ–‡æ¡£ã€‚