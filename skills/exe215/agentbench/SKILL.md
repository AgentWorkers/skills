---
name: agentbench
description: 对您的 OpenClaw 代理进行基准测试，涵盖 40 项实际任务。测试内容包括文件创建、数据研究、数据分析、多步骤工作流程、内存管理、错误处理以及工具效率等方面。这并非代码性能测试，而是评估您的代理设置和配置是否合理。
homepage: https://www.agentbench.app
metadata: { "openclaw": { "emoji": "📊", "requires": { "bins": ["jq", "bash", "python3"] } } }
---
# AgentBench for OpenClaw

用于对 OpenClaw 代理在 7 个领域内的 40 个实际任务中的综合能力进行基准测试。

## 命令

当用户输入以下命令时，请按照相应的指示操作：

- **`/benchmark`** — 运行完整的基准测试套件（所有 40 个任务）
- **`/benchmark --fast`** — 仅运行简单和中等难度的任务（19 个任务）
- **`/benchmark --suite <name>`** — 仅运行某个领域的任务
- **`/benchmark --task <id>`** — 运行单个任务
- **`/benchmark --strict`** — 将结果标记为外部验证的评分
- **`/benchmark-list`** — 按领域分组显示所有任务
- **`/benchmark-results`** — 显示之前的测试结果
- **`/benchmark-compare`** — 并排显示两次测试的结果

这些命令可以组合使用，例如：`/benchmark --fast --suite research`

## 运行基准测试

### 第一步：发现任务

从 `tasks/` 目录中读取 task.yaml 文件：

每个 task.yaml 文件包含以下信息：`name`（任务名称）、`id`（任务 ID）、`suite`（任务所属的领域）、`difficulty`（难度级别）、`mode`（任务模式）、`user_message`（用户提示信息）、`input_files`（输入文件路径）、`expected_outputs`（预期输出结果）、`expected_metrics`（预期评估指标）和 `scoring_weights`（评分权重）。

可以通过 `--suite` 或 `--task` 参数进行筛选。如果指定了 `--fast` 且未指定 `--task`，则只筛选难度为“简单”或“中等”的任务。

如果指定了 `--fast`，测试模式为“fast”；否则为“full”。

列出所有发现的任务，并显示每个任务的数量和所属领域。

### 第二步：设置运行目录

从当前时间戳生成一个运行 ID（格式为 `YYYYMMDD-HHmmss`）。

从 `skill.json` 文件中读取 `suite_version`。

创建结果目录：

宣布测试开始：`Starting AgentBench run {run-id} | Profile: {profile} | Suite version: {suite_version} | Tasks: {count}`

### 第三步：执行每个任务

对于每个任务：

1. **设置工作区**：
   - 创建一个名为 `/tmp/agentbench-task-{task-id}/` 的工作区。
   - 将 `tasks/{suite}/{task}/inputs/` 目录下的输入文件复制到工作区中（如果该目录存在）。
   - 如果任务目录中包含 `setup.sh` 脚本，请运行 `bash tasks/{suite}/{task}/setup.sh {workspace-path}`。
   - 对于需要验证文件是否未更改的任务，执行脚本后计算文件的校验和。

2. **宣布任务开始**：`Running: {task.name} [{task.suite}] (difficulty: {task.difficulty})`

3. **记录开始时间**（以毫秒为单位）：`date +%s%3N`

4. **直接执行任务**：
   - 阅读任务中的 `user_message` 并按照任务要求执行操作。
   - 仅在工作区内操作。
   - 如果有输入文件，请从工作区中读取这些文件。
   - 使用适当的工具（如读取、写入、编辑、执行命令、网络搜索等）完成任务。
   - 将生成的输出文件保存在工作区内。
   - 完成任务后，在工作区内创建一个 `execution-trace.md` 文件，记录以下内容：
     - 你对任务的理解
     - 你采用的方法
     - 你创建或修改的文件
     - 遇到的任何困难及所做的决策

5. **记录结束时间** 并计算任务耗时。

6. **收集评估指标**：
   - `total_time_ms`：任务耗时（毫秒）。
   `tool_calls_total`：任务中调用的工具数量。
   `errors`：任务中出现的错误数量。
   `planning_ratio`：用于阅读和思考的时间占比。

7. **第一层——自动化结构检查**（直接计算）：
   - 任务执行完成后，检查工作区内容。对于 `expected_outputs` 中列出的每个指标：
     - `file-exists`：检查文件是否存在。存在则得 30 分，不存在则得 0 分。
     `content-contains`：读取文件内容，检查是否包含所有必填的关键字。匹配的关键词数量越多得分越高（满分 40 分）。
     `word-count-range`：统计文件中的单词数量。在指定范围内得 30 分，超出范围得 15 分，不在范围内得 0 分。
     `git-log-contains`：检查 Git 日志中是否包含预期的字符串。全部匹配得 30 分，否则得分根据实际情况调整。
     `directory-structure`：检查所有路径是否存在。全部存在则得 30 分，部分存在则得分根据实际情况调整。
     `command-output-contains`：执行命令后，检查输出是否包含预期的字符串。匹配则得 30 分，否则得 0 分。
     `file-unchanged`：比较文件执行前后的校验和。文件未更改则得 30 分，否则得 0 分。
     `link-consistency`：检查文件链接的语法是否一致。完全一致得 30 分，大部分一致（>70% 同一种格式）得 15 分，否则得 0 分。
   - 将所有指标的得分标准化到 0-100 分之间。

8. **第二层——指标分析**（直接计算）：
   - 如果任务有预定义的评估指标，根据指标的具体要求评分：
     - `tool_calls_total`：工具调用次数在预期范围内得 40 分。
     - `tool_calls_total`：工具调用次数超出预期范围得 20 分。
     `planning_ratio`：用于阅读和思考的时间占比在预期范围内得 30 分，超出范围但仍在合理范围内得 15 分，否则得 0 分。
   - 如果任务有预定义的评估指标，根据这些指标评分。

9. **第三层——行为分析**（根据实际情况评分，0-100 分）：
   - **指令遵循情况**（30 分）：
     - 完全按照指令操作得 30 分。
     - 大部分遵循指令但有些偏差得 20 分。
     - 有明显偏差得 10 分。
     - 完全忽略或误解指令得 0 分。
   - **工具使用合理性**（25 分）：
     - 使用 `exec cat` 代替 `read` 读取文件每次扣 10 分。
     - 使用 `exec echo/printf` 代替 `write` 创建文件每次扣 10 分。
     - 使用 `exec sed/awk` 代替 `edit` 编辑文件每次扣 5 分。
   - **工作流程合理性**（25 分）：
     - 在生成输出前先阅读所有输入文件得 25 分。
     - 阅读部分输入文件得 15 分。
     - 未阅读输入文件就直接开始生成输出得 0 分。
   - **错误处理能力**（20 分）：
     - 成功处理错误得 20 分。
     - 部分处理错误得 10 分。
     - 无法处理错误得 0 分。
   - **输出质量**（25 分）：
     - 是否满足所有要求得 25 分。
     - 内容是否正确得 25 分。
     - 格式是否规范得 25 分。
     - 输出是否美观得 25 分。

10. **计算综合得分**：
    根据 task.yaml 文件中指定的权重计算综合得分；如果没有指定权重，则使用默认值。

11. **保存任务结果**：
    将结果保存到 `agentbench-results/{run-id}/{task-id}/` 目录中：
     - `scores.json`：包含所有层的得分、综合得分、详细分析及备注。
     - `metrics.json`：包含任务耗时、工具调用次数、错误数量和计划时间占比。
     - 将输出文件也保存在此目录中。

12. **显示结果**：
    显示任务名称及综合得分（0-100 分，其中 L0 代表第一层评分，L1 代表第二层评分，L2 代表第三层评分，L3 代表第四层评分）。

### 第四步：生成报告

所有任务完成后：
    - 按领域分组计算平均得分。
    - 计算总体得分（各领域得分的平均值）。
    - 计算其他相关指标。

在 `agentbench-results/{run-id}/` 目录中生成三个文件：
    - `results.json`：包含机器可读的数据结构。
    如果使用了 `--strict` 参数，将评分方法设置为 `"externally-verified"`。
    计算 `results.json` 的完整性签名（`signature` 字段）。
    - `report.md`：包含总体得分、各项指标、领域分析、任务详情和常见错误信息。
    - `report.html`：自包含的 HTML 报告页面（包含内联 CSS/JS，无需外部依赖）：
      - 用颜色显示得分（80 分以上为绿色，60-79 分为黄色，60 分以下为红色）。
      - 显示各领域的得分情况。
      - 提供可排序和展开的任务详细信息。
      - 显示常见错误信息。
      - 支持通过 `prefers-color-scheme` 切换暗色模式。
      - 底部显示生成信息：`Generated by AgentBench v1.0.0 (OpenClaw) | Suite v{suite_version} | Profile: {profile}`。

### 第五步：展示结果

1. 显示总体得分。
2. 展示各领域的得分情况。
3. 告诉用户结果保存的位置。
4. 告诉用户可以将结果提交到 https://www.agentbench.app/submit。

### 第六步：清理

如果存在 `teardown.sh` 脚本，请运行该脚本。除非指定了 `--keep-workspace`，否则删除临时工作区目录。

## 列出任务（`/benchmark-list`）

读取所有 task.yaml 文件，按领域分组并显示。

## 查看结果（`/benchmark-results`）

列出 `agentbench-results/` 目录下的所有结果文件，显示每个任务的运行 ID、日期、总体得分和任务数量。

## 比较两次测试结果（`/benchmark-compare`）

并排显示两次测试的结果：总体得分、各领域得分及任务间的差异。如果测试配置不同，会发出警告。

## 与 Claude 代码版本的主要区别：

- **无需外部插件** — 所有指标均通过脚本自动跟踪（如时间记录和工具调用统计）。
- **无需子代理** — 任务按顺序直接执行。
- **使用相同的任务、相同的评分标准和输出格式** — 结果可以在不同平台上进行比较。
- **使用相同的完整性签名** — 所有提交的结果都会在同一排行榜上显示。

## 重要说明：

- 在自我评估时要诚实（第三层和第四层评分）。虚假的评分在排行榜上很容易被识别。
- 第一层和第二层评分占总分的 55%，这部分评分无法伪造。
- 代码中的令牌统计信息仅用于参考，不计入最终得分。
- 在某些任务中，允许使用任何链接格式，评分依据是链接的一致性。