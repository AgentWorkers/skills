---
name: pdf-reader
description: ä»PDFæ–‡ä»¶ä¸­æå–æ–‡æœ¬ã€åœ¨å…¶ä¸­è¿›è¡Œæœç´¢ï¼Œå¹¶ç”Ÿæˆæ‘˜è¦ã€‚
homepage: "https://www.pymupdf.com"
metadata:
  {
    "openclaw":
      {
        "emoji": "ğŸ“„",
        "requires": { "bins": ["python3"], "pip": ["PyMuPDF"] },
        "install":
          [
            {
              "id": "pymupdf",
              "kind": "pip",
              "package": "PyMuPDF",
              "label": "Install PyMuPDF",
            },
          ],
      },
  }
---

# PDFé˜…è¯»å™¨æŠ€èƒ½

`pdf-reader`æŠ€èƒ½æä¾›äº†æå–PDFæ–‡ä»¶ä¸­çš„æ–‡æœ¬ã€åœ¨PDFå†…è¿›è¡Œæœç´¢ã€ç”Ÿæˆæ–‡æ¡£æ‘˜è¦ä»¥åŠæ£€ç´¢å…ƒæ•°æ®çš„åŠŸèƒ½ã€‚

## å·¥å…·API

è¯¥æŠ€èƒ½æä¾›äº†å››ä¸ªåŠŸèƒ½ï¼š

### extract_text
ä»æŒ‡å®šçš„PDFæ–‡ä»¶ä¸­æå–çº¯æ–‡æœ¬ã€‚

- **å‚æ•°ï¼š**
  - `file_path` (string): éœ€è¦æå–æ–‡æœ¬çš„PDFæ–‡ä»¶è·¯å¾„ã€‚
  - `max_pages` (integer, å¯é€‰): æœ€å¤§æå–é¡µæ•°ã€‚

```python
from pdfminer.high_level import extract_text

def extract_text_from_pdf(file_path: str, max_pages=None) -> str:
    """Extracts text from a PDF, up to max_pages."""
    return extract_text(file_path, maxpages=max_pages)
```

### search
åœ¨PDFæ–‡ä»¶ä¸­æœç´¢ç‰¹å®šçš„æœ¯è¯­æˆ–çŸ­è¯­ã€‚

- **å‚æ•°ï¼š**
  - `file_path` (string): PDFæ–‡ä»¶çš„è·¯å¾„ã€‚
  - `query` (string): éœ€è¦åœ¨æ–‡æ¡£ä¸­æœç´¢çš„æœ¯è¯­æˆ–çŸ­è¯­ã€‚

```python
from typing import List

def search_pdf(file_path: str, query: str) -> List[str]:
    """Searches for a term in the PDF and returns lines containing it."""
    pdf_text = extract_text_from_pdf(file_path)
    return [line.strip() for line in pdf_text.split("\n") if query.lower() in line.lower()]
```

### summarize
å°†æ–‡æ¡£åˆ†å‰²æˆæ˜“äºç†è§£çš„ç‰‡æ®µï¼Œç”Ÿæˆæ–‡æ¡£æ‘˜è¦ã€‚

- **å‚æ•°ï¼š**
  - `file_path` (string): PDFæ–‡ä»¶çš„è·¯å¾„ã€‚

```python
from typing import List

def chunk_text(text: str, max_tokens=2000) -> List[str]:
    """Divides text into manageable chunks for processing."""
    words = text.split()
    max_word_count = max_tokens
    chunks = []
    current_chunk = []

    for word in words:
        if len(current_chunk) + len(word.split()) <= max_word_count:
            current_chunk.append(word)
        else:
            chunks.append(" ".join(current_chunk))
            current_chunk = [word]

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks

def summarize_pdf(file_path: str) -> str:
    """Summarizes a PDF file by processing its text."""
    pdf_text = extract_text_from_pdf(file_path)
    chunks = chunk_text(pdf_text)
    summaries = [call_llm("Summarize this:", chunk) for chunk in chunks]
    return "\n".join(summaries)
```

### metadata
æ£€ç´¢å…³äºè¯¥æ–‡æ¡£çš„å…ƒæ•°æ®ã€‚

- **å‚æ•°ï¼š**
  - `file_path` (string): PDFæ–‡ä»¶çš„è·¯å¾„ã€‚

```python
from PyPDF2 import PdfReader

def get_pdf_metadata(file_path: str) -> dict:
    """Extracts metadata from a PDF file."""
    reader = PdfReader(file_path)
    metadata = reader.metadata
    return {
        "title": metadata.get("/Title", "Unknown"),
        "author": metadata.get("/Author", "Unknown"),
        "pages": len(reader.pages),
    }
```

## æµ‹è¯•
ä½¿ç”¨ç¤ºä¾‹PDFæ–‡ä»¶æ¥ç¡®ä¿æ‰€æœ‰åŠŸèƒ½éƒ½èƒ½æ­£å¸¸è¿è¡Œå¹¶è¾“å‡ºå‡†ç¡®çš„ç»“æœï¼š

- æµ‹è¯•ä¸åŒå¸ƒå±€æ ¼å¼ä¸‹çš„æ–‡æœ¬æå–åŠŸèƒ½ã€‚
- éªŒè¯æ‘˜è¦æ˜¯å¦æ¶µç›–äº†æ–‡æ¡£çš„å…³é”®å†…å®¹ã€‚
- ç¡®è®¤æœç´¢ç»“æœæ˜¯å¦åŒ…å«æ‰€æœ‰ç›¸å…³å†…å®¹ã€‚

è¯·ç¡®ä¿æ‘˜è¦å’Œæœç´¢ç»“æœåŸºäºç”¨æˆ·çš„ç‰¹å®šè¾“å…¥ï¼Œå¹¶ç¬¦åˆç”¨æˆ·çš„æœŸæœ›ã€‚