---
name: desearch-crawl
description: >
  **åŠŸèƒ½è¯´æ˜ï¼š**  
  èƒ½å¤Ÿçˆ¬å–/æŠ“å–ä»»æ„ç½‘é¡µçš„æ–‡æœ¬å†…å®¹ï¼Œå¹¶å°†å…¶æå–ä¸ºçº¯æ–‡æœ¬æˆ–åŸå§‹HTMLæ ¼å¼ã€‚å½“æ‚¨éœ€è¦è·å–ç‰¹å®šç½‘é¡µçš„å…¨éƒ¨å†…å®¹æ—¶ï¼Œå¯ä»¥ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚  
  **ä½¿ç”¨åœºæ™¯ï¼š**  
  - ç”¨äºåˆ†æç½‘é¡µçš„ç»“æ„å’Œæ•°æ®  
  - ç”¨äºæå–ç½‘é¡µä¸Šçš„ç‰¹å®šä¿¡æ¯ï¼ˆå¦‚æ ‡é¢˜ã€é“¾æ¥ã€å›¾ç‰‡ç­‰ï¼‰  
  - ç”¨äºè‡ªåŠ¨åŒ–å¤„ç†å¤§é‡ç½‘é¡µ  
  **è¿”å›ç»“æœç±»å‹ï¼š**  
  - çº¯æ–‡æœ¬ï¼ˆplain textï¼‰  
  - åŸå§‹HTMLï¼ˆraw HTMLï¼‰  
  **ç¤ºä¾‹ç”¨æ³•ï¼š**  
  ```python
  # ä½¿ç”¨çˆ¬è™«åº“ï¼ˆå¦‚requestsã€BeautifulSoupç­‰ï¼‰ä»ç½‘é¡µURLè·å–å†…å®¹  
  response = requests.get('https://example.com')  
  html_content = response.text  # è·å–çº¯æ–‡æœ¬å†…å®¹  
  soup = BeautifulSoup(html_content, 'html.parser')  # ä½¿ç”¨BeautifulSoupè§£æHTMLå†…å®¹  
  # æ ¹æ®éœ€æ±‚é€‰æ‹©è¿”å›ç±»å‹ï¼š  
  if you_need_pure_text:  
      pure_text = soup.text  # è·å–çº¯æ–‡æœ¬  
  else:  
      html_content = soup.get_html()  # è·å–åŸå§‹HTMLå†…å®¹  
  ```
metadata: {"clawdbot":{"emoji":"ğŸ•·ï¸","homepage":"https://desearch.ai","requires":{"env":["DESEARCH_API_KEY"]}}}
---
# ä½¿ç”¨ Desearch çˆ¬å–ç½‘é¡µ

ä»ä»»æ„ç½‘é¡µ URL ä¸­æå–å†…å®¹ã€‚è¿”å›çº¯æ–‡æœ¬æˆ–åŸå§‹ HTMLã€‚

## è®¾ç½®

1. ä» [https://console.desearch.ai](https://console.desearch.ai) è·å– API å¯†é’¥ã€‚
2. è®¾ç½®ç¯å¢ƒå˜é‡ï¼š`export DESEARCH_API_KEY='your-key-here'`ï¼ˆå°† `your-key-here` æ›¿æ¢ä¸ºå®é™…çš„ API å¯†é’¥ï¼‰ã€‚

## ä½¿ç”¨æ–¹æ³•

```bash
# Crawl a webpage (returns clean text by default)
scripts/desearch.py crawl "https://en.wikipedia.org/wiki/Artificial_intelligence"

# Get raw HTML
scripts/desearch.py crawl "https://example.com" --crawl-format html
```


## é€‰é¡¹

| é€‰é¡¹ | æè¿° |
|--------|-------------|
| `--crawl-format` | è¾“å‡ºå†…å®¹æ ¼å¼ï¼š`text`ï¼ˆé»˜è®¤ï¼‰æˆ– `html` |

## ç¤ºä¾‹

### é˜…è¯»æ–‡æ¡£é¡µé¢
```bash
scripts/desearch.py crawl "https://docs.python.org/3/tutorial/index.html"
```

### è·å–åŸå§‹ HTML ç”¨äºåˆ†æ
```bash
scripts/desearch.py crawl "https://example.com/page" --crawl-format html
```