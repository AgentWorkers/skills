---
name: firecrawl-cli
description: |
  Firecrawl CLI for web scraping, crawling, and search. Scrape single pages or entire websites, map site URLs, and search the web with full content extraction. Returns clean markdown optimized for LLM context. Use for research, documentation extraction, competitive intelligence, and content monitoring.
---

# Firecrawl CLI

ä½¿ç”¨ `firecrawl` CLI å¯ä»¥æŠ“å–å¹¶æœç´¢ç½‘é¡µã€‚Firecrawl ä¼šè¿”å›é€‚åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½¿ç”¨ç¯å¢ƒçš„ä¼˜åŒ–åçš„ Markdown æ ¼å¼æ•°æ®ï¼Œå¤„ç† JavaScript çš„æ¸²æŸ“é€»è¾‘ï¼Œç»•è¿‡å¸¸è§çš„ç½‘é¡µé˜»å¡å› ç´ ï¼Œå¹¶æä¾›ç»“æ„åŒ–çš„æ•°æ®ã€‚

## å®‰è£…

æ£€æŸ¥çŠ¶æ€ã€è®¤è¯ä¿¡æ¯ä»¥åŠè¯·æ±‚é€Ÿç‡é™åˆ¶ï¼š

```bash
firecrawl --status
```

å®‰è£…å®Œæˆåï¼Œè¾“å‡ºä»¥ä¸‹ä¿¡æ¯ï¼š

```
  ğŸ”¥ firecrawl cli v1.0.2

  â— Authenticated via FIRECRAWL_API_KEY
  Concurrency: 0/100 jobs (parallel scrape limit)
  Credits: 500,000 remaining
```

- **å¹¶å‘ä»»åŠ¡æ•°**ï¼šå…è®¸çš„æœ€å¤§å¹¶è¡Œä»»åŠ¡æ•°é‡ã€‚è¯·åœ¨æ¥è¿‘ä½†ä¸è¦è¶…è¿‡æ­¤é™åˆ¶çš„æƒ…å†µä¸‹è¿è¡Œå¤šä¸ªä»»åŠ¡ã€‚
- **å‰©ä½™ API ä¿¡ç”¨é¢åº¦**ï¼šæ¯æ¬¡æŠ“å–æ“ä½œéƒ½ä¼šæ¶ˆè€—ä¸€å®šçš„ä¿¡ç”¨é¢åº¦ã€‚

å¦‚æœå°šæœªå®‰è£…ï¼Œè¯·æ‰§è¡Œï¼š`npm install -g firecrawl-cli`

å¦‚æœç”¨æˆ·æœªç™»å½•ï¼Œè¯·åŠ¡å¿…å‚è€ƒ [rules/install.md](rules/install.md) ä¸­çš„å®‰è£…è¯´æ˜ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

## è®¤è¯

å¦‚æœæœªè¿›è¡Œè®¤è¯ï¼Œè¯·æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
firecrawl login --browser
```

`--browser` æ ‡å¿—ä¼šè‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨è¿›è¡Œè®¤è¯ï¼Œè€Œæ— éœ€ç”¨æˆ·æ‰‹åŠ¨æ“ä½œã€‚

## æ–‡ä»¶ç»„ç»‡ç»“æ„

åœ¨å·¥ä½œç›®å½•ä¸­åˆ›å»ºä¸€ä¸ª `.firecrawl/` æ–‡ä»¶å¤¹ï¼ˆå¦‚æœè¯¥æ–‡ä»¶å¤¹å°šä¸å­˜åœ¨ï¼‰ï¼Œç”¨äºå­˜å‚¨æŠ“å–ç»“æœã€‚å¦‚æœ `.gitignore` æ–‡ä»¶ä¸­å°šæœªåŒ…å« `.firecrawl/` æ–‡ä»¶ï¼Œè¯·å°†å…¶æ·»åŠ åˆ°è¯¥æ–‡ä»¶ä¸­ã€‚åœ¨ä¿å­˜æ•°æ®æ—¶ï¼Œè¯·å§‹ç»ˆä½¿ç”¨ `-o` é€‰é¡¹ï¼Œä»¥é¿å…æ•°æ®å ç”¨è¿‡å¤šç£ç›˜ç©ºé—´ï¼š

```bash
# Search the web (most common operation)
firecrawl search "your query" -o .firecrawl/search-{query}.json

# Search with scraping enabled
firecrawl search "your query" --scrape -o .firecrawl/search-{query}-scraped.json

# Scrape a page
firecrawl scrape https://example.com -o .firecrawl/{site}-{path}.md
```

## ç¤ºä¾‹ç”¨æ³•

```
.firecrawl/search-react_server_components.json
.firecrawl/search-ai_news-scraped.json
.firecrawl/docs.github.com-actions-overview.md
.firecrawl/firecrawl.dev.md
```

## å‘½ä»¤è¯´æ˜

### Searchï¼ˆæœç´¢ï¼‰ - å¯é€‰æ‹©è¿›è¡Œæ•°æ®æŠ“å–çš„ç½‘é¡µæœç´¢

```bash
# Basic search (human-readable output)
firecrawl search "your query" -o .firecrawl/search-query.txt

# JSON output (recommended for parsing)
firecrawl search "your query" -o .firecrawl/search-query.json --json

# Limit results
firecrawl search "AI news" --limit 10 -o .firecrawl/search-ai-news.json --json

# Search specific sources
firecrawl search "tech startups" --sources news -o .firecrawl/search-news.json --json
firecrawl search "landscapes" --sources images -o .firecrawl/search-images.json --json
firecrawl search "machine learning" --sources web,news,images -o .firecrawl/search-ml.json --json

# Filter by category (GitHub repos, research papers, PDFs)
firecrawl search "web scraping python" --categories github -o .firecrawl/search-github.json --json
firecrawl search "transformer architecture" --categories research -o .firecrawl/search-research.json --json

# Time-based search
firecrawl search "AI announcements" --tbs qdr:d -o .firecrawl/search-today.json --json  # Past day
firecrawl search "tech news" --tbs qdr:w -o .firecrawl/search-week.json --json          # Past week

# Location-based search
firecrawl search "restaurants" --location "San Francisco,California,United States" -o .firecrawl/search-sf.json --json
firecrawl search "local news" --country DE -o .firecrawl/search-germany.json --json

# Search AND scrape content from results
firecrawl search "firecrawl tutorials" --scrape -o .firecrawl/search-scraped.json --json
firecrawl search "API docs" --scrape --scrape-formats markdown,links -o .firecrawl/search-docs.json --json
```

**æœç´¢é€‰é¡¹ï¼š**

| é€‰é¡¹ | æè¿° |
|--------|-------------|
| `--limit <n>` | æœ€å¤§æœç´¢ç»“æœæ•°é‡ï¼ˆé»˜è®¤å€¼ï¼š5ï¼Œæœ€å¤§å€¼ï¼š100ï¼‰ |
| `--sources <sources>` | ä»¥é€—å·åˆ†éš”çš„æ¥æºç±»å‹ï¼šwebï¼ˆç½‘é¡µï¼‰ã€imagesï¼ˆå›¾ç‰‡ï¼‰ã€newsï¼ˆæ–°é—»ï¼‰ï¼ˆé»˜è®¤å€¼ï¼šwebï¼‰ |
| `--categories <categories>` | ä»¥é€—å·åˆ†éš”çš„ç±»åˆ«ç±»å‹ï¼šgithubï¼ˆGitHub æ–‡æ¡£ï¼‰ã€researchï¼ˆç ”ç©¶èµ„æ–™ï¼‰ã€pdfï¼ˆPDF æ–‡æ¡£ï¼‰ |
| `--tbs <value>` | æ—¶é—´è¿‡æ»¤é€‰é¡¹ï¼šqdr:hï¼ˆå°æ—¶ï¼‰ã€qdr:dï¼ˆå¤©ï¼‰ã€qdr:wï¼ˆå‘¨ï¼‰ã€qdr:mï¼ˆæœˆï¼‰ã€qdr:yï¼ˆå¹´ï¼‰ |
| `--location <location>` | åœ°ç†å®šä½é€‰é¡¹ï¼ˆä¾‹å¦‚ï¼šâ€œGermanyâ€ï¼‰ |
| `--country <code>` | ISO å›½å®¶ä»£ç ï¼ˆé»˜è®¤å€¼ï¼šUSï¼‰ |
| `--scrape` | å¯ç”¨æœç´¢ç»“æœçš„æŠ“å–åŠŸèƒ½ |
| `--scrape-formats <formats>` | å½“ `--scrape` é€‰é¡¹è¢«å¯ç”¨æ—¶ï¼ŒæŒ‡å®šæŠ“å–æ•°æ®çš„æ ¼å¼ï¼ˆé»˜è®¤å€¼ï¼šmarkdownï¼‰ |
| `-o, --output <path>` | å°†æœç´¢ç»“æœä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶ |

### Scrapeï¼ˆæŠ“å–ï¼‰ - æå–å•é¡µå†…å®¹

```bash
# Basic scrape (markdown output)
firecrawl scrape https://example.com -o .firecrawl/example.md

# Get raw HTML
firecrawl scrape https://example.com --html -o .firecrawl/example.html

# Multiple formats (JSON output)
firecrawl scrape https://example.com --format markdown,links -o .firecrawl/example.json

# Main content only (removes nav, footer, ads)
firecrawl scrape https://example.com --only-main-content -o .firecrawl/example.md

# Wait for JS to render
firecrawl scrape https://spa-app.com --wait-for 3000 -o .firecrawl/spa.md

# Extract links only
firecrawl scrape https://example.com --format links -o .firecrawl/links.json

# Include/exclude specific HTML tags
firecrawl scrape https://example.com --include-tags article,main -o .firecrawl/article.md
firecrawl scrape https://example.com --exclude-tags nav,aside,.ad -o .firecrawl/clean.md
```

**æŠ“å–é€‰é¡¹ï¼š**

| é€‰é¡¹ | æè¿° |
|--------|-------------|
| `-f, --format <formats>` | è¾“å‡ºæ ¼å¼ï¼šmarkdownã€htmlã€rawHtmlã€linksã€screenshotsã€json |
| `-H, --html` | `--format html` çš„ç®€å†™å½¢å¼ |
| `--only-main-content` | ä»…æå–é¡µé¢ä¸»è¦å†…å®¹ |
| `--wait-for <ms>` | åœ¨æŠ“å–é¡µé¢å†…å®¹å‰ç­‰å¾…æŒ‡å®šçš„æ—¶é—´ï¼ˆç”¨äºå¤„ç† JavaScript åŠ¨ç”»ç­‰ï¼‰ |
| `--include-tags <tags>` | ä»…åŒ…å«æŒ‡å®šçš„ HTML æ ‡ç­¾ |
| `--exclude-tags <tags>` | æ’é™¤æŒ‡å®šçš„ HTML æ ‡ç­¾ |
| `-o, --output <path>` | å°†æŠ“å–ç»“æœä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶ |

### Crawlï¼ˆçˆ¬å–ï¼‰ - å…¨é¢çˆ¬å–æ•´ä¸ªç½‘ç«™

```bash
# Start a crawl (returns job ID)
firecrawl crawl https://example.com

# Wait for crawl to complete
firecrawl crawl https://example.com --wait

# With progress indicator
firecrawl crawl https://example.com --wait --progress

# Check crawl status
firecrawl crawl <job-id>

# Limit pages
firecrawl crawl https://example.com --limit 100 --max-depth 3

# Crawl blog section only
firecrawl crawl https://example.com --include-paths /blog,/posts

# Exclude admin pages
firecrawl crawl https://example.com --exclude-paths /admin,/login

# Crawl with rate limiting
firecrawl crawl https://example.com --delay 1000 --max-concurrency 2

# Save results
firecrawl crawl https://example.com --wait -o crawl-results.json --pretty
```

**çˆ¬å–é€‰é¡¹ï¼š**

| é€‰é¡¹ | æè¿° |
|--------|-------------|
| `--wait` | ç­‰å¾…çˆ¬å–ä»»åŠ¡å®Œæˆ |
| `--progress` | åœ¨çˆ¬å–è¿‡ç¨‹ä¸­æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯ |
| `--limit <n>` | æœ€å¤§çˆ¬å–é¡µæ•° |
| `--max-depth <n>` | æœ€å¤§çˆ¬å–æ·±åº¦ |
| `--include-paths <paths>` | ä»…çˆ¬å–æŒ‡å®šçš„è·¯å¾„ |
| `--exclude-paths <paths>` | è·³è¿‡æŒ‡å®šçš„è·¯å¾„ |
| `--delay <ms>` | è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿæ—¶é—´ |
| `--max-concurrency <n>` | æœ€å¤§å¹¶å‘è¯·æ±‚æ•°é‡ |

### Mapï¼ˆæ˜ å°„ï¼‰ - å‘ç°ç½‘ç«™ä¸Šçš„æ‰€æœ‰ URL

```bash
# List all URLs (one per line)
firecrawl map https://example.com -o .firecrawl/urls.txt

# Output as JSON
firecrawl map https://example.com --json -o .firecrawl/urls.json

# Search for specific URLs
firecrawl map https://example.com --search "blog" -o .firecrawl/blog-urls.txt

# Limit results
firecrawl map https://example.com --limit 500 -o .firecrawl/urls.txt

# Include subdomains
firecrawl map https://example.com --include-subdomains -o .firecrawl/all-urls.txt
```

**æ˜ å°„é€‰é¡¹ï¼š**

| é€‰é¡¹ | æè¿° |
|--------|-------------|
| `--limit <n>` | æœ€å¤§è¦å‘ç°çš„ URL æ•°é‡ |
| `--search <query>` | æ ¹æ®æœç´¢æŸ¥è¯¢ç­›é€‰ URL |
| `--sitemap <mode>` | æ˜¯å¦åŒ…å«å­åŸŸåã€è·³è¿‡å­åŸŸåæˆ–ä»…åŒ…å«ä¸»åŸŸå |
| `--json` | ä»¥ JSON æ ¼å¼è¾“å‡ºç»“æœ |
| `-o, --output <path>` | å°†ç»“æœä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶ |

## ä¿¡ç”¨é¢åº¦ä½¿ç”¨

```bash
# Show credit usage
firecrawl credit-usage

# Output as JSON
firecrawl credit-usage --json --pretty
```

## æŸ¥çœ‹æŠ“å–æ–‡ä»¶

é™¤éç‰¹åˆ«éœ€è¦ï¼Œå¦åˆ™åˆ‡å‹¿ä¸€æ¬¡æ€§è¯»å–æ•´ä¸ª `firecrawl` çš„è¾“å‡ºæ–‡ä»¶ï¼ˆè¿™äº›æ–‡ä»¶å¯èƒ½åŒ…å«æ•°åƒè¡Œæ•°æ®ï¼‰ã€‚å»ºè®®ä½¿ç”¨ `grep`ã€`head` å‘½ä»¤æˆ–åˆ†æ‰¹è¯»å–æ–‡ä»¶å†…å®¹ï¼š

```bash
# Check file size and preview structure
wc -l .firecrawl/file.md && head -50 .firecrawl/file.md

# Use grep to find specific content
grep -n "keyword" .firecrawl/file.md
grep -A 10 "## Section" .firecrawl/file.md
```

## å¹¶è¡Œå¤„ç†

å¯ä»¥ä½¿ç”¨ `&` ç¬¦å·å’Œ `wait` å‘½ä»¤å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæŠ“å–ä»»åŠ¡ï¼š

```bash
# Parallel scraping (fast)
firecrawl scrape https://site1.com -o .firecrawl/1.md &
firecrawl scrape https://site2.com -o .firecrawl/2.md &
firecrawl scrape https://site3.com -o .firecrawl/3.md &
wait
```

å¯¹äºå¤§é‡ URLï¼Œå¯ä»¥ä½¿ç”¨ `xargs` å‘½ä»¤ç»“åˆ `-P` é€‰é¡¹å®ç°å¹¶è¡Œå¤„ç†ï¼š

```bash
cat urls.txt | xargs -P 10 -I {} sh -c 'firecrawl scrape "{}" -o ".firecrawl/$(echo {} | md5).md"'
```

## ä¸å…¶ä»–å·¥å…·çš„ç»“åˆä½¿ç”¨

```bash
# Extract URLs from search results
jq -r '.data.web[].url' .firecrawl/search-query.json

# Get titles from search results
jq -r '.data.web[] | "\(.title): \(.url)"' .firecrawl/search-query.json

# Count URLs from map
firecrawl map https://example.com | wc -l
```