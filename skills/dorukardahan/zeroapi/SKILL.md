---
name: zeroapi
version: 2.3.0
description: 通过 OpenClaw 网关，将任务路由到付费订阅中最佳的 AI 模型（Claude、ChatGPT、Codex、Gemini、Kimi）。在以下情况下使用该功能：用户提及模型路由、多模型设置、要求使用 Codex 完成某项任务、将任务委托给 Gemini、需要将任务路由到最佳模型，或者用户的 OpenClaw 代理已配置了多个 AI 提供商。请勿将其用于单模型对话或普通聊天场景。
homepage: https://github.com/dorukardahan/ZeroAPI
user-invocable: true
compatibility: Requires OpenClaw 2026.2.6+ with at least one AI subscription. Bootstrap budget config requires 2026.2.14+.
metadata: {"openclaw":{"emoji":"⚡","category":"routing","os":["darwin","linux"],"requires":{"anyBins":["openclaw","claude"],"config":["agents"]}}}
---
# ZeroAPI — 基于订阅的模型路由系统

您是 OpenClaw 的一个代理。本技能将指导您如何将任务路由到合适的模型。您无需调用外部 API — OpenClaw 负责处理连接。您的任务是分类传入的任务，并将其分配给相应的代理/模型。

## 首次设置

当首次加载此技能时，请确定用户可用的提供者：

1. 询问：“您有哪些 AI 订阅服务？”（例如：Claude Max 5x/20x、ChatGPT Plus/Pro、Gemini Advanced、Kimi）
2. 将订阅服务与可用的模型层级进行匹配（见下表）
3. 对于缺失的提供者，禁用相应的层级 — 这些步骤将被跳过
4. 与用户确认当前的配置

如果只有 Claude 可用，所有任务都将留在 Opus 上。无需进行路由 — 但仍需应用冲突解决和协作规则来评估任务复杂性。

为了验证提供者是否正常工作，请让用户运行以下命令：
```
openclaw models status
```
任何显示 `missing` 或 `auth_expired` 的模型都不可用。在用户解决问题之前，将其从活动层级中移除。

有关提供者配置的详细信息，请参阅 `references/provider-config.md`（位于与本技能相同的目录中）。

## 模型层级

| 层级 | 模型 | OpenClaw ID | 执行速度（tok/s） | 总延迟（TTFT） | 智能度 | 上下文处理能力 | 最适合的任务类型 |
|------|-------|-------------|-------|------|-------------|---------|---------|
| SIMPLE | Gemini 2.5 Flash-Lite | `google-gemini-cli/gemini-2.5-flash-lite` | 495 tok/s | 0.23s | 21.6 | 低延迟任务，简单格式的处理 |
| FAST | Gemini 3 Flash | `google-gemini-cli/gemini-3-flash-preview` | 206 tok/s | 12.75s | 46.4 | 遵循指令，结构化输出，心跳请求 |
| RESEARCH | Gemini 3 Pro | `google-gemini-cli/gemini-3-pro-preview` | 131 tok/s | 29.59s | 48.4 | 科学研究，复杂上下文分析 |
| CODE | GPT-5.3 Codex | `openai-codex/gpt-5.3-codex` | 113 tok/s | 20.00s | 51.5 | 代码生成，数学问题（99.0%准确率） |
| DEEP | Claude Opus 4.6 | `anthropic/claude-opus-4-6` | 67 tok/s | 1.76s | 53.0 | 推理，规划，判断 |
| ORCHESTRATE | Kimi K2.5 | `kimi-coding/k2p5` | 39 tok/s | 1.65s | 46.7 | 多代理协调（TAU-2：0.959） |

**关键基准测试分数**（分数越高表示性能越好）：
- **GPQA**（科学领域）：Gemini Pro 0.908，Opus 0.769，Codex 0.738*
- **编码**（SWE-bench）：Codex 49.3*，Opus 43.3，Gemini Pro 35.1
- **数学**（AIME '25）：Codex 99.0*，Gemini Flash 97.0，Opus 54.0
- **IFBench**（指令遵循）：Gemini Flash 0.780，Opus 0.639，Codex 0.590*
- **TAU-2**（代理工具使用）：Kimi K2.5 0.959，Codex 0.811*，Opus 0.780

带 * 的分数来自供应商报告，未经独立验证。来源：Artificial Analysis API v4，2026 年 2 月。详细数据见 `benchmarks.json`。

## 决策算法

对于每个传入的任务，按以下 9 个步骤依次进行判断。第一个匹配的模型将被选中。如果所需模型不可用，则跳过该步骤，继续下一个步骤。

**步骤 1：上下文是否超过 100,000 个字符？**
**提示**：大型文件、长文档、粘贴内容、批量数据、CSV 文件、日志输出、整个代码库等 → 路由到 **RESEARCH**（Gemini Pro，支持 100 万字符的上下文处理能力）/ 备用方案：Opus（支持 20 万字符）

**步骤 2：涉及数学计算/证明/数值推理？**
**提示**：需要计算、解方程、证明、积分、导数、概率、统计分析、优化、公式验证等 → 路由到 **CODE**（Codex，数学领域：99.0%准确率）/ 备用方案：Gemini Flash（数学领域：97.0%准确率）/ Opus

**步骤 3：代码编写/生成？**
**提示**：需要编写代码、实现功能、重构代码、创建脚本、进行 API 集成、单元测试等 → 路由到 **CODE**（Codex，编码领域：49.3%准确率）/ 备用方案：Opus

**步骤 4：代码审查/架构/安全性？**
**提示**：需要审查代码、进行安全评估、设计决策等 → 保持使用 **DEEP**（Opus，智能度：53.0%）模型 — 这是默认选择

**步骤 5：任务是否需要快速完成？**
**提示**：任务简单、格式化要求高、需要快速处理、数据量小等 → 路由到 **FAST**（Gemini Flash，执行速度：206 tok/s，IFBench 0.780%准确率）/ 备用方案：Flash-Lite（适用于亚秒级响应时间）/ Opus

**注意**：对于那些对响应时间要求高于智能度的任务（如心跳请求、健康检查等），使用 **SIMPLE**（Gemini Flash-Lite，执行速度：0.23s）。对于心跳请求和定时任务，使用 **FAST**（Gemini Flash）模型，因为它的指令遵循能力更强（IFBench 0.780%准确率；Flash-Lite 没有经过验证的 IFBench 分数）。

**步骤 6：需要研究/科学分析？**
**提示**：涉及研究、数据查找、解释、比较、分析、论文阅读、事实核查等 → 路由到 **RESEARCH**（Gemini Pro，GPQA：0.908%准确率）/ 备用方案：Opus

**步骤 7：需要多步骤协调？**
**提示**：需要协调多个代理、处理复杂的工作流程等 → 路由到 **ORCHESTRATE**（Kimi K2.5，TAU-2：0.959%准确率）/ 备用方案：Codex / Opus

**步骤 8：需要遵循具体指令或生成结构化输出？**
**提示**：需要严格按照指令操作、生成 JSON 格式的数据等 → 路由到 **FAST**（Gemini Flash，IFBench：0.780%准确率）/ 备用方案：Opus

**步骤 9：没有明确匹配的步骤？**
**提示**：保持使用 **DEEP**（Opus，智能度：53.0%）模型 — 这是最可靠的通用选择

## 模型选择示例

当一个任务符合多个步骤的要求时：
- “分析这份 200 页的 PDF 并为其编写 Python 解析器” → 首先根据上下文大小使用 **RESEARCH** 模型，然后委托代码编写任务给 **CODE** 模型。
- “快速解决这个数学问题” → 优先使用 **步骤 2**（数学计算）而非 **步骤 5**（速度）。
- “为这个 API 生成 JSON 格式” → 使用 **步骤 8**（结构化输出）而非 **步骤 3**（代码编写）。
- “审查这段代码并重构认证模块” → 先使用 **步骤 4**（代码审查），然后使用 **步骤 3**（代码重构）。

## 何时不进行路由

在以下情况下，不要更换当前使用的模型：
1. **用户明确要求使用特定模型**。例如：“使用 Opus 完成这个任务” 或 “不要将任务委托给其他模型” — 始终遵循用户的直接指令。
2. **涉及敏感信息的任务**。如果任务需要处理凭证、私钥、秘密信息或个人身份数据，请将任务保留在主代理上，不要发送给子代理。
3. **需要调试特定模型**。如果用户正在测试或比较不同模型的表现，请使用用户指定的模型。
4. **在多轮对话中进行任务时**。即使后续任务很简单，也不要为了切换模型而中断对话，除非用户明确要求。

## 冲突解决规则

当多个模型都符合任务要求时，按照以下优先级进行选择：
1. **智能度优先**。如果任务具有模糊性或风险性，优先选择智能度较高的模型。
2. **专业模型优先**。如果某个模型在特定任务类型上有出色的表现，优先使用该模型。
3. **代码编写任务 → 使用 Codex**；代码审查任务 → 使用 Opus**。
4. **处理大量上下文的数据 → 使用 Gemini**。只有 Gemini 模型支持处理超过 100 万字符的上下文。
5. **对于交互式任务，响应速度很重要**。Flash-Lite（0.23 秒）、Kimi（1.65 秒）和 Opus（1.76 秒）响应速度快；Codex（20 秒）和 Pro（29.59 秒）启动较慢，不适合快速交互。
6. **当所有模型表现相当时**，选择智能度最高的模型，以降低出错风险。

## 子代理的委托

使用 OpenClaw 的代理系统进行任务委托：
```
/agent <agent-id> <instruction>
```

1. 发送命令 `/agent codex <指令>` — OpenClaw 会启动相应的子代理来执行该指令。
2. 子代理会在自己的工作空间中运行并返回文本响应。
3. 子代理不会共享您的对话上下文或工作空间文件。请在指令中提供所有必要的信息。

**需要传递的信息**：具体任务内容、相关代码片段、预期的输出格式以及任何约束条件。

## 错误处理与重试机制

1. **超时**（60 秒内无响应）：在同一模型上重试一次。如果仍然失败，切换到下一个备用模型。
2. **认证错误**（401/403）：立即切换到下一个备用模型，并提示用户重新认证。详情请参阅 `references/oauth-setup.md`。
3. **速率限制**：等待 30 秒后重试一次。如果仍然失败，继续切换到下一个备用模型。
4. **响应不完整或无效**：重试一次。如果问题仍然存在，继续切换到下一个备用模型。
5. **模型不可用**：直接跳过当前模型，继续使用下一个模型。

**最大重试次数**：在同一模型上最多重试 1 次，然后切换到下一个备用模型。如果所有备用模型都失败，继续使用 Opus。总共最多尝试 3 次。

当触发备用模型时，向用户简要说明情况：
> “Codex 不可用，因此切换到 Opus。”

## 多轮对话的路由规则

- 在同一主题的后续对话中，保持使用相同的模型。上下文连续性比选择最佳模型更重要。
- **只有当任务类型发生变化时才切换模型**。例如：用户先讨论架构，然后要求编写代码，此时将代码编写任务委托给 Codex。

**切换模型时的注意事项**：
1. 总结当前对话的相关内容，并将其作为委托指令的一部分传递给子代理。
2. 在继续使用原模型（Opus）时，要考虑到子代理已经完成的工作。

## 工作空间隔离规则

- 子代理无法读取您的文件，请将文件内容作为指令的一部分传递。
- 子代理无法修改您的工作空间文件，它们的输出将以文本形式返回。
- 子代理之间完全隔离，设计上确保没有数据共享。

## 协作模式

- **顺序执行**：适用于需要先收集数据再执行任务的场景。
- **并行处理+合并**：适用于需要探索多种解决方案或时间紧迫的情况。
- **安全审查**：适用于处理敏感代码或关键生产环境中的变更。
- **多代理协调**：适用于需要多个代理协同完成的任务，尤其是涉及复杂依赖关系的场景。

## 备用模型链

当某个模型不可用或受到速率限制时，按照以下顺序切换备用模型：
### 完整的模型栈（4 个提供者）
| 任务类型 | 主要模型 | 备用模型 1 | 备用模型 2 | 备用模型 3 |
|-----------|---------|------------|------------|------------|
| 推理 | Opus | Gemini Pro | Codex | Kimi K2.5 |
| 代码编写 | Codex | Opus | Gemini Pro | Kimi K2.5 |
| 研究 | Gemini Pro | Opus | Codex | Kimi K2.5 |
| 快速任务 | Flash-Lite | Flash | Opus | Codex |
| 代理任务 | Kimi K2.5 | Codex | Gemini Pro | Opus |

**重要提示**：始终使用跨提供者的备用模型。同一提供者的备用模型（例如 Gemini Pro → Flash）只能解决特定模型的问题，无法解决提供者本身的故障。每个备用模型链应至少包含 2 个不同的提供者。

### 特定组合的模型使用方式

- **Claude + Gemini**（2 个提供者）：
  | 任务类型 | 主要模型 | 备用模型 1 |
  |-----------|---------|------------|
  | 推理 | Opus | Gemini Pro |
  | 代码编写 | Opus | Gemini Pro |
  | 研究 | Gemini Pro | Opus |
  | 快速任务 | Flash-Lite | Flash |

- **Claude + Codex**（2 个提供者）：
  | 任务类型 | 主要模型 | 备用模型 |
  |-----------|---------|
  | 推理 | Opus | Codex |
  | 代码编写 | Codex |
  | 其他所有任务 | Opus | Codex |

- **仅使用 Claude**：所有任务都路由到 Opus。无需使用备用模型。

## 提供者配置

有关认证设置、OAuth 流程（包括无头 VPS）和多设备安全性的详细信息，请参阅 `references/oauth-setup.md`（位于与本技能相同的目录中）。

有关提供者配置的详细信息（如 `openclaw.json`、`per-agent models.json`、Google Gemini 的使用技巧等），请参阅 `references/provider-config.md`。

**快速参考**：

| 提供者 | 认证方式 | 维护要求 |
|----------|-----------|-------------|
| Anthropic | 使用 Setup-token（OAuth） | 自动刷新频率较低 |
| Google Gemini | 使用 OAuth（CLI 插件） | 令牌有效期较长 |
| OpenAI Codex | 使用 OAuth（ChatGPT PKCE） | 自动刷新频率较低 |
| Kimi | 使用静态 API 密钥 | 令牌永不过期 |

**故障排除**

有关详细的故障排除方法，请参阅 `references/troubleshooting.md`（位于与本技能相同的目录中）。常见问题包括：
- “没有注册 API 提供者” → 提供者配置中缺少 `api` 字段
- “API 密钥无效”（使用 Gemini 订阅服务时） → 使用错误的 API 类型；请使用 `google-gemini-cli` 而不是 `google-generative-ai`
- 模型显示为 `missing` → 模型 ID 不匹配；例如使用 `gemini-2.5-flash-lite`（缺少 `-preview` 后缀）
- “Codex 401 Unauthorized” → 令牌过期；请按照 `references/oauth-setup.md` 中的说明重新运行 OAuth 流程
- 子代理提示“未知模型” → 子代理的认证配置中缺少相应的提供者信息

## 成本总结

| 成本构成 | 月费 | 备注 |
|-------|---------|-------|
| **仅使用 Claude**（最多 5 次访问） | $100 | 不支持路由，所有任务由 Opus 处理 |
| **仅使用 Claude**（最多 20 次访问） | $200 | 不支持路由，但限制访问频率为 20 次/天 |
| **平衡配置**（最多 20 次访问 + 使用 Gemini） | $220 | 增加了 Flash 模型的速度和 Gemini Pro 的研究功能 |
| **以代码编写为主**（包含 ChatGPT Plus 订阅） | $240 | 增加了 Codex 模型的支持，适用于代码编写和数学计算 |
| **全套服务**（使用所有 4 个提供者，包含 ChatGPT Plus 订阅） | $250 | 提供全面的专业服务 |
| **全套服务（使用所有 4 个提供者，包含 ChatGPT Pro 订阅） | $430 | 支持最高频率限制 |

数据来源：Artificial Analysis API v4，2026 年 2 月。Codex 的分数估计值（*）来自 OpenAI 的官方博客。详细基准测试数据见 `benchmarks.json`。