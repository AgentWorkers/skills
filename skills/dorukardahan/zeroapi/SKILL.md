---
name: zeroapi
version: 2.3.0
description: 通过 OpenClaw 网关，将任务路由到最适合用户需求的 AI 模型（包括 Claude、ChatGPT、Codex、Gemini、Kimi）。适用于以下场景：用户提及模型路由、多模型设置、要求使用 Codex 完成某项任务、将任务委托给 Gemini、需要将任务路由到最优模型，或者用户的 OpenClaw 代理已配置了多个 AI 提供商。请勿将此功能用于单模型对话或普通聊天场景。
homepage: https://github.com/dorukardahan/ZeroAPI
user-invocable: true
compatibility: Requires OpenClaw 2026.2.6+ with at least one AI subscription. Bootstrap budget config requires 2026.2.14+.
metadata: {"openclaw":{"emoji":"⚡","category":"routing","os":["darwin","linux"],"requires":{"anyBins":["openclaw","claude"],"config":["agents"]}}}
---
# ZeroAPI — 基于订阅的模型路由系统

该系统负责将传入的任务路由到最适合的AI模型，并通过可用的提供商来执行。OpenClaw负责处理所有的API连接，本技能定义了任务的分类和分配逻辑。系统会根据任务类型对任务进行分类，并将其分配给相应的代理或模型。

## 首次设置

首次加载此技能时，需要确定用户可用的提供商：

1. 询问用户：“您有哪些AI订阅服务？”（例如：Claude Max 5x/20x、ChatGPT Plus/Pro、Gemini Advanced、Kimi）
2. 将这些订阅服务与可用的服务等级对应起来（见下表）
3. 对于缺失的提供商，将其对应的等级禁用——这些步骤将被跳过
4. 与用户确认当前的配置是否正确

如果只有Claude可用，所有任务都将直接在Opus上处理，无需进行路由；不过仍然需要应用冲突解决和协作规则来评估任务的复杂性。

为了验证提供商在设置后是否正常工作，可以要求用户运行以下命令：
```bash
openclaw models status
```
如果某个模型显示为`missing`或`auth_expired`，则表示该模型不可用。在用户解决问题之前，请将其从活跃的等级列表中移除。

有关提供商的完整配置信息，请参阅`references/provider-config.md`（位于与本技能文件相同的目录中）。

## 模型等级

| 等级 | 模型 | OpenClaw ID | 处理速度（TTFT） | 智力水平 | 上下文处理能力 | 最适合的任务类型 |
|------|-------|-------------|-------------|------------|----------------------|
| SIMPLE | Gemini 2.5 Flash-Lite | `google-gemini-cli/gemini-2.5-flash-lite` | 495 tok/s | 0.23s | 低延迟任务、简单格式的处理 |
| FAST | Gemini 3 Flash | `google-gemini-cli/gemini-3-flash-preview` | 206 tok/s | 12.75s | 需要指令执行的任务、结构化输出 |
| RESEARCH | Gemini 3 Pro | `google-gemini-cli/gemini-3-pro-preview` | 131 tok/s | 29.59s | 科学研究、复杂上下文分析 |
| CODE | GPT-5.3 Codex | `openai-codex/gpt-5.3-codex` | 113 tok/s | 20.00s | 代码生成、数学计算（准确率99.0%） |
| DEEP | Claude Opus 4.6 | `anthropic/claude-opus-4-6` | 67 tok/s | 1.76s | 推理、规划、判断能力 |
| ORCHESTRATE | Kimi K2.5 | `kimi-coding/k2p5` | 39 tok/s | 1.65s | 多代理协调（TAU-2：0.959） |

**关键基准测试分数**（分数越高，性能越好）：
- **GPQA**（科学领域）：Gemini Pro 0.908，Opus 0.769，Codex 0.738*
- **编码**（SWE-bench）：Codex 49.3*，Opus 43.3，Gemini Pro 35.1
- **数学**（AIME '25）：Codex 99.0*，Gemini Flash 97.0，Opus 54.0
- **IFBench**（指令执行）：Gemini Flash 0.780，Opus 0.639，Codex 0.590*
- **TAU-2**（代理工具使用）：Kimi K2.5 0.959，Codex 0.811*，Opus 0.780

带*号的分数是根据供应商的报告估算的，未经独立验证。数据来源：Artificial Analysis API v4，2026年2月。详细数据请参见`benchmarks.json`。

## 决策算法

对于每个传入的任务，系统会按以下9个步骤依次进行判断。第一个匹配到的模型将负责处理任务。如果所需的模型不可用，则跳过该步骤，继续执行下一个步骤。

**步骤1：估算任务所需的令牌数量**：计算输入内容的字符数，然后除以4。大约100,000个令牌对应400,000个字符。如果用户上传了大型文件、代码库或请求“分析整个代码库”，则认为任务需要超过100,000个令牌。

| 步骤 | 判断依据 | 路由目标 | 备选方案 |
|------|---------|----------------|-------------------------|
| 1. 上下文超过100,000个令牌 | 大型文件、长文档、批量数据、CSV文件、日志文件、整个代码库、或“分析这个PDF文件” | RESEARCH（Pro等级，支持1M字节的上下文处理能力） | Opus（支持200K字节的上下文处理能力） |
| 2. 数学/证明问题 | 需要计算、解方程、证明、积分、优化或应用公式 | CODE（Codex模型，数学相关任务准确率99.0%） | Flash（97.0%）或Opus |
| 3. 代码编写 | 需要编写代码、实现功能、重构代码、编写脚本或进行代码迁移 | CODE（Codex模型，编码相关任务准确率49.3%） | Opus |
| 4. 代码审查/架构评估 | 需要审查代码、进行安全评估或设计决策 | DEEP（Opus模型，智能水平53.0%） | 通常会留在主代理上处理 |
| 5. 对速度要求高/任务简单 | 需要快速响应、格式化数据、总结信息、提取内容或进行翻译 | FAST（Flash模型，处理速度206 tok/s） | Flash-Lite或Opus |
| 6. 研究/科学分析 | 需要研究、查找信息、解释结果、进行比较或分析 | RESEARCH（Pro等级，GPQA准确率0.908） | Opus |
| 7. 多步骤任务协调 | 需要协调多个代理、制定工作流程或并行处理 | ORCHESTRATE（Kimi模型，TAU-2准确率0.959） | Codex或Opus |
| 8. 结构化输出 | 需要严格遵循规则、输出格式化的数据 | FAST（Flash模型，IFBench准确率0.780） | Opus |
| 9. 无明确匹配的模型 | 使用默认的DEEP模型（Opus模型，智能水平53.0%） | 作为最安全的通用解决方案 |

**注意：**对于需要极短响应时间的任务（例如心跳检测或健康检查），使用SIMPLE（Flash-Lite模型，处理速度0.23s）。对于需要指令执行的任务（例如定时任务），使用FAST（Flash模型，IFBench准确率0.780）。

### 模型选择示例

- 当任务要求“分析这份200页的PDF文件并为其编写Python解析器”时，首先根据上下文大小选择RESEARCH模型，然后委托CODE模型进行代码编写。
- 当任务需要“快速解决一个数学问题”时，优先选择FAST模型。
- 当任务需要“为某个API生成JSON格式的数据结构”时，选择FAST模型。
- 当任务需要“审查代码并重构认证模块”时，先使用CODE模型进行代码审查，再使用CODE模型进行重构。

## 何时不进行路由

在以下情况下，不要将任务路由到其他模型：
1. **用户明确指定了使用某个模型**：例如“使用Opus模型处理这个任务”或“不要将任务委托给其他模型”。
2. **涉及敏感信息的任务**：如果任务包含凭证、私钥、机密信息或个人身份数据，应确保任务在主代理上处理，避免将敏感数据发送给子代理。
3. **需要调试特定模型**：如果用户正在测试或比较模型行为，应使用用户指定的模型。
4. **在多轮对话中**：如果用户提出了一个简单的后续问题，不要因为问题简单就切换模型，以保持对话的连贯性。

## 冲突解决规则

当多个模型都符合任务要求时，按照以下优先级规则进行选择：
1. **判断能力比处理速度更重要**：如果任务具有模糊性、复杂性或风险，优先选择Opus模型。
2. **专业模型优先**：如果某个模型在特定任务类型上有出色的表现，优先选择该模型。
3. **代码编写任务**：选择Codex模型；**代码审查任务**：选择Opus模型。
4. **处理大量上下文的任务**：仅选择Gemini模型（Gemini 2.5 Flash-Lite模型支持最大1M字节的上下文处理能力）。
5. **对于交互式任务**：选择处理速度快的模型（Flash-Lite、Kimi、Opus）。

## 子代理的委托

使用OpenClaw的代理系统来委托任务：
```text
/agent <agent-id> <instruction>
```

1. 通过`/agent codex <指令>`发送委托指令。
2. 子代理会在自己的工作空间中执行任务，并返回文本形式的响应。
3. 子代理不会共享用户的对话上下文或工作空间文件。在指令中需要提供所有必要的信息，包括具体任务内容、相关代码片段、期望的输出格式和约束条件。

## 错误处理与重试机制

1. **超时**（60秒内没有响应）：在同一模型上重试一次。如果仍然失败，切换到下一个备用模型。
2. **认证错误**（401/403）：立即切换到下一个备用模型，并提示用户重新认证。详情请参阅`references/oauth-setup.md`。
3. **速率限制**：等待30秒后重试一次。如果仍然失败，继续切换到下一个备用模型。
4. **部分或无效的响应**：重试一次。如果问题仍未解决，继续切换到下一个备用模型。
5. **模型不可用**：直接跳过该模型，继续使用下一个模型。

**最大重试次数**：同一模型最多重试1次，然后切换到下一个备用模型。所有备用模型总共最多尝试3次。

当触发备用模型时，需要向用户简要提示：“Codex模型不可用，因此任务被路由到Opus模型。”

## 多轮对话的路由规则

- 在同一主题的后续对话中，继续使用相同的模型，以保持上下文的连贯性。
- 仅当任务类型发生变化时，才切换模型。例如：用户先讨论架构，然后要求编写代码，此时将任务委托给CODE模型。

## 工作空间隔离

- 子代理无法读取用户的文件，用户需要将文件内容直接粘贴到指令中。
- 子代理无法修改用户的工作空间文件，输出结果将以文本形式返回。
- 子代理之间彼此之间没有数据共享，设计上实现了完全的隔离。

## 协作模式

| 协作模式 | 工作流程 | 适用场景 |
|---------|----------------|----------------------|
| 管道式协作 | 研究代理 → 主代理 → 代码代理 | 任务需要在执行前收集相关数据 |
| 并行处理 + 合并 | 主代理生成代码（方法A）+ 研究代理（方法B），然后合并结果 | 适用于需要探索多种解决方案或时间紧迫的情况 |
| 对抗式审查 | 代码代理编写代码 → 主代理审查代码 → 代码代理修改代码 | 适用于处理敏感信息或关键生产环境的代码 |
| 协调式协作（Kimi模型） | 使用`/agent kimi-orchestrator Plan and execute: <任务>` | 适用于涉及多个代理的复杂任务（Kimi模型处理速度较慢，但协调能力较强） |

## 备用模型链

当某个模型不可用或受到速率限制时，按照可靠性顺序切换到下一个备用模型。

### 全套服务（4个提供商）

| 任务类型 | 主要使用的模型 | 备用模型1 | 备用模型2 | 备用模型3 |
|-----------|------------|------------|------------|------------|
| 推理任务 | Opus | Gemini Pro | Codex | Kimi K2.5 |
| 代码编写任务 | Codex | Opus | Gemini Pro | Kimi K2.5 |
| 研究任务 | Gemini Pro | Opus | Codex | Kimi K2.5 |
| 快速任务 | Flash-Lite | Flash | Opus | Codex |
| 代理相关任务 | Kimi K2.5 | Codex | Gemini Pro | Opus |

**重要提示**：始终使用跨提供商的备用模型。相同提供商的备用模型（例如Gemini Pro → Flash）仅适用于解决特定模型的问题，无法解决提供商本身的故障。每个备用模型链至少应包含2个不同的提供商。

### 具体提供商组合

| 提供商组合 | 主要使用的模型 | 备用模型 |
|-----------|------------|------------|
| Claude + Gemini | Opus | Gemini Pro |
| Claude + Codex | Opus |
| 仅使用Claude | Opus |

## 提供商设置

有关认证设置、OAuth流程（包括无头VPS）和多设备安全性的详细信息，请参阅`references/oauth-setup.md`（位于与本技能文件相同的目录中）。

有关提供商的详细配置信息（如`openclaw.json`、`per-agent models.json`和Google Gemini的配置设置），请参阅`references/provider-config.md`。

**快速参考**：

| 提供商 | 认证方式 | 维护要求 |
|---------|------------|-------------------|
| Anthropic | 使用Setup-token（OAuth） | 自动刷新机制 |
| Google Gemini | 使用OAuth（CLI插件） | 令牌有效期较长 |
| OpenAI Codex | 使用OAuth（ChatGPT PKCE） | 自动刷新机制 |
| Kimi | 使用静态API密钥 | 令牌永不过期 |

## 故障排除

有关详细的故障排除方法，请参阅`references/troubleshooting.md`（位于与本技能文件相同的目录中）。常见问题包括：
- “没有注册API提供商”：可能是提供商配置文件中缺少`api`字段。
- “API密钥无效”：可能是使用了错误的API类型，请使用`google-gemini-cli`而不是`google-generative-ai`。
- 模型显示为“missing”：可能是模型ID不匹配（例如`gemini-2.5-flash-lite`缺少`-preview`后缀）。
- “Codex模型返回401 Unauthorized错误”：可能是令牌过期，请按照`references/oauth-setup.md`中的说明重新执行认证流程。
- 子代理提示“Unknown model”：可能是子代理的认证配置中缺少相应的提供商信息。

## 成本统计

| 服务类型 | 月费 | 备注 |
|-------|---------|-------------------|
| 仅使用Claude（最多5次请求） | $100 | 不支持路由功能，所有任务由Opus模型处理 |
| 仅使用Claude（最多20次请求） | $200 | 不支持路由功能，但存在20次请求的速率限制 |
| 平衡型服务（最多20次请求 + 使用Gemini） | $220 | 增加了Flash模型的处理速度和Gemini Pro模型的研究功能 |
| 以代码处理为主的服务（包含ChatGPT Plus） | $240 | 增加了Codex模型的代码生成和数学计算功能 |
| 全套服务（使用所有4个模型 + ChatGPT Plus） | $250 | 提供全面的专业服务 |
| 全套服务（使用所有4个模型 + ChatGPT Pro） | $430 | 支持最高的请求速率限制 |

数据来源：Artificial Analysis API v4，2026年2月。Codex模型的分数是根据OpenAI官方博客数据估算的。详细基准测试数据请参见`references/benchmarks.json`。

## 参考资料

| 文件 | 内容 |
|------|---------|
| [references/oauth-setup.md] | 认证设置、OAuth流程和多设备安全性相关内容 |
| [references/provider-config.md] | openclaw.json、各代理模型的配置信息及Gemini模型的使用技巧 |
| [references/troubleshooting.md] | 常见问题及解决方法 |
| [references/benchmarks.json] | 所有模型的基准测试数据 |