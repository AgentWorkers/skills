---
name: strands
version: 2.0.0
description: ä½¿ç”¨ AWS Strands SDK æ„å»ºå¹¶è¿è¡ŒåŸºäº Python çš„ AI ä»£ç†ã€‚å½“æ‚¨éœ€è¦åˆ›å»ºè‡ªä¸»ä»£ç†ã€å¤šä»£ç†å·¥ä½œæµç¨‹ã€è‡ªå®šä¹‰å·¥å…·æˆ–ä¸ MCP æœåŠ¡å™¨é›†æˆæ—¶ï¼Œå¯ä»¥ä½¿ç”¨è¯¥ SDKã€‚å®ƒæ”¯æŒ Ollamaï¼ˆæœ¬åœ°ï¼‰ã€Anthropicã€OpenAIã€Bedrock ç­‰æ¨¡å‹æä¾›å•†ï¼Œå¯ç”¨äºä»£ç†çš„æ­å»ºã€å·¥å…·çš„å¼€å‘ä»¥åŠä»£ç†ä»»åŠ¡çš„ç¨‹åºåŒ–è¿è¡Œã€‚
homepage: https://github.com/strands-agents/sdk-python
metadata:
  openclaw:
    emoji: ğŸ§¬
    requires:
      bins: [python3]
      packages: [strands-agents]
---

# Strands Agents SDK

ä½¿ç”¨ [Strands SDK](https://github.com/strands-agents/sdk-python)ï¼ˆåŸºäº Apache-2.0 åè®®ï¼Œç”± AWS æä¾›ï¼‰åœ¨ Python ä¸­æ„å»º AI ä»£ç†ã€‚

éªŒè¯ç‰ˆæœ¬ï¼š`strands-agents==1.23.0`ï¼Œ`strands-agents-tools==0.2.19`

## å…ˆå†³æ¡ä»¶

```bash
# Install SDK + tools (via pipx for isolation â€” recommended)
pipx install strands-agents-builder  # includes strands-agents + strands-agents-tools + CLI

# Or install directly
pip install strands-agents strands-agents-tools
```

## æ ¸å¿ƒæ¦‚å¿µï¼šé»˜è®¤ä½¿ç”¨ Bedrock

å½“ `Agent()` æ–¹æ³•æ²¡æœ‰ `model=` å‚æ•°æ—¶ï¼Œä¼šé»˜è®¤ä½¿ç”¨ **Amazon Bedrock** â€” å…·ä½“æ¥è¯´æ˜¯ `us.anthropic.claude-sonnet-4-20250514-v1:0`ï¼ˆä½äº `us-west-2` åŒºåŸŸï¼‰ã€‚è¿™éœ€è¦ AWS å‡­æ®ã€‚å¦‚æœè¦ä½¿ç”¨å…¶ä»–æä¾›å•†ï¼Œè¯·æ˜¾å¼æŒ‡å®š `model=` å‚æ•°ã€‚

é»˜è®¤æ¨¡å‹å¸¸é‡ï¼š`strands.models BEDrock.DEFAULT_BEDROCK_MODEL_ID`

## å¿«é€Ÿå…¥é—¨ â€” æœ¬åœ°ä»£ç†ï¼ˆOllamaï¼‰

```python
from strands import Agent
from strands.models.ollama import OllamaModel

# host is a required positional argument
model = OllamaModel("http://localhost:11434", model_id="qwen3:latest")
agent = Agent(model=model)
result = agent("What is the capital of France?")
print(result)
```

**æ³¨æ„ï¼š** å¹¶éæ‰€æœ‰å¼€æºæ¨¡å‹éƒ½æ”¯æŒè°ƒç”¨å¤–éƒ¨å·¥å…·ã€‚éƒ¨åˆ†æ¨¡å‹åœ¨å¤„ç†è¿‡ç¨‹ä¸­ä¼šå¤±å»è°ƒç”¨å¤–éƒ¨å·¥å…·çš„åŠŸèƒ½ã€‚å»ºè®®å…ˆä½¿ç”¨é»˜è®¤æ¨¡å‹ï¼ˆå¦‚ qwen3ã€llama3.xã€mistralï¼‰è¿›è¡Œæµ‹è¯•ã€‚

## å¿«é€Ÿå…¥é—¨ â€” Bedrockï¼ˆé»˜è®¤æä¾›å•†ï¼‰

```python
from strands import Agent

# No model specified â†’ BedrockModel (Claude Sonnet 4, us-west-2)
# Requires AWS credentials (~/.aws/credentials or env vars)
agent = Agent()
result = agent("Explain quantum computing")

# Explicit Bedrock model:
from strands.models import BedrockModel
model = BedrockModel(model_id="us.anthropic.claude-sonnet-4-20250514-v1:0")
agent = Agent(model=model)
```

## å¿«é€Ÿå…¥é—¨ â€” Anthropicï¼ˆç›´æ¥ä½¿ç”¨ APIï¼‰

```python
from strands import Agent
from strands.models.anthropic import AnthropicModel

# max_tokens is Required[int] â€” must be provided
model = AnthropicModel(model_id="claude-sonnet-4-20250514", max_tokens=4096)
agent = Agent(model=model)
result = agent("Explain quantum computing")
```

éœ€è¦è®¾ç½® `ANTHROPIC_API_KEY` ç¯å¢ƒå˜é‡ã€‚

## å¿«é€Ÿå…¥é—¨ â€” OpenAI

```python
from strands import Agent
from strands.models.openai import OpenAIModel

model = OpenAIModel(model_id="gpt-4.1")
agent = Agent(model=model)
```

éœ€è¦è®¾ç½® `OPENAI_API_KEY` ç¯å¢ƒå˜é‡ã€‚

## åˆ›å»ºè‡ªå®šä¹‰å·¥å…·

ä½¿ç”¨ `@tool` è£…é¥°å™¨ã€‚è¯¥è£…é¥°å™¨ä¼šè‡ªåŠ¨ç”Ÿæˆå·¥å…·çš„ç±»å‹æç¤ºå’Œæ–‡æ¡£å­—ç¬¦ä¸²ï¼š

```python
from strands import Agent, tool

@tool
def read_file(path: str) -> str:
    """Read contents of a file at the given path.

    Args:
        path: Filesystem path to read.
    """
    with open(path) as f:
        return f.read()

@tool
def write_file(path: str, content: str) -> str:
    """Write content to a file.

    Args:
        path: Filesystem path to write.
        content: Text content to write.
    """
    with open(path, 'w') as f:
        f.write(content)
    return f"Wrote {len(content)} bytes to {path}"

agent = Agent(model=model, tools=[read_file, write_file])
agent("Read /tmp/test.txt and summarize it")
```

### ToolContext

å·¥å…·å¯ä»¥é€šè¿‡ `ToolContext` è®¿é—®ä»£ç†çš„çŠ¶æ€ï¼š

```python
from strands import tool
from strands.types.tools import ToolContext

@tool
def stateful_tool(query: str, tool_context: ToolContext) -> str:
    """A tool that accesses agent state.

    Args:
        query: Input query.
    """
    # Access shared agent state
    count = tool_context.state.get("call_count", 0) + 1
    tool_context.state["call_count"] = count
    return f"Call #{count}: {query}"
```

## å†…ç½®å·¥å…·ï¼ˆå…± 46 ä¸ªï¼‰

`strands-agents-tools` æä¾›äº†å¤šç§é¢„æ„å»ºçš„å·¥å…·ï¼š

```python
from strands_tools import calculator, file_read, file_write, shell, http_request
agent = Agent(model=model, tools=[calculator, file_read, shell])
```

å®Œæ•´å·¥å…·åˆ—è¡¨ï¼š`calculator`ã€`file_read`ã€`file_write`ã€`shell`ã€`http_request`ã€`editor`ã€`image_reader`ã€`python_repl`ã€`current_time`ã€`think`ã€`stop`ã€`sleep`ã€`environment`ã€`retrieve`ã€`search_video`ã€`chat_video`ã€`speak`ã€`generate_image`ã€`generate_image_stability`ã€`diagram`ã€`journal`ã€`memory`ã€`agent_core_memory`ã€`elasticsearch_memory`ã€`mongodb_memory`ã€`mem0_memory`ã€`rss`ã€`cron`ã€`batch`ã€`workflow`ã€`use_agent`ã€`use_llm`ã€`use_aws`ã€`use_computer`ã€`load_tool`ã€`handoff_to_user`ã€`slack`ã€`swarm`ã€`graph`ã€`a2a_client`ã€`mcp_client`ã€`exa`ã€`tavily`ã€`bright_data`ã€`nova_reels`ã€‚

**çƒ­é‡è½½åŠŸèƒ½**ï¼š`Agent LOAD_tools_from_directory=True` ä¼šç›‘æ§ `./tools/` ç›®å½•ä¸‹çš„æ–‡ä»¶å˜åŒ–ï¼Œå¹¶è‡ªåŠ¨é‡æ–°åŠ è½½å·¥å…·ã€‚

## MCP é›†æˆ

å¯ä»¥è¿æ¥åˆ°ä»»ä½• Model Context Protocol æœåŠ¡å™¨ã€‚`MCPClient` å®ç°äº† `ToolProvider` æ¥å£ï¼Œå¯ä»¥ç›´æ¥å°†å…¶æ·»åŠ åˆ°å·¥å…·åˆ—è¡¨ä¸­ï¼š

```python
from strands import Agent
from strands.tools.mcp import MCPClient
from mcp import stdio_client, StdioServerParameters

# MCPClient takes a callable that returns the transport
mcp = MCPClient(lambda: stdio_client(StdioServerParameters(
    command="uvx",
    args=["some-mcp-server@latest"]
)))

# Use as context manager â€” MCPClient is a ToolProvider
with mcp:
    agent = Agent(model=model, tools=[mcp])
    agent("Use the MCP tools to do something")
```

**SSE ä¼ è¾“**ï¼š
```python
from mcp.client.sse import sse_client
mcp = MCPClient(lambda: sse_client("http://localhost:8080/sse"))
```

## å¤šä»£ç†æ¨¡å¼

### ä»£ç†ä½œä¸ºå·¥å…·

å†…éƒ¨ä»£ç†å¯ä»¥ä½œä¸ºå¤–éƒ¨ä»£ç†çš„å·¥å…·ä½¿ç”¨ï¼š

```python
researcher = Agent(model=model, system_prompt="You are a research assistant.")
writer = Agent(model=model, system_prompt="You are a writer.")

orchestrator = Agent(
    model=model,
    tools=[researcher, writer],
    system_prompt="You coordinate research and writing tasks."
)
orchestrator("Research quantum computing and write a blog post")
```

### Swarm æ¨¡å¼

è‡ªç»„ç»‡çš„ä»£ç†å›¢é˜Ÿï¼Œå…·æœ‰å…±äº«çš„ä¸Šä¸‹æ–‡å’Œè‡ªåŠ¨çš„ä»»åŠ¡äº¤æ¥æœºåˆ¶ï¼š

```python
from strands.multiagent.swarm import Swarm

# Agents need name + description for handoff identification
researcher = Agent(
    model=model,
    name="researcher",
    description="Finds and summarizes information"
)
writer = Agent(
    model=model,
    name="writer",
    description="Creates polished content"
)

swarm = Swarm(
    nodes=[researcher, writer],
    entry_point=researcher,    # optional â€” defaults to first agent
    max_handoffs=20,           # default
    max_iterations=20,         # default
    execution_timeout=900.0,   # 15 min default
    node_timeout=300.0         # 5 min per node default
)
result = swarm("Research AI agents, then hand off to writer for a blog post")
```

Swarm ä¼šè‡ªåŠ¨æ’å…¥ `handoff_to_agent` å·¥å…·ã€‚ä»£ç†å¯ä»¥é€šè¿‡è°ƒç”¨è¯¥å·¥å…·æ¥äº¤æ¥ä»»åŠ¡ã€‚æ”¯æŒä¸­æ–­/æ¢å¤ã€ä¼šè¯æŒä¹…åŒ–ä»¥åŠé‡å¤æ€§ä»»åŠ¡äº¤æ¥çš„åŠŸèƒ½ã€‚

### Graph æ¨¡å¼ï¼ˆæœ‰å‘æ— ç¯å›¾ï¼‰

é€šè¿‡ `GraphBuilder` å®ç°åŸºäºä¾èµ–å…³ç³»çš„ç¡®å®šæ€§æ‰§è¡Œï¼š

```python
from strands.multiagent.graph import GraphBuilder

builder = GraphBuilder()
research_node = builder.add_node(researcher, node_id="research")
writing_node = builder.add_node(writer, node_id="writing")
builder.add_edge("research", "writing")
builder.set_entry_point("research")

# Optional: conditional edges
# builder.add_edge("research", "writing",
#     condition=lambda state: "complete" in str(state.completed_nodes))

graph = builder.build()
result = graph("Write a blog post about AI agents")
```

æ”¯æŒå¾ªç¯ï¼ˆåé¦ˆå¾ªç¯ï¼‰ï¼Œå¯ä»¥é€šè¿‡ `builder.reset_on_revisit(True)` æ¥è®¾ç½®é‡è¯•æœºåˆ¶ï¼ŒåŒæ—¶æ”¯æŒæ‰§è¡Œè¶…æ—¶å’ŒåµŒå¥—å›¾ç»“æ„ã€‚

### A2A åè®®ï¼ˆä»£ç†é—´é€šä¿¡ï¼‰

å¯ä»¥å°† Strands ä»£ç†ä½œä¸º A2A å…¼å®¹çš„æœåŠ¡å™¨æ¥ä½¿ç”¨ï¼Œä»¥å®ç°ä»£ç†é—´çš„é€šä¿¡ï¼š

```python
from strands.multiagent.a2a import A2AServer

server = A2AServer(
    agent=my_agent,
    host="127.0.0.1",
    port=9000,
    version="0.0.1"
)
server.start()  # runs uvicorn
```

å¯ä»¥ä½¿ç”¨ `strands-agents-tools` ä¸­çš„ `a2a_client` å·¥å…·è¿æ¥åˆ°å…¶ä»– A2A ä»£ç†ã€‚A2A åè®®æ”¯æŒæ ‡å‡†çš„è·¨è¿›ç¨‹/è·¨ç½‘ç»œä»£ç†é€šä¿¡ã€‚

## ä¼šè¯æŒä¹…åŒ–

å¯ä»¥åœ¨ä»£ç†è¿è¡ŒæœŸé—´æŒä¹…åŒ–å¯¹è¯å†…å®¹ï¼š

```python
from strands.session.file_session_manager import FileSessionManager

session = FileSessionManager(session_file_path="./sessions/my_session.json")
agent = Agent(model=model, session_manager=session)

# Also available:
from strands.session.s3_session_manager import S3SessionManager
session = S3SessionManager(bucket_name="my-bucket", session_id="session-1")
```

Swarm å’Œ Graph éƒ½æ”¯æŒä¼šè¯ç®¡ç†å™¨ï¼Œç”¨äºä¿å­˜å¤šä»£ç†çš„çŠ¶æ€ã€‚

## åŒå‘æµå¼é€šä¿¡ï¼ˆå®éªŒæ€§ï¼‰

æ”¯æŒå®æ—¶è¯­éŸ³/æ–‡æœ¬å¯¹è¯ï¼Œå¹¶ä¿ç•™éŸ³é¢‘æµï¼š

```python
from strands.experimental.bidi.agent import BidiAgent
from strands.experimental.bidi.models.nova_sonic import NovaSonicModel

# Supports: NovaSonicModel, GeminiLiveModel, OpenAIRealtimeModel
model = NovaSonicModel(region="us-east-1")
agent = BidiAgent(model=model, tools=[my_tool])
```

æ”¯æŒä¸­æ–­æ£€æµ‹ã€å¹¶å‘å·¥å…·æ‰§è¡Œä»¥åŠè¿ç»­çš„éŸ³é¢‘ä¼ è¾“ã€‚æ­¤åŠŸèƒ½ä»å¤„äºå®éªŒé˜¶æ®µï¼ŒAPI å¯èƒ½ä¼šå‘ç”Ÿå˜åŒ–ã€‚

## ç³»ç»Ÿæç¤º

**Strands** è¿˜æ”¯æŒä½¿ç”¨ `list[SystemContentBlock]` æ¥ç”Ÿæˆç»“æ„åŒ–çš„ç³»ç»Ÿæç¤ºï¼Œå¹¶æ”¯æŒç¼“å­˜æ§åˆ¶ã€‚

## å¯è§‚æµ‹æ€§

æ”¯æŒåŸç”Ÿ OpenTelemetry è¿½è¸ªåŠŸèƒ½ï¼š

```python
agent = Agent(
    model=model,
    trace_attributes={"project": "my-agent", "environment": "dev"}
)
```

æ‰€æœ‰çš„å·¥å…·è°ƒç”¨ã€æ¨¡å‹è°ƒç”¨ã€ä»»åŠ¡äº¤æ¥ä»¥åŠç”Ÿå‘½å‘¨æœŸäº‹ä»¶éƒ½å¯ä»¥è¢«è¿½è¸ªè®°å½•ã€‚

## Bedrock ç‰¹æœ‰åŠŸèƒ½

- **å®‰å…¨æœºåˆ¶**ï¼š`BedrockModel` é…ç½®ä¸­çš„ `guardrail_id` å’Œ `guardrail_version` å¯ç”¨äºå†…å®¹è¿‡æ»¤å’Œ PIIï¼ˆä¸ªäººèº«ä»½ä¿¡æ¯ï¼‰æ£€æµ‹ï¼Œä»¥åŠè¾“å…¥/è¾“å‡ºå†…å®¹çš„ç¼–è¾‘ã€‚
- **ç¼“å­˜æœºåˆ¶**ï¼šç³»ç»Ÿæç¤ºå’Œå·¥å…·å®šä¹‰ä¼šè¢«ç¼“å­˜ä»¥ä¼˜åŒ–æ€§èƒ½ã€‚
- **æµå¼ä¼ è¾“**ï¼šé»˜è®¤å¯ç”¨ï¼Œå¯ä»¥é€šè¿‡ `streaming=False` æ¥ç¦ç”¨ã€‚
- **åŒºåŸŸè®¾ç½®**ï¼šé»˜è®¤ä½¿ç”¨ `us-west-2` åŒºåŸŸï¼Œå¯ä»¥é€šè¿‡ `region_name` å‚æ•°æˆ– `AWS_REGION` ç¯å¢ƒå˜é‡è¿›è¡Œæ›´æ”¹ã€‚
- **è·¨åŒºåŸŸæ¨ç†**ï¼šä»¥ `us.` å¼€å¤´çš„æ¨¡å‹ ID ä¼šä½¿ç”¨è·¨åŒºåŸŸæ¨ç†åŠŸèƒ½ã€‚

## æ„å»ºæ–°ä»£ç†

```bash
python3 {baseDir}/scripts/create-agent.py my-agent --provider ollama --model qwen3:latest
python3 {baseDir}/scripts/create-agent.py my-agent --provider anthropic
python3 {baseDir}/scripts/create-agent.py my-agent --provider bedrock
python3 {baseDir}/scripts/create-agent.py my-agent --provider openai --model gpt-4.1
```

ä¼šåˆ›å»ºä¸€ä¸ªåŒ…å«å·¥å…·ã€é…ç½®æ–‡ä»¶å’Œå…¥å£ç‚¹çš„å¯è¿è¡Œä»£ç†ç›®å½•ã€‚

## è¿è¡Œä»£ç†

```bash
python3 {baseDir}/scripts/run-agent.py path/to/agent.py "Your prompt here"
python3 {baseDir}/scripts/run-agent.py path/to/agent.py --interactive
```

## æ¨¡å‹æä¾›å•†å‚è€ƒï¼ˆå…± 11 ç§ï¼‰

| æä¾›å•† | ç±»å‹ | åˆå§‹åŒ–æ–¹æ³• | å¤‡æ³¨ |
|----------|-------|------|-------|
| Bedrock | `BedrockModel` | `BedrockModel(model_id=...)` | é»˜è®¤æä¾›å•†ï¼Œä¼šç«‹å³åŠ è½½ |
| Ollama | `OllamaModel` | `OllamaModel("http://host:11434", model_id=...)` | `host` å‚æ•°æ˜¯å¯é€‰çš„ |
| Anthropic | `AnthropicModel` | `AnthropicModel(model_id=..., max_tokens=4096)` | `max_tokens` å‚æ•°æ˜¯å¿…éœ€çš„ |
| OpenAI | `OpenAIModel` | `OpenAIModel(model_id=...)` | éœ€è¦ `OPENAI_API_KEY` |
| Gemini | `GeminiModel` | `GeminiModel(model_id=...)` | `api_key` å‚æ•°éœ€è¦åœ¨å®¢æˆ·ç«¯å‚æ•°ä¸­æä¾› |
| Mistral | `MistralModel` | `MistralModel(model_id=...)` | éœ€è¦ Mistral API å¯†é’¥ |
| LiteLLM | `LiteLLMModel` | `LiteLLMModel(model_id=...)` | é€‚ç”¨äº Cohereã€Groq ç­‰æ¨¡å‹ |
| LlamaAPI | `LlamaAPIModel` | `LlamaAPIModel(model_id=...)` | Meta Llama API |
| llama.cpp | `LlamaCppModel` | `LlamaCppModel(...)` | é€‚ç”¨äºæœ¬åœ°æœåŠ¡å™¨ï¼Œå…¼å®¹ OpenAI |
| SageMaker | `SageMakerAIModel` | `SageMakerAIModel(...)` | éœ€è¦è‡ªå®šä¹‰ AWS ç«¯ç‚¹ |
| Writer | `WriterModel` | `WriterModel(model_id=...)` | é€‚ç”¨äº Writer å¹³å° |

æ‰€æœ‰é Bedrock ç±»å‹çš„æä¾›å•†éƒ½æ˜¯ **æŒ‰éœ€åŠ è½½** çš„ â€” åªæœ‰åœ¨å¼•ç”¨æ—¶æ‰ä¼šå¯¼å…¥ç›¸åº”çš„ä¾èµ–é¡¹ã€‚

å¯¼å…¥æ–¹å¼ï¼š`from strands.models.<provider> import <Class>`ï¼ˆæˆ–è€…ä½¿ç”¨ `from strands.models import <Class>` æ¥å®ç°æŒ‰éœ€åŠ è½½ï¼‰ã€‚

## æç¤º

- å¦‚æœ `Agent()` æ–¹æ³•æ²¡æœ‰ `model=` å‚æ•°ï¼Œåˆ™éœ€è¦ AWS å‡­æ®ï¼ˆé»˜è®¤ä½¿ç”¨ Bedrockï¼‰ã€‚
- `AnthropicModel` éœ€è¦ `max_tokens` å‚æ•°ï¼›çœç•¥è¯¥å‚æ•°ä¼šå¯¼è‡´è¿è¡Œæ—¶é”™è¯¯ã€‚
- `OllamaModel` ä¸­çš„ `host` å‚æ•°æ˜¯å¯é€‰çš„ï¼Œæ ¼å¼ä¸º `OllamaModel("http://...", model_id="..."`ã€‚
- éƒ¨åˆ†è¢«åˆ é™¤çš„ Ollama æ¨¡å‹å¯èƒ½æ— æ³•è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼Œå»ºè®®ä½¿ç”¨é»˜è®¤æ¨¡å‹ã€‚
- Swarm ä»£ç†éœ€è¦ `name=` å’Œ `description=` å‚æ•°æ¥è¿›è¡Œä»»åŠ¡äº¤æ¥ã€‚
- `Agent LOAD_tools_from_directory=True` ä¼šç›‘æ§ `./tools/` ç›®å½•ä¸‹çš„æ–‡ä»¶å˜åŒ–ï¼Œå¹¶è‡ªåŠ¨é‡æ–°åŠ è½½å·¥å…·ã€‚
- å¯ä»¥ä½¿ç”¨ `agent.tool.my_tool()` ç›´æ¥è°ƒç”¨å·¥å…·ï¼Œæ— éœ€é€šè¿‡ LLM ä»£ç†è¿›è¡Œè½¬å‘ã€‚
- `MCPClient` æ˜¯ä¸€ä¸ª `ToolProvider`ï¼Œå¯ä»¥ç›´æ¥åœ¨å·¥å…·åˆ—è¡¨ `tools=[mcp]` ä¸­ä½¿ç”¨ï¼›åœ¨ä½¿ç”¨ `Agent` æ—¶æ— éœ€æ‰‹åŠ¨è°ƒç”¨ `list_tools_sync()`ã€‚
- ä¼šè¯ç®¡ç†å™¨é€‚ç”¨äº Agentã€Swarm å’Œ Graph æ¨¡å¼ã€‚
- è¯·ç¡®ä¿ä½¿ç”¨çš„æ˜¯æœ€æ–°ç‰ˆæœ¬çš„ `strands-agents` SDKï¼Œå› ä¸º API å¯èƒ½ä¼šåœ¨åç»­ç‰ˆæœ¬ä¸­å‘ç”Ÿå˜åŒ–ã€‚