#!/usr/bin/env python3
"""
Translate SKILL.md files from upstream skills repository.

This script:
1. Downloads upstream main.zip from GitHub
2. Extracts archive to .cache/skills-main/skills
3. Scans for all SKILL.md files
4. Checks if content is already in Chinese
5. Sends non-Chinese files to the translation service
6. Updates the files with translated content
7. Replaces ./skills with translated directory
8. Cleans up intermediate files

Features:
- Downloads and extracts upstream archive automatically
- Skips files that are already in Chinese
- Uses translation cache for efficiency
- Supports dry-run mode
- Supports skip-download and skip-replace options
"""

import argparse
import base64
import hashlib
import io
import json
import re
import shutil
import subprocess
import sys
import tarfile
import threading
import time
import zipfile
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from pathlib import Path
from typing import Optional

import os

import httpx


@dataclass
class TranslationTask:
    """éœ€è¦ç¿»è¯‘çš„æ–‡ä»¶ä»»åŠ¡"""

    file_path: str  # è¿œç¨‹æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºæ—¥å¿—ï¼‰
    local_path: Path  # æœ¬åœ°ä¿å­˜è·¯å¾„
    content: str  # æ–‡ä»¶å†…å®¹
    content_hash: str  # å†…å®¹ hash


# Allow overriding via environment variables for GitHub Actions
# UPSTREAM_REPO_URL: Override the upstream GitHub repository URL
upstream_repo_lastest_archive = os.environ.get(
    "UPSTREAM_REPO_URL",
    "https://github.com/openclaw/skills/archive/refs/heads/main.zip",
)

# Cache directory paths
cache_dir = Path(".cache")
archive_path = cache_dir / "main.zip"
target_skills_dir = Path("skills")

# Incremental sync constants
SYNC_COMMIT_ID_FILE = Path("SYNC_COMMIT_ID")
GITHUB_TOKEN = os.environ.get("GITHUB_TOKEN", "")
UPSTREAM_REPO = os.environ.get("UPSTREAM_REPO", "openclaw/skills")
UPSTREAM_REPO_URL = f"https://github.com/{UPSTREAM_REPO}"

# Dynamically determined after extraction
extracted_dir: Optional[Path] = None
source_skills_dir: Optional[Path] = None

# Retry settings
MAX_RETRIES = 3
RETRY_DELAY = 2  # seconds


def get_latest_commit_id(
    repo_url: str = "https://github.com/openclaw/skills", branch_name: str = "main"
):
    """
    è·å–è¿œç¨‹ä»“åº“æŒ‡å®šåˆ†æ”¯çš„æœ€æ–° Commit ID
    """
    # æ„é€  git ls-remote å‘½ä»¤
    # æ ¼å¼: git ls-remote <repo_url> refs/heads/<branch_name>
    command = ["git", "ls-remote", repo_url, f"refs/heads/{branch_name}"]

    try:
        # æ‰§è¡Œå‘½ä»¤
        # capture_output=True è¡¨ç¤ºæ•è·æ ‡å‡†è¾“å‡ºå’Œæ ‡å‡†é”™è¯¯
        # text=True è¡¨ç¤ºå°†è¾“å‡ºè§£ç ä¸ºå­—ç¬¦ä¸² (Python 3.7+)
        # check=True å¦‚æœå‘½ä»¤è¿”å›éé›¶çŠ¶æ€ç åˆ™æŠ›å‡ºå¼‚å¸¸
        result = subprocess.run(command, capture_output=True, text=True, check=True)

        # æ ‡å‡†è¾“å‡ºé€šå¸¸æ ¼å¼ä¸º: "<commit_id>\t<ref_name>"
        # ä¾‹å¦‚: "a1b2c3d4e5f...\trefs/heads/main\n"
        output = result.stdout.strip()

        if not output:
            print(f"æœªæ‰¾åˆ°åˆ†æ”¯: {branch_name}")
            return None

        # æŒ‰åˆ¶è¡¨ç¬¦åˆ†å‰²ï¼Œå–ç¬¬ä¸€éƒ¨åˆ†å³ä¸º Commit ID
        commit_id = output.split("\t")[0]
        return commit_id

    except subprocess.CalledProcessError as e:
        print(f"å‘½ä»¤æ‰§è¡Œå¤±è´¥: {e}")
        print(f"é”™è¯¯ä¿¡æ¯: {e.stderr}")
        return None
    except FileNotFoundError:
        print("é”™è¯¯: ç³»ç»Ÿä¸­æœªæ‰¾åˆ° git å‘½ä»¤ï¼Œè¯·ç¡®ä¿å·²å®‰è£… Git å¹¶æ·»åŠ åˆ°ç¯å¢ƒå˜é‡ã€‚")
        return None


def get_gitdiffs() -> str:
    """
    ä¸‹è½½ gitdiffs å·¥å…·åˆ° .cache/bin ç›®å½•

    Returns:
        gitdiffs å¯æ‰§è¡Œæ–‡ä»¶çš„è·¯å¾„
    """
    gitdiffs_dir = cache_dir / "bin"
    gitdiffs_dir.mkdir(parents=True, exist_ok=True)
    gitdiffs_path = gitdiffs_dir / "gitdiffs"

    # å¦‚æœå·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
    if gitdiffs_path.exists():
        return str(gitdiffs_path)

    print("ğŸ“¥ Downloading gitdiffs tool...")

    url = "https://github.com/AgentWorkers/gitdiffs/releases/download/v0.1.0/gitdiffs-x86_64-linux.tar.gz"

    for attempt in range(MAX_RETRIES):
        try:
            response = httpx.get(url, follow_redirects=True, timeout=30.0)
            response.raise_for_status()

            # è§£å‹ tar.gz æ–‡ä»¶
            with tarfile.open(fileobj=io.BytesIO(response.content), mode="r:gz") as tar:
                tar.extractall(gitdiffs_dir)

            print(f"âœ… gitdiffs downloaded to: {gitdiffs_path}")
            return str(gitdiffs_path)
        except Exception as e:
            if attempt < MAX_RETRIES - 1:
                print(f"âš ï¸  Download failed (attempt {attempt + 1}/{MAX_RETRIES}): {e}")
                time.sleep(RETRY_DELAY)
            else:
                print(
                    f"âŒ Failed to download gitdiffs after {MAX_RETRIES} attempts: {e}"
                )
                raise


def get_incremental_changes() -> Optional[dict]:
    """
    è·å–å¢é‡å˜æ›´åˆ—è¡¨

    Returns:
        åŒ…å«å˜æ›´ä¿¡æ¯çš„å­—å…¸ï¼Œæ ¼å¼ï¼š
        {
            "repo": "openclaw/skills",
            "base": "commit_id",
            "head": "commit_id",
            "files": [{"status": "ADD|DEL|MODIFY", "path": "..."}, ...]
        }
        å¦‚æœå¤±è´¥è¿”å› None
    """
    # æ£€æŸ¥ SYNC_COMMIT_ID æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not SYNC_COMMIT_ID_FILE.exists():
        print("âŒ SYNC_COMMIT_ID file not found. Please use --full for initial sync.")
        return None

    sync_commit_id = SYNC_COMMIT_ID_FILE.read_text().strip()
    latest_commit_id = get_latest_commit_id(UPSTREAM_REPO_URL)

    if not latest_commit_id:
        print("âŒ Failed to get latest commit ID")
        return None

    if sync_commit_id == latest_commit_id:
        print("âœ… Already up to date, no changes to sync")
        sys.exit(0)

    print(f"ğŸ“Œ Syncing from {sync_commit_id[:8]} to {latest_commit_id[:8]}")

    # è·å– gitdiffs å·¥å…·
    gitdiffs_path = get_gitdiffs()

    # è°ƒç”¨ gitdiffs è·å–å·®å¼‚
    cmd = [gitdiffs_path, UPSTREAM_REPO_URL, sync_commit_id, latest_commit_id]

    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        changes = json.loads(result.stdout.strip())
        print(f"ğŸ“‹ Found {len(changes.get('files', []))} changed files")
        return changes
    except subprocess.CalledProcessError as e:
        print(f"âŒ gitdiffs command failed: {e}")
        print(f"   stderr: {e.stderr}")
        return None
    except json.JSONDecodeError as e:
        print(f"âŒ Failed to parse gitdiffs output: {e}")
        return None


def download_single_file(file_path: str, commit: str) -> Optional[bytes]:
    """
    ä» GitHub ä¸‹è½½å•ä¸ªæ–‡ä»¶

    Args:
        file_path: æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ skills/xxx/SKILL.mdï¼‰
        commit: commit ID

    Returns:
        æ–‡ä»¶å†…å®¹ï¼ˆbytesï¼‰ï¼Œå¤±è´¥è¿”å› None
    """
    # ä½¿ç”¨ GitHub raw API
    url = f"https://raw.githubusercontent.com/{UPSTREAM_REPO}/{commit}/{file_path}"

    headers = {}
    if GITHUB_TOKEN:
        headers["Authorization"] = f"token {GITHUB_TOKEN}"

    for attempt in range(MAX_RETRIES):
        try:
            response = httpx.get(
                url, headers=headers, timeout=30.0, follow_redirects=True
            )
            response.raise_for_status()
            return response.content
        except httpx.HTTPStatusError as e:
            if e.response.status_code == 404:
                print(f"âš ï¸  File not found: {file_path}")
                return None
            if attempt < MAX_RETRIES - 1:
                print(
                    f"âš ï¸  Download failed (attempt {attempt + 1}/{MAX_RETRIES}): HTTP {e.response.status_code}"
                )
                time.sleep(RETRY_DELAY)
            else:
                print(f"âŒ Failed to download {file_path} after {MAX_RETRIES} attempts")
                return None
        except Exception as e:
            if attempt < MAX_RETRIES - 1:
                print(f"âš ï¸  Download failed (attempt {attempt + 1}/{MAX_RETRIES}): {e}")
                time.sleep(RETRY_DELAY)
            else:
                print(
                    f"âŒ Failed to download {file_path} after {MAX_RETRIES} attempts: {e}"
                )
                return None

    return None


def process_single_add_del_task(
    file_info: dict,
    head_commit: str,
    stats: dict,
    translation_tasks: list,
):
    """
    å¤„ç†å•ä¸ª ADD/DEL ä»»åŠ¡ï¼ˆä¸²è¡Œæ‰§è¡Œï¼‰

    Args:
        file_info: æ–‡ä»¶ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å« status å’Œ path
        head_commit: æœ€æ–° commit ID
        stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        translation_tasks: ç¿»è¯‘ä»»åŠ¡åˆ—è¡¨
    """
    status = file_info.get("status")
    file_path = file_info.get("path")

    # åªå¤„ç† skills ç›®å½•ä¸‹çš„æ–‡ä»¶
    if not file_path.startswith("skills/"):
        return

    # æœ¬åœ°è·¯å¾„ï¼ˆå»æ‰ skills/ å‰ç¼€ï¼‰
    local_path = target_skills_dir / file_path[7:]

    if status == "DEL":
        # åˆ é™¤æ–‡ä»¶
        if local_path.exists():
            local_path.unlink()
            print(f"ğŸ—‘ï¸  Deleted: {file_path}")
            stats["deleted"] += 1
        else:
            print(f"âš ï¸  File not found for deletion: {file_path}")
    else:
        # ADD æˆ– MODIFY - ä¸‹è½½æ–‡ä»¶
        print(f"ğŸ“¥ Downloading: {file_path}")

        content = download_single_file(file_path, head_commit)
        if content is None:
            stats["failed"] += 1
            return

        # åˆ›å»ºçˆ¶ç›®å½•
        local_path.parent.mkdir(parents=True, exist_ok=True)

        # åˆ¤æ–­æ˜¯å¦éœ€è¦ç¿»è¯‘
        if file_path.endswith("SKILL.md"):
            # éœ€è¦ç¿»è¯‘çš„æ–‡ä»¶
            try:
                text_content = content.decode("utf-8")
            except UnicodeDecodeError as e:
                print(f"âš ï¸  Encoding error for {file_path}: {e}")
                stats["failed"] += 1
                return

            # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯ä¸­æ–‡
            if is_chinese_content(text_content):
                local_path.write_bytes(content)
                print(f"â­ï¸  Already Chinese: {file_path}")
                stats["skipped_chinese"] += 1
                return

            # å…ˆä¿å­˜æºæ–‡ä»¶åˆ°æœ¬åœ°ï¼ˆä¿è¯å³ä½¿ç¿»è¯‘å¤±è´¥ä¹Ÿæœ‰æºæ–‡ä»¶ï¼‰
            local_path.write_bytes(content)

            # åŠ å…¥ç¿»è¯‘ä»»åŠ¡åˆ—è¡¨
            content_hash = compute_hash(text_content)

            task = TranslationTask(
                file_path=file_path,
                local_path=local_path,
                content=text_content,
                content_hash=content_hash,
            )
            translation_tasks.append(task)
            print(f"ğŸ“‹ Queued for translation: {file_path}")
        else:
            # å…¶ä»–æ–‡ä»¶ç›´æ¥ä¿å­˜
            local_path.write_bytes(content)
            print(f"âœ… Saved: {file_path}")
            stats["downloaded"] += 1


def execute_add_del_phase(
    config: TranslateConfig, changes: dict
) -> tuple[dict, list[TranslationTask]]:
    """
    ç¬¬ä¸€é˜¶æ®µï¼šä¸²è¡Œæ‰§è¡Œ ADD/DEL ä»»åŠ¡ï¼Œæ”¶é›†ç¿»è¯‘ä»»åŠ¡åˆ—è¡¨

    Args:
        config: ç¿»è¯‘é…ç½®
        changes: å˜æ›´ä¿¡æ¯å­—å…¸

    Returns:
        Tuple of (stats, translation_tasks)
    """
    stats = {
        "total_changes": len(changes.get("files", [])),
        "downloaded": 0,
        "translated": 0,
        "deleted": 0,
        "skipped_chinese": 0,
        "failed": 0,
    }
    translation_tasks: list[TranslationTask] = []

    if not changes.get("files"):
        return stats, translation_tasks

    head_commit = changes.get("head", "main")
    files = changes.get("files", [])

    print(f"ğŸ”§ Phase 1: Executing ADD/DEL tasks (sequential)")
    print(f"ğŸ“‹ Total files to process: {len(files)}")

    for file_info in files:
        try:
            process_single_add_del_task(
                file_info, head_commit, stats, translation_tasks
            )
        except Exception as e:
            print(f"âŒ Unexpected error for {file_info.get('path')}: {e}")
            stats["failed"] += 1

    print(f"âœ… Phase 1 complete. Translation tasks queued: {len(translation_tasks)}")
    return stats, translation_tasks


def process_single_translation(
    config: TranslateConfig,
    task: TranslationTask,
    stats: dict,
    stats_lock: threading.Lock,
    index: int,
    total: int,
):
    """
    å¤„ç†å•ä¸ªç¿»è¯‘ä»»åŠ¡ï¼ˆç”¨äºå¹¶å‘æ‰§è¡Œï¼‰

    Args:
        config: ç¿»è¯‘é…ç½®
        task: ç¿»è¯‘ä»»åŠ¡
        stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        stats_lock: ç»Ÿè®¡ä¿¡æ¯é”
        index: å½“å‰ç´¢å¼•
        total: æ€»æ•°
    """
    print(f"[{index}/{total}] Translating: {task.file_path}")

    relative_path = str(task.local_path.relative_to(target_skills_dir))

    result = translate_file(config, task.content, relative_path, task.content_hash)

    if result:
        translated_content, translated_hash, metadata = result
        task.local_path.write_text(translated_content, encoding="utf-8")

        with stats_lock:
            if metadata.get("cached", False):
                stats["cached"] += 1
                print(f"[{index}/{total}] âœ… Translated (cached): {task.file_path}")
            else:
                stats["translated"] += 1
                print(f"[{index}/{total}] âœ… Translated: {task.file_path}")
    else:
        # ç¿»è¯‘å¤±è´¥ï¼Œä¿å­˜åŸæ–‡ä»¶
        task.local_path.write_text(task.content, encoding="utf-8")
        print(f"[{index}/{total}] âš ï¸  Translation failed, saved original: {task.file_path}")
        with stats_lock:
            stats["failed"] += 1


def translate_collected_phase(
    config: TranslateConfig, translation_tasks: list[TranslationTask], max_workers: int = 5
) -> dict:
    """
    ç¬¬äºŒé˜¶æ®µï¼šå¹¶å‘ç¿»è¯‘æ”¶é›†åˆ°çš„æ–‡ä»¶

    Args:
        config: ç¿»è¯‘é…ç½®
        translation_tasks: ç¿»è¯‘ä»»åŠ¡åˆ—è¡¨
        max_workers: æœ€å¤§å¹¶å‘æ•°

    Returns:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    stats = {
        "translated": 0,
        "cached": 0,
        "failed": 0,
    }
    stats_lock = threading.Lock()

    if not translation_tasks:
        return stats

    total = len(translation_tasks)
    print(f"\nğŸ”§ Phase 2: Translating {total} files with {max_workers} workers")

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(
                process_single_translation,
                config,
                task,
                stats,
                stats_lock,
                i,
                total,
            ): task
            for i, task in enumerate(translation_tasks, 1)
        }

        for future in as_completed(futures):
            try:
                future.result()
            except Exception as e:
                task = futures[future]
                print(f"âŒ Unexpected error translating {task.file_path}: {e}")
                with stats_lock:
                    stats["failed"] += 1

    print(f"âœ… Phase 2 complete. Translated: {stats['translated']}, Cached: {stats['cached']}, Failed: {stats['failed']}")
    return stats


def process_incremental_changes(config: TranslateConfig, changes: dict) -> dict:
    """
    å¤„ç†å¢é‡å˜æ›´ï¼ˆä¸¤é˜¶æ®µï¼‰

    ç¬¬ä¸€é˜¶æ®µï¼šä¸²è¡Œæ‰§è¡Œ ADD/DEL ä»»åŠ¡ï¼Œæ”¶é›†ç¿»è¯‘ä»»åŠ¡åˆ—è¡¨
    ç¬¬äºŒé˜¶æ®µï¼šå¹¶å‘ç¿»è¯‘æ”¶é›†åˆ°çš„æ–‡ä»¶

    Args:
        config: ç¿»è¯‘é…ç½®
        changes: å˜æ›´ä¿¡æ¯å­—å…¸

    Returns:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    # ç¬¬ä¸€é˜¶æ®µï¼šä¸²è¡Œæ‰§è¡Œ ADD/DEL ä»»åŠ¡
    stats, translation_tasks = execute_add_del_phase(config, changes)

    # ç¬¬äºŒé˜¶æ®µï¼šå¹¶å‘ç¿»è¯‘ï¼ˆå›ºå®š 5 ä¸ªå¹¶å‘ï¼‰
    if translation_tasks:
        translation_stats = translate_collected_phase(
            config, translation_tasks, max_workers=5
        )
        # åˆå¹¶ç¿»è¯‘ç»Ÿè®¡
        stats["translated"] = translation_stats.get("translated", 0)
        stats["cached"] = translation_stats.get("cached", 0)
        # ç¿»è¯‘å¤±è´¥è®¡å…¥æ€»å¤±è´¥æ•°
        stats["failed"] += translation_stats.get("failed", 0)
    else:
        stats["cached"] = 0

    return stats


def download_upstream_archive(skip_download: bool = False) -> bool:
    """
    Download the upstream archive to cache directory.

    Args:
        skip_download: If True, skip download and use existing archive.

    Returns:
        True if archive exists (downloaded or already present), False otherwise.
    """
    cache_dir.mkdir(parents=True, exist_ok=True)

    if skip_download and archive_path.exists():
        print(f"ğŸ“¦ Using existing archive: {archive_path}")
        return True

    if skip_download and not archive_path.exists():
        print(f"âŒ Archive not found: {archive_path}")
        return False

    print("â¬‡ï¸  Downloading upstream archive...")
    print(f"   URL: {upstream_repo_lastest_archive}")

    try:
        with httpx.Client(timeout=300.0, follow_redirects=True) as client:
            response = client.get(upstream_repo_lastest_archive)
            response.raise_for_status()

            archive_path.write_bytes(response.content)
            print(f"âœ… Downloaded to: {archive_path}")
            return True
    except Exception as e:
        print(f"âŒ Failed to download archive: {e}")
        return False


def extract_archive() -> bool:
    """
    Extract the downloaded archive to cache directory.
    Dynamically detects the extracted directory name from the zip file.

    Returns:
        True if extraction successful, False otherwise.
    """
    global extracted_dir, source_skills_dir

    if not archive_path.exists():
        print(f"âŒ Archive not found: {archive_path}")
        return False

    print("ğŸ“¦ Extracting archive...")

    try:
        with zipfile.ZipFile(archive_path, "r") as zf:
            # Detect the root directory name from the zip file
            # GitHub archives have format: {repo_name}-{ref}/
            namelist = zf.namelist()
            if not namelist:
                print("âŒ Empty archive")
                return False

            # Get the root directory name (first path component)
            root_dir = namelist[0].split("/")[0]
            if not root_dir:
                print("âŒ Could not determine archive root directory")
                return False

            zf.extractall(cache_dir)

            # Set global paths based on detected directory
            extracted_dir = cache_dir / root_dir
            source_skills_dir = extracted_dir / "skills"

            if source_skills_dir.exists():
                print(f"âœ… Extracted to: {extracted_dir}")
                return True
            else:
                print(f"âŒ Skills directory not found in archive: {source_skills_dir}")
                return False
    except zipfile.BadZipFile as e:
        print(f"âŒ Invalid zip file: {e}")
        return False
    except Exception as e:
        print(f"âŒ Failed to extract archive: {e}")
        return False


def replace_skills_dir() -> bool:
    """
    Replace the target skills directory with the translated source directory.

    Returns:
        True if replacement successful, False otherwise.
    """
    if source_skills_dir is None or not source_skills_dir.exists():
        print(f"âŒ Source skills directory not found: {source_skills_dir}")
        return False

    print(f"ğŸ”„ Replacing {target_skills_dir} with translated files...")

    try:
        # Remove target directory if it exists
        if target_skills_dir.exists():
            shutil.rmtree(target_skills_dir)

        # Copy source to target
        shutil.copytree(source_skills_dir, target_skills_dir)

        print(f"âœ… Replaced {target_skills_dir}")
        return True
    except Exception as e:
        print(f"âŒ Failed to replace skills directory: {e}")
        return False


def cleanup_cache():
    """Clean up intermediate files in cache directory."""
    print("ğŸ§¹ Cleaning up cache...")

    cleaned = []

    if extracted_dir is not None and extracted_dir.exists():
        shutil.rmtree(extracted_dir)
        cleaned.append(str(extracted_dir))

    if archive_path.exists():
        archive_path.unlink()
        cleaned.append(str(archive_path))

    if cleaned:
        print(f"âœ… Cleaned up: {', '.join(cleaned)}")
    else:
        print("âœ… No cache files to clean")


class TranslateConfig:
    """Configuration for the translation process."""

    def __init__(
        self,
        skills_dir: Optional[str] = None,
        api_url: Optional[str] = None,
        api_key: Optional[str] = None,
        source_language: str = "en",
        target_language: str = "zh-CN",
        max_concurrent: int = 2,
    ):
        self.skills_dir = Path(skills_dir) if skills_dir else None
        # Use environment variable TRANSLATE_API_URL as default if not provided
        self.api_url = (
            api_url or os.environ.get("TRANSLATE_API_URL", "http://127.0.0.1:8080")
        ).rstrip("/")
        # Use environment variable TRANSLATE_API_KEY as default if not provided
        self.api_key = api_key or os.environ.get("TRANSLATE_API_KEY", "")
        self.source_language = source_language
        self.target_language = target_language
        self.max_concurrent = max_concurrent


def compute_hash(content: str) -> str:
    """Compute SHA256 hash of content."""
    return f"sha256:{hashlib.sha256(content.encode('utf-8')).hexdigest()}"


def encode_content(content: str) -> str:
    """Encode content to base64."""
    return base64.b64encode(content.encode("utf-8")).decode("ascii")


def decode_content(encoded: str) -> str:
    """Decode content from base64."""
    return base64.b64decode(encoded.encode("utf-8")).decode("utf-8")


def is_chinese_content(content: str | None) -> bool:
    """
    Check if the content is primarily in Chinese.

    Returns True if the content contains significant Chinese characters.
    Returns False if content is None or empty.
    """
    # Handle None or empty content
    if not content:
        return False

    # Remove code blocks and frontmatter for analysis
    text_content = content

    # Remove YAML frontmatter
    if text_content.startswith("---"):
        parts = text_content.split("---", 2)
        if len(parts) >= 3:
            text_content = parts[2]

    # Remove code blocks
    text_content = re.sub(r"```[\s\S]*?```", "", text_content)

    # Remove inline code
    text_content = re.sub(r"`[^`]+`", "", text_content)

    # Remove URLs
    text_content = re.sub(r"https?://\S+", "", text_content)

    # Remove English words (sequences of ASCII letters)
    text_content = re.sub(r"[a-zA-Z]+", "", text_content)

    # Count Chinese characters (CJK Unified Ideographs) - use processed text_content
    chinese_chars = re.findall(r"[\u4e00-\u9fff]", text_content)

    # Count total text characters (Chinese + ASCII letters + digits) - use processed text_content
    total_text_chars = len(
        re.findall(
            r"[\u4e00-\u9fff\u0030-\u0039\u0041-\u005a\u0061-\u007a]", text_content
        )
    )

    if total_text_chars == 0:
        return False

    chinese_ratio = len(chinese_chars) / total_text_chars

    # If more than 30% Chinese characters, consider it Chinese content
    return chinese_ratio > 0.3


def read_file_safe(path: Path) -> tuple[Optional[str], Optional[str]]:
    """
    Safely read a file with UTF-8 encoding.

    Returns:
        Tuple of (content, error_message). If successful, error_message is None.
        If failed, content is None and error_message contains the error.
    """
    try:
        content = path.read_text(encoding="utf-8")
        return content, None
    except UnicodeDecodeError as e:
        return None, f"UnicodeDecodeError: {e}"
    except Exception as e:
        return None, f"Error: {e}"


def find_skill_files(config: TranslateConfig) -> list[Path]:
    """Find all SKILL.md files in the skills directory."""
    skill_files = []
    for path in config.skills_dir.rglob("SKILL.md"):
        # Skip if in .git directory
        if ".git" in str(path):
            continue
        skill_files.append(path)
    return sorted(skill_files)


def translate_file(
    config: TranslateConfig, content: str, path: str, content_hash: str
) -> Optional[tuple[str, str, dict]]:
    """
    Send a file to the translation service.

    Returns:
        Tuple of (translated_content, translated_hash, metadata) or None on failure
    """
    url = f"{config.api_url}/api/translate"

    payload = {
        "content": encode_content(content),
        "path": path,
        "content_hash": content_hash,
        "options": {
            "source_language": config.source_language,
            "target_language": config.target_language,
        },
    }

    headers = {}
    if config.api_key:
        headers["Authorization"] = f"Bearer {config.api_key}"

    try:
        with httpx.Client(timeout=600.0) as client:
            response = client.post(url, json=payload, headers=headers)
            response.raise_for_status()
            data = response.json()

            translated_content = decode_content(data["translated_content"])
            translated_hash = data["translated_hash"]
            metadata = data["metadata"]

            return translated_content, translated_hash, metadata
    except httpx.HTTPStatusError as e:
        print(f"âŒ Translation failed for {path}: HTTP {e.response.status_code}")
        print(f"   Response: {e.response.text}")
        return None
    except Exception as e:
        print(f"âŒ Translation failed for {path}: {str(e)}")
        return None


def write_translated_file(
    config: TranslateConfig, file_path: Path, content: str, metadata: dict
):
    """Write a translated file."""
    # Write the translated content
    file_path.write_text(content, encoding="utf-8")


def process_single_file(
    config: TranslateConfig,
    skill_path: Path,
    index: int,
    total: int,
    stats: dict,
    stats_lock: threading.Lock,
):
    """
    Process a single file: read, check Chinese, translate, write.

    This function is designed to be run in a thread pool.
    """
    relative_path = str(skill_path.relative_to(config.skills_dir))
    print(f"\n[{index}/{total}] Processing: {relative_path}")

    # Read content safely
    content, error = read_file_safe(skill_path)
    if error:
        with stats_lock:
            stats["skipped_encoding"] += 1
        print(f"  âš ï¸  Skipped (encoding error: {error})")
        return

    # Type guard: ensure content is not None
    if content is None:
        with stats_lock:
            stats["skipped_encoding"] += 1
        print("  âš ï¸  Skipped (empty content)")
        return

    content_hash = compute_hash(content)

    # Check if already Chinese
    if is_chinese_content(content):
        with stats_lock:
            stats["skipped_chinese"] += 1
        print("  â­ï¸  Already in Chinese, skipping")
        return

    # Translate
    result = translate_file(config, content, relative_path, content_hash)

    if result:
        translated_content, translated_hash, metadata = result

        # Write to file
        write_translated_file(config, skill_path, translated_content, metadata)

        with stats_lock:
            if metadata.get("cached", False):
                stats["cached"] += 1
                print("  âœ… (cached)")
            else:
                stats["translated"] += 1
                print("  âœ… Translated")
    else:
        with stats_lock:
            stats["failed"] += 1
        print("  âŒ Failed")


def translate_files(config: TranslateConfig, dry_run: bool = False) -> dict:
    """
    Translate all SKILL.md files with concurrent processing.

    Returns:
        Statistics dictionary
    """
    stats = {
        "total_files": 0,
        "translated": 0,
        "cached": 0,
        "failed": 0,
        "skipped_chinese": 0,
        "skipped_unchanged": 0,
        "skipped_encoding": 0,
    }
    stats_lock = threading.Lock()

    # Find all SKILL.md files
    skill_files = find_skill_files(config)
    stats["total_files"] = len(skill_files)

    print(f"\nğŸ“‚ Found {len(skill_files)} SKILL.md files")
    print(f"ğŸ”§ Using {config.max_concurrent} concurrent workers")

    if dry_run:
        print("\nğŸ” DRY RUN - Would process the following files:")
        for i, path in enumerate(skill_files, 1):
            relative_path = str(path.relative_to(config.skills_dir))
            content, error = read_file_safe(path)
            if error:
                stats["skipped_encoding"] += 1
                print(f"  {i}. {relative_path} - âš ï¸  Skipped (encoding error: {error})")
                continue
            # Type guard: ensure content is not None
            if content is None:
                stats["skipped_encoding"] += 1
                print(f"  {i}. {relative_path} - âš ï¸  Skipped (empty content)")
                continue
            is_chinese = is_chinese_content(content)
            status = "ğŸ‡¨ğŸ‡³ Chinese (skip)" if is_chinese else "ğŸŒ Needs translation"
            print(f"  {i}. {relative_path} - {status}")
        return stats

    # Process files concurrently
    total = len(skill_files)
    with ThreadPoolExecutor(max_workers=config.max_concurrent) as executor:
        futures = {
            executor.submit(
                process_single_file, config, skill_path, i, total, stats, stats_lock
            ): skill_path
            for i, skill_path in enumerate(skill_files, 1)
        }

        # Wait for all tasks to complete
        for future in as_completed(futures):
            # Just wait, errors are handled in process_single_file
            try:
                future.result()
            except Exception as e:
                skill_path = futures[future]
                relative_path = str(skill_path.relative_to(config.skills_dir))
                print(f"  âŒ Unexpected error for {relative_path}: {e}")
                with stats_lock:
                    stats["failed"] += 1

    return stats


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Translate SKILL.md files from upstream skills repository"
    )
    parser.add_argument(
        "--full",
        action="store_true",
        help="Force full sync mode (download entire repository, default is incremental)",
    )
    parser.add_argument(
        "--skills-dir",
        default=None,
        help="Directory containing SKILL.md files to translate (auto-detected from archive if not specified)",
    )
    parser.add_argument(
        "--api-url",
        default=os.environ.get("TRANSLATE_API_URL", "http://127.0.0.1:8080"),
        help="Translation service API URL (or set TRANSLATE_API_URL env var)",
    )
    parser.add_argument(
        "--api-key",
        default=os.environ.get("TRANSLATE_API_KEY", ""),
        help="Translation service API Key (or set TRANSLATE_API_KEY env var)",
    )
    parser.add_argument("--source-language", default="en", help="Source language code")
    parser.add_argument(
        "--target-language", default="zh-CN", help="Target language code"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show what would be done without making changes",
    )
    parser.add_argument(
        "--max-concurrent",
        type=int,
        default=10,
        help="Maximum number of concurrent translations (default: 10)",
    )
    parser.add_argument(
        "--skip-download",
        action="store_true",
        help="Skip downloading archive, use existing .cache/main.zip",
    )
    parser.add_argument(
        "--skip-replace",
        action="store_true",
        help="Skip replacing ./skills directory after translation",
    )

    args = parser.parse_args()

    # Create config
    config = TranslateConfig(
        skills_dir=args.skills_dir,
        api_url=args.api_url,
        api_key=args.api_key,
        source_language=args.source_language,
        target_language=args.target_language,
        max_concurrent=args.max_concurrent,
    )

    print("=" * 60)
    print("ğŸ”„ SKILL.md Translation Script")
    print("=" * 60)
    print(f"API URL: {config.api_url}")
    print(f"Languages: {config.source_language} â†’ {config.target_language}")
    print(f"Mode: {'Full sync' if args.full else 'Incremental'}")
    print("=" * 60)

    # Check if translation service is running (for non-dry-run)
    if not args.dry_run:
        try:
            with httpx.Client(timeout=5.0) as client:
                response = client.get(f"{config.api_url}/api/health")
                response.raise_for_status()
                print("âœ… Translation service is running")
        except Exception as e:
            print(f"âŒ Translation service is not available: {e}")
            print("   Please start the translation service first:")
            print("   cd skill-translator && python -m server.main")
            sys.exit(1)

    if args.full:
        # Full sync mode
        stats = run_full_sync(config, args)
    else:
        # Incremental sync mode (default)
        stats = run_incremental_sync(config, args)

    # Print summary
    print("\n" + "=" * 60)
    print("ğŸ“Š Summary")
    print("=" * 60)
    print(f"Mode: {'Full sync' if args.full else 'Incremental'}")
    for key, value in stats.items():
        print(f"{key}: {value}")
    print("=" * 60)


def run_full_sync(config: TranslateConfig, args) -> dict:
    """Run full sync mode - download entire repository."""
    # Step 1: Download upstream archive
    if not args.skip_download or not archive_path.exists():
        if not download_upstream_archive(skip_download=args.skip_download):
            sys.exit(1)
    else:
        print(f"ğŸ“¦ Using existing archive: {archive_path}")

    # Step 2: Extract archive
    if not extract_archive():
        sys.exit(1)

    # Update config.skills_dir with the dynamically detected path
    if source_skills_dir is not None:
        config.skills_dir = source_skills_dir

    # Step 3: Check if skills directory exists
    if config.skills_dir is None or not config.skills_dir.exists():
        print(f"\nâŒ Skills directory not found: {config.skills_dir}")
        sys.exit(1)

    print(f"ğŸ“‚ Using skills directory: {config.skills_dir}")

    # Step 4: Translate files
    stats = translate_files(config, dry_run=args.dry_run)

    # Step 5: Replace skills directory (if not dry-run and not skip-replace)
    if not args.dry_run and not args.skip_replace:
        if not replace_skills_dir():
            sys.exit(1)

    # Step 6: Clean up cache
    if not args.dry_run:
        cleanup_cache()

    # Step 7: Update SYNC_COMMIT_ID
    if not args.dry_run:
        latest_commit = get_latest_commit_id(UPSTREAM_REPO_URL)
        if latest_commit:
            SYNC_COMMIT_ID_FILE.write_text(latest_commit)
            print(f"ğŸ“ Updated SYNC_COMMIT_ID to {latest_commit[:8]}")

    return stats


def run_incremental_sync(config: TranslateConfig, args) -> dict:
    """Run incremental sync mode - only sync changed files."""
    # Get incremental changes
    changes = get_incremental_changes()

    if changes is None:
        print("âŒ Failed to get incremental changes")
        sys.exit(1)

    # No changes to sync
    if not changes.get("files"):
        # Update SYNC_COMMIT_ID even when no changes to sync
        if not args.dry_run:
            head_commit = changes.get("head")
            if head_commit:
                SYNC_COMMIT_ID_FILE.write_text(head_commit)
                print(f"ğŸ“ Updated SYNC_COMMIT_ID to {head_commit[:8]}")
        return {"total_changes": 0, "message": "Already up to date"}

    # Process changes
    stats = process_incremental_changes(config, changes)

    # Update SYNC_COMMIT_ID
    if not args.dry_run:
        head_commit = changes.get("head")
        if head_commit:
            SYNC_COMMIT_ID_FILE.write_text(head_commit)
            print(f"ğŸ“ Updated SYNC_COMMIT_ID to {head_commit[:8]}")

    return stats


if __name__ == "__main__":
    main()
